@' pathfinder.mx
@'
@' XQuery runtime environment
@'
@/
Copyright Notice:
-----------------

The contents of this file are subject to the Pathfinder Public License
Version 1.1 (the "License"); you may not use this file except in
compliance with the License.  You may obtain a copy of the License at
http://monetdb.cwi.nl/Legal/PathfinderLicense-1.1.html

Software distributed under the License is distributed on an "AS IS"
basis, WITHOUT WARRANTY OF ANY KIND, either express or implied.  See
the License for the specific language governing rights and limitations
under the License.

The Original Code is the Pathfinder system.

The Original Code has initially been developed by the Database &
Information Systems Group at the University of Konstanz, Germany and
the Database Group at the Technische Universitaet Muenchen, Germany.
It is now maintained by the Database Systems Group at the Eberhard
Karls Universitaet Tuebingen, Germany.  Portions created by the
University of Konstanz, the Technische Universitaet Muenchen, and the
Universitaet Tuebingen are Copyright (C) 2000-2005 University of
Konstanz, (C) 2005-2008 Technische Universitaet Muenchen, and (C)
2008-2011 Eberhard Karls Universitaet Tuebingen, respectively.  All
Rights Reserved.
@
@' $Id$
@'

@f pathfinder
@a Torsten Teggy Grust
@a Maurice van Keulen
@a Henning Rode
@a Jan Flokstra
@a Jens Teubner
@a Stefan Manegold
@a Peter Boncz 
@a Ying Zhang
@a Niels Nes

@t Runtime Support for the Pathfinder XQuery Compiler

@m
.MODULE pathfinder;

.USE logger;
.USE pf_support;

.COMMAND xquery_frontend() : ptr = xquery_frontend;
 "create/return the xquery_frontend callback interface to pass to mapi register. "

.BUILTIN xquery(mode str, xquery str, is_url bit) : str = CMDxquery;
 "xquery execution. parameters: mode, query, is_url (optional)
 usage:  var result := xquery(\"xml\", \"1+1\", false); printf(\"%s\",result);
 or:  printf(\"%s\",xquery(\"xml\", \"1+1\"));"

.COMMAND xquery_start_query_cache(lng maxsize) : void = CMDxquery_start_query_cache; 
 "Cached xquery clients use the query plan cache (also flushes the cache when called). if nonzero, the parameter is a per-connection size limit to the plan cache in bytes"

.COMMAND xquery_logger_create( int debug, str fn, str dirname, str dbname, int version ) : logger = CMDxquery_logger_create;
 "Create logger, check if recovery is needed.  The XRPC handler for PRE_COMMIT is passed."

.PRELUDE = xquery_prelude;
.EPILOGUE = xquery_epilogue;

.END pathfinder;
@mil
if (clientid() != 0) {
    ERROR("pathfinder module must be loaded in Mserver console first.");
}

const XQUERY_STATUS_INITIALIZING := 0; 
const XQUERY_STATUS_RECOVERING   := 1; 
const XQUERY_STATUS_GARBAGECOLL  := 2; 
const XQUERY_STATUS_READY        := 3; 

proc tj_is_indexed(str name) : bit { return false; }

var CATCH_module_pftijah := CATCH(module("pftijah")); # load pftijah only if available
if (not(isnil(CATCH_module_pftijah))) {
    if (trim(CATCH_module_pftijah) != "!ERROR: moduleClient: module(pftijah) load error.") {
        ERROR("pathfinder: when trying to load module 'pftijah':\n" + CATCH_module_pftijah);
    }
}
else {
    if (monet_environment.find("monet_welcome") = "yes")
        printf("%c PF/Tijah module v0.13.0 loaded. http://dbappl.cs.utwente.nl/pftijah\n", int
(35));
}


module(mapi);       # remote client access
module(pf_support);
module(logger);
module(mkey);
module(xrpc_server);
module(xrpc_client);
module(pf_standoff);

# standoff is only available if you ask for it
var standoff := false;
if (monet_environment.exist("standoff"))
  standoff := =(monet_environment.find("standoff"),"enabled"); 


# The XML Meta-Database #############################################################
#
# Global information on persistent stored XML collections:
# - collection_name   BAT[oid,name]             coll-ID / collection name 
# - collection_size   BAT[oid,lng]              coll-ID / size in bytes
# - collection_zombie BAT[oid,lng]              coll-ID / ws-ID that blocked its deletion (TRANSIENT) 
#
# A collection may contain one ore more XML documents:
# - doc_collection    BAT[oid,oid]          document-ID / coll-ID 
# - doc_name          BAT[oid,str]          document-ID / document name
# - doc_location      BAT[oid,str]          document-ID / document URI
# - doc_timestamp     BAT[oid,timestamp]    document-ID / end-of-cache-time (nil if none)
# - doc_undo          BAT[oid,lng]          document-ID / ws-ID that created it (TRANSIENT) 
#
# some runtime (TRANSIENT) information is also kept on collections, by name: 
# - colname_shredlock BAT[str,lock]     collection name / append-lock
# - colname_runtime   BAT[str,bat]      collection name / index+locking structures 
# - colname_pins      BAT[str,lng]      collection name / ws-ID using it 
# - colname_locks     BAT[str,lng]      collection name / ws-ID having it locked
#
# Meta-information on running transactions (table only contains still active queries Y)
# - ws_overlaps_ws    BAT[lng,lng]                ws-ID / ws-ID 
#   X/Y means execution of X overlapped that of Y (and both used the same collection)
#
# Meta-information controlling the cache:
# - uri_lifetime      BAT[str,lng]          URI prefix  / time-to-live (seconds), 
#                                                         nil if not to be cached 
# these tables are protected by the pf_short lock.

# Document Containers ##############################################################
#
# Each imported XML-document (collection) is stored in a series of persistent 
# BATs (columns) whose name starts with the collection ID and ends with the 
# column name. The full set of these bats is called a 'document container'.
# Thus, each document collection maps on a single persistent document container
# (a set of bats). At run-time, multiple isolated copies (containers) for a certain
# collection may be open. Each query also creates a temporary 'transient document
# container' consisting of transient BATs, which holds only nodes that are
# constructed in the query.
#
# Each document container contains the following rid_* BATs from which the pre_* 
# BATs are derived:
# - map_pid            BAT[void,oid] 64K tuple page-id / its page index in the rid_table
# - nid_rid            BAT[void,oid]        stable NID / RID 
# - rid_size           BAT[void,int]               RID / #descendants,
# - rid_level          BAT[void,chr]               RID / distance from root,
# - rid_prop           BAT[void,oid]               RID / property-ID,
# - rid_kind           BAT[void,chr]               RID / node kind (determines prop_* column)
# - rid_nid            BAT[void,oid]               RID / stable NID 
# - frag_root          BAT[oid,oid]        document-ID / root NID
#                              
# NOTE: read-only collections have NID_RID and MAP_PID as BAT[void,void] views. 
#       For them RID=PRE=NID in all cases, the RID-PRE-NID transformations (leftfetchjoin/ 
#       siwzzle) are zero-cost. This is also true for the transient document container. 
#
# NOTE: MAP_PID page indices start at oid(0). by putting the rid_* physical pages 
#       in the order specified by MAP_PID we get the original pre_* tables back.
#
# qn_* tables contain qualified names (elements, attributes). 
# They are double-eliminated on (prefix_uri_loc) and we keep a histogram statistic 
# on them, that is used for indexing only 'relatively infrequent' qnames:
# - qn_prefix_uri_loc: BAT[void,str]          qname-ID / prefix|uri|loc
# - qn_uri_loc         BAT[void,str]          qname-ID / uri|loc only
# - qn_prefix          BAT[void,str]          qname-ID / prefix
# - qn_ns              BAT[void,str]          qname-ID / namespace URI
# - qn_loc             BAT[void,str]          qname-ID / local name (e.g. element name)
# - qn_histogram       BAT[void,lng]          qname-ID / occurence count (estimate)
#
# the prop_* bats are independent (varying sizes for each kind) and keep UTF-8 string data
# - prop_text          BAT[void,str]       property-ID / text,
# - prop_com           BAT[void,str]       property-ID / comment,
# - prop_ins           BAT[void,str]       property-ID / processing instruction,
# - prop_tgt           BAT[void,str]       property-ID / processing instruction target,
# - prop_val           BAT[void,str]       property-ID / string value of attribute node
#
# the attr_* bats contain attribute data:
# - attr_own           BAT[void,oid]      attribute-ID / NID of owner
# - attr_qn            BAT[void,oid]      attribute-ID / and qname-ID
# - attr_prop          BAT[void,oid]      attribute-ID / property-ID (for prop_val)
#
# attr_prop stores the values separately to allow "shallow copying", where in a transient 
# document container, an attribute is both defined (qname) and its value stored (prop_val) 
# in a different collection (see PRE_CONT and ATTR_CONT below).


# The Working Set #################################################################
#
# A "working set" contains all XML data currently accessible to a running query;
# each running XQuery has its own working set. 
#
# Thus, for each bat in a document container, the working set contains a bat-of-bats.
# It contains one entry for each loaded container.
#
# For updatable collections, the  bats in the working set used for query execution 
# are *views* on the persistent bats, to provide isolation.
# - for the pre/post table pages from the rid_* are remapped into isolate pre_* bats
# - all other container bats are copy-on-write mmap views on the masters
#   (mmap is only used for big heaps; small heaps are malloced copies)
# - the working set also contains all masters. Update queries need access
#   to the masters when they commit. The master column of "X" is called "_X".
# - for read-only collections (including all documents shredded on-the-fly)
#   there is no difference between the queryable bats and the masters.
#
# The working set can have multiple documents open. Those multiple documents
# may stem from a single, or multiple collections. The code is optimized to deal
# with huge numbers of (small) documents from a single collection efficiently. 
#
# Each open collection has a corresponding container in the working set,
# administered in a table:
# - CONT_COLL          BAT[void,oid]      container-ID / collection-ID 
# - CONT_NAME          BAT[void,str]      container-ID / collection-ID 
#                                                       (both nil for transient container)
# - CONT_RUNTIME       BAT[void,bat]      container-ID / index bat
#                                                       (empty for transient container)
# the idea behind having NAME and RUNTIME (from collection_name and colname_runtime) 
# replicated inside the ws besides the collection -ID (with which those could be looked up)
# is that during the query we want to limit taking the pf_short lock to do that each time.
#
# since each container can contain multiple documents, these are in a separate table:
# - OPEN_CONT          BAT[void,oid]        open-docid / container-ID 
# - OPEN_NAME          BAT[void,str]        open-docid / document name
# - OPEN_DOCID         BAT[void,oid]        open-docid / persistent document-ID 
#
# NOTE: when the query starts, a first, empty container is created, the
#       'transient document container'. It has container-ID (cont) 0.
#
# NOTE: open temporary shredded docmuments that are not cached have both 
#       _COLL and _DOCID fields set to an "artificial" doc_oid == open-docid.
#       These are values < DOCID_MIN (1 billion). It is only unique within the ws.
#       Persistent documents and container have globally unique keys that 
#       point into the doc_* and collection_* tables.
#
# For each container, the working set contains two extra virtual bats:
# - PRE_CONT           BAT[void,BAT[void,oid]] 
#                      list of bats with containers oids for each pre value 
#                      (copied in element construction)
# - ATTR_CONT          BAT[void,BAT[void,oid]] 
#                      list of bats with containers oids for each attr value 
#                      (copied in element/attribute construction)
#
# These bats are *constant* (i.e. have an identical value for each tuple)
# for all shredded documents, hence they are not stored on disk (we create a
# view when they are loaded instead). Their function is to allow shallow copying 
# in the transient document container.  i.e. for the transient container, these 
# bats are not views, and their CONT values refer to other containers.
#
# We turned out to have a performance issue for accessing attributes of
# a node: due to isolation, hash-table indices on ATTR_OWN get lost after
# each query.  For this purpose, two extra working-set entries appeared:
# - ATTR_OWN_SHARED    BAT[void,BAT[void,oid]] 
# - ATTR_OWN_PRIVATE   BAT[void,BAT[void,oid]] 
#
# Attributes are modified in the following way:
# - added attributes are append-only
# - removed attributes have their owner nid set to nil 
#
# This pattern can be exploited by sharing an out-of-date copy of ATTR_OWN
# as an index structure; such that a hash table on it can be re-used by multiple 
# queries. This is ATTR_OWN_SHARED. The slice of ATTR_OWN of missing tuples 
# (i.e. those appended after the copy was taken) is denominated ATTR_OWN_PRIVATE. 
#
# The PROC get_attr_own() (pf_support.mx) finds the attributes of a number 
# of nodes (NIDs), by consulting ATTR_OWN_SHARED (with hash table!) and 
# ATTR_OWN_PRIVATE separately. The results from ATTR_OWN_SHARED must be 
# re-checked in ATTR_OWN to filter out deleted attributes.
#
# ATTR_OWN_SHARED is stored as a runtime index structure in RT_ATTR_OWN.
#
# cache bats used for caching node sequences:
# - CACHE_ID[void,str]    name
# - CACHE_VAL[void,bat]   bat-of-bats (iter,pos,..)
# - CACHE_LRU[void,lng]   last time used (usec)
# - CACHE_SIZE[void,lng]  size in bytes of VAL

@- ws definition

This macro is used for the MIL const defs, the C const defs,
but also in the MIL procs for creating, filling and destroying
a working set.

We define a table with the column name, its number,
the type of data (typically again a bat), and if
so the head and tail type of that bat.

actually the field 'tpe' is taken to be bat *always*, 
EXCEPT when (child-T == void)

('tpe' = void means it is a view; without persistent name)

All ws entries starting with '_' are master bats, whereas 
their versions without '_' copies/views. The latter ones
are used for querying, as they provide isolation against
concurrent updates.

         name           number  htp  ttp  col[H,T]    col-seqbase 
         ========       ======  ===  ===  ==========  ===========
@= ws
@:ws_@1(MAP_PID,            0, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(PRE_SIZE,           1, void, bat, void,  int, PRE_BASE)@
@:ws_@1(PRE_LEVEL,          2, void, bat, void,  chr, PRE_BASE)@
@:ws_@1(PRE_PROP,           3, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(PRE_KIND,           4, void, bat, void,  chr, PRE_BASE)@
@:ws_@1(PRE_NID,            5, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(NID_RID,            6, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(FRAG_ROOT,          7, void, bat,  oid,  oid, oid_nil)@
@:ws_@1(QN_HISTOGRAM,       8, void, bat, void,  lng, PRE_BASE)@
@:ws_@1(QN_PREFIX_URI_LOC,  9, void, bat, void,  str, PRE_BASE)@
@:ws_@1(QN_URI_LOC,        10, void, bat, void,  str, PRE_BASE)@
@:ws_@1(QN_PREFIX,         11, void, bat, void,  str, PRE_BASE)@
@:ws_@1(QN_URI,            12, void, bat, void,  str, PRE_BASE)@
@:ws_@1(QN_LOC,            13, void, bat, void,  str, PRE_BASE)@
@:ws_@1(PROP_TEXT,         14, void, bat, void,  str, PRE_BASE)@
@:ws_@1(PROP_COM,          15, void, bat, void,  str, PRE_BASE)@
@:ws_@1(PROP_INS,          16, void, bat, void,  str, PRE_BASE)@
@:ws_@1(PROP_TGT,          17, void, bat, void,  str, PRE_BASE)@
@:ws_@1(PROP_VAL,          18, void, bat, void,  str, PRE_BASE)@
@:ws_@1(ATTR_OWN,          19, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(ATTR_QN,           20, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(ATTR_PROP,         21, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(QN_NID,            22, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(QN_NID_DEL,        23, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(QN_NID_INS,        24, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(QN_NID_UNQ,        25, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(VX_HSH_NID,        26, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(VX_HSH_NID_INS,    27, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(VX_HSH_NID_DEL,    28, void, bat, void,  oid, PRE_BASE)@

@:ws_@1(PRE_CONT,          29, void, void,void,  oid, PRE_BASE)@
@:ws_@1(ATTR_CONT,         30, void, void,void,  oid, PRE_BASE)@
@:ws_@1(PID_MAP,           31, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(REGION_PRE,        32, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(REGION_START,      33, void, bat, void,  lng, PRE_BASE)@
@:ws_@1(REGION_END,        34, void, bat, void,  lng, PRE_BASE)@
@:ws_@1(ATTR_OWN_SHARED,   35, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(ATTR_OWN_PRIVATE,  36, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(OPEN_NAME,         37, void, str, void, void, oid_nil)@
@:ws_@1(OPEN_DOCID,        38, void, oid, void, void, oid_nil)@
@:ws_@1(OPEN_CONT,         39, void, oid, void, void, oid_nil)@
@:ws_@1(CONT_COLL,         40, void, oid, void, void, oid_nil)@
@:ws_@1(CONT_NAME,         41, void, str, void, void, oid_nil)@
@:ws_@1(CONT_RUNTIME,      42, void, bat, void, void, oid_nil)@
@:ws_@1(CONT_LOCKED,       43, void, lock,void, void, oid_nil)@
@:ws_@1(XRPC_PARTICIPANTS, 44, void, str, void, void, oid_nil)@

@:ws_@1(CACHE_ID,          45, void, str, void, void, oid_nil)@
@:ws_@1(CACHE_VAL,         46, void, bat, void, void, oid_nil)@
@:ws_@1(CACHE_LRU,         47, void, lng, void, void, oid_nil)@
@:ws_@1(CACHE_SIZE,        48, void, lng, void, void, oid_nil)@

@:ws_@1(_MAP_PID,          49, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(_RID_SIZE,         50, void, bat, void,  int, PRE_BASE)@
@:ws_@1(_RID_LEVEL,        51, void, bat, void,  chr, PRE_BASE)@
@:ws_@1(_RID_PROP,         52, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(_RID_KIND,         53, void, bat, void,  chr, PRE_BASE)@
@:ws_@1(_RID_NID,          54, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(_NID_RID,          55, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(_FRAG_ROOT,        56, void, bat,  oid,  oid, oid_nil)@
@:ws_@1(_QN_HISTOGRAM,     57, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_QN_PREFIX_URI_LOC,58, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_QN_URI_LOC,       59, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_QN_PREFIX,        60, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_QN_URI,           61, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_QN_LOC,           62, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_PROP_TEXT,        63, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_PROP_COM,         64, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_PROP_INS,         65, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_PROP_TGT,         66, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_PROP_VAL,         67, void, bat, void,  str, PRE_BASE)@
@:ws_@1(_ATTR_OWN,         68, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(_ATTR_QN,          69, void, bat, void,  oid, PRE_BASE)@
@:ws_@1(_ATTR_PROP,        70, void, bat, void,  oid, PRE_BASE)@
@-
The bottom segment are the master copies of the top segment.
The middle segment is private to the query.

BEWARE: WS_SIZE (below) should *always* be the size of ws (above)

BEWARE: the logger version number below needs to be incremented whenever the working set schema is changed.

@= ws_decl
@:ws_@1_decl(WS_SIZE, 71)@
@:ws_@1_decl(QNAME,    2)@
@:ws_@1_decl(BOOL,     3)@
@:ws_@1_decl(INT,      4)@
@:ws_@1_decl(DEC,      5)@
@:ws_@1_decl(DBL,      6)@
@:ws_@1_decl(STR,      7)@
@:ws_@1_decl(U_A,      8)@
@:ws_@1_decl(ATOMIC,  31)@
@:ws_@1_decl(NODE,    32)@
@:ws_@1_decl(ATTR,    33)@
@:ws_@1_decl(ELEM,    34)@
@:ws_kind_decl(@1)@
@:ws_@1_decl(NS_ACCEL_SEP,'|',,,,,,.str())@

@= ws_mil_decl
const @1 := @2@8;
@mil
@:ws(mil_decl)@
@:ws_decl(mil)@

# logger version number, needs to be incremented whenever the working set schema is changed
const LOGGER_VERSION := 2;

# KIND constants, carefully chosen
# atomic value items can be retrieved with 'kind.select(int_nil,ATOMIC)'
# NODE is not a type but all node types can be retrieved with 'kind.select(NODE,int_nil)'
@= ws_kind_decl
@:ws_@1_decl(ELEMENT,   0,,,,,,.chr())@
@:ws_@1_decl(TEXT,      1,,,,,,.chr())@
@:ws_@1_decl(COMMENT,   2,,,,,,.chr())@
@:ws_@1_decl(PI,        3,,,,,,.chr())@
@:ws_@1_decl(DOCUMENT,  4,,,,,,.chr())@
@:ws_@1_decl(COLLECTION,5,,,,,,.chr())@
@:ws_@1_decl(REFERENCE ,6,,,,,,.chr())@

@mil
# zero
const WS               := 0@0; # first container in ws = transient doc container
const PRE_BASE         := 0@0; # all our PRE bats start at 0 (the super-root)
const TEMP_DOC         := 0@0; # doc/coll key that is never used for persistent docs
const DOCID_CURID_HACK := 0@0; # doc/coll key that is never used for persistent docs
const DOCID_PGBIT_HACK := 1@0; # doc/coll key that is never used for persistent docs
const DOCID_MIN        := 1000000000@0; # persistent doc-ids start here
const DOCID_MAX        := 2000000000@0; # and end here (so we have 1G of them)
const WS_MAXID         := 1LL << 62;

# slots in the col_runtime bats
const RT_LOCK_FREELIST := 0; # free page ids
const RT_NID_FREELIST  := 1; # free nids 
const RT_REGION_WSID   := 2; # wsid that is allowed to make the region index
const RT_REGION_PRE    := 3; # standoff pre numbers
const RT_REGION_START  := 4; # standoff start offset
const RT_REGION_END    := 5; # standoff end numbers
const RT_PAGE_WS       := 6; # pages modified by transactions 
const RT_ATTR_WS       := 7; # attributes modified by transactions
const RT_ATTR_OWN      := 8; # a shared (old) ATTR_OWN bat with hash-table on OWN
const RT_QN_NID        := 9; # qn-nid nsloc index, with hash-table on QN
const RT_QN_NID_INS    := 10;# qn-nid nsloc index: extra items
const RT_QN_NID_DEL    := 11;# qn-nid nsloc index: deleted items
const RT_QN_NID_UNQ    := 12;# qn-nid nsloc index: subset of indexed qns (small)
                             
# value indexing for both text and attr. stores hashcodes that combines qn and value
const RT_VX_HSH_NID    := 13;# [int,oid] owner = ATTR/NID (text), hash = qn^hash(strval)
const RT_VX_HSH_NID_INS:= 14;# value index, inserted items
const RT_VX_HSH_NID_DEL:= 15;# value index, deleted items

# for coll_lock_[un]set, you can request SHORTLOCK, LONGLOCK or BOTHLOCK mode
const COLL_SHORTLOCK   := 1;
const COLL_LONGLOCK    := 2;
const COLL_BOTHLOCK    := 3;

const WSID_UPDATE := (1LL << 31);

# dirty solution (proposed by Peter ;) to be able to locate (and replace) the
# bats inside the colname_runtime-bat.
const RT_REGION_PRE_LOCK    := lock(RT_REGION_PRE);
const RT_REGION_START_LOCK  := lock(RT_REGION_START);
const RT_REGION_END_LOCK    := lock(RT_REGION_END);

# these constants are used by the update code
# the values should remain in this order and be consecutive with the indices from the working set
const MAP_PID_UPDATE := _ATTR_PROP + 1; # assume _ATTR_PROP is the last entry in the working set
const RID_SIZE_UPDATE := MAP_PID_UPDATE + 1;
const RID_LEVEL_UPDATE := RID_SIZE_UPDATE + 1;
const RID_KIND_UPDATE := RID_LEVEL_UPDATE + 1;
const RID_PROP_UPDATE := RID_KIND_UPDATE + 1;
const RID_NID_UPDATE := RID_PROP_UPDATE + 1;
const NID_RID_UPDATE := RID_NID_UPDATE + 1;
const ATTR_QN_UPDATE := NID_RID_UPDATE + 1;
const ATTR_PROP_UPDATE := ATTR_QN_UPDATE + 1;
const ATTR_OWN_UPDATE := ATTR_PROP_UPDATE + 1;
const PROP_VAL_UPDATE := ATTR_OWN_UPDATE + 1;
const PROP_TEXT_UPDATE := PROP_VAL_UPDATE + 1;
const PROP_COM_UPDATE := PROP_TEXT_UPDATE + 1;
const PROP_INS_UPDATE := PROP_COM_UPDATE + 1;
const PROP_TGT_UPDATE := PROP_INS_UPDATE + 1;
# the next 6 entries must be in the same order as QN_HISTOGRAM through QN_LOC above
const QN_HISTOGRAM_UPDATE := PROP_TGT_UPDATE + 1;
const QN_PREFIX_URI_LOC_UPDATE := QN_HISTOGRAM_UPDATE + 1;
const QN_URI_LOC_UPDATE := QN_PREFIX_URI_LOC_UPDATE + 1;
const QN_PREFIX_UPDATE := QN_URI_LOC_UPDATE + 1;
const QN_URI_UPDATE := QN_PREFIX_UPDATE + 1;
const QN_LOC_UPDATE := QN_URI_UPDATE + 1;
const UPDATED_TEXT := QN_LOC_UPDATE + 1;
const NID_QN_INS_UPDATE := UPDATED_TEXT + 1; # note, NID_QN and not QN_NID
const NID_QN_DEL_UPDATE := NID_QN_INS_UPDATE + 1;
const MODIFIED_NID := NID_QN_DEL_UPDATE + 1;
const ANCESTOR_NID := MODIFIED_NID + 1;
const MODIFIED_ATTR := ANCESTOR_NID + 1;
const MODIFIED_PAGE := MODIFIED_ATTR + 1;
const NEW_PAGE := MODIFIED_PAGE + 1;
const DEL_PAGE := NEW_PAGE + 1;
const DELETED_NID := DEL_PAGE + 1;
const ADDED_NID := DELETED_NID + 1;
const ADDED_ATTR := ADDED_NID + 1;
const DELETED_TEXT_NID := ADDED_ATTR + 1;
const ADDED_TEXT_NID := DELETED_TEXT_NID + 1;
# the following are used for XRPC updates
const COLLECTED_COMMAND := ADDED_TEXT_NID + 1;
const COLLECTED_PRE_TGT := COLLECTED_COMMAND + 1;
const COLLECTED_PRE_CONT_TGT := COLLECTED_PRE_TGT + 1;
const COLLECTED_ATTR_TGT := COLLECTED_PRE_CONT_TGT + 1;
const COLLECTED_ATTR_CONT_TGT := COLLECTED_ATTR_TGT + 1;
const COLLECTED_REPLACE_STRINGS := COLLECTED_ATTR_CONT_TGT + 1;
const COLLECTED_RENAME_QN_URI := COLLECTED_REPLACE_STRINGS + 1;
const COLLECTED_RENAME_QN_PREFIX := COLLECTED_RENAME_QN_URI + 1;
const COLLECTED_RENAME_QN_LOCAL := COLLECTED_RENAME_QN_PREFIX + 1;
const COLLECTED_PRE_INS := COLLECTED_RENAME_QN_LOCAL + 1;
const COLLECTED_PRE_CONT_INS := COLLECTED_PRE_INS + 1;
const COLLECTED_ATTR_INS := COLLECTED_PRE_CONT_INS + 1;
const COLLECTED_ATTR_CONT_INS := COLLECTED_ATTR_INS + 1;

# meta-data BATs for multi-request XRPC transactions
var xrpc_qids     := bat("xrpc_qids");      # bat[void,str] query-id (host|timestamp)
var xrpc_timeouts := bat("xrpc_timeouts");  # bat[void,lng] query timeout in usec
var xrpc_wsbats   := bat("xrpc_wsbats");    # bat[void,lng] ID of the workingset associated with this query
var xrpc_locks    := bat("xrpc_locks");     # bat[void,lock] lock for the workingset of this query 
var xrpc_statuses := bat("xrpc_statuses");  # bat[void,str] 2PC status of this query: "exec", "wait", "prepare", "commit", "abort" or "timeout"
var xrpc_lock     := pflock_get(6); # master XRPC lock, to protect above xrpc_* BATs
var xrpc_hostport := my_hostname() + ":" + str(get_xrpc_port());
var xrpc_querynr  := (lng(get_xrpc_port()) * 100000LL) - 1LL;

# dummy variables to make MIL test scripts work
# (otherwise always overridden by a declaration in the pf-generated MIL plan)
var xrpc_seqnr, xrpc_coord, xrpc_mode := "", xrpc_qid := "", xrpc_method := "", xrpc_timeout := 30000LL;

# abort a 2PC transaction, keeping the record (note ws should have been destroyed already)
PROC _ws_xrpc_abort(oid idx, str status) : void
{
    var wslock := xrpc_locks.find(idx);
    var ws := xrpc_wsbats.find(idx);
    ws_destroy(ws);
    xrpc_statuses.inplace(idx, status);
    xrpc_wsbats.inplace(idx, xrpc_statuses); # use xrpc_statuses as a dummy bat, replacing the ws (that gets freed)
    xrpc_locks.inplace(idx, lock_nil);
    if (xrpc_qids.find(idx).search(":") < 0) xrpc_qids.inplace(idx, "");
    lock_destroy(wslock);
}

# end a 2PC request. On success, unlock ws and keep waiting for more. On failure, abort it.
PROC _ws_xrpc_end(str xrpc_qid, str errmsg) : void
{
    var idx := reverse(xrpc_qids).find(xrpc_qid);
    if (isnil(errmsg) and (xrpc_timeout > 0LL)) { 
        xrpc_statuses.inplace(idx, "wait");
    } else {
        xrpc_statuses.inplace(idx, "abort");
    }
    lock_unset(xrpc_locks.find(idx));
}

# background check to time-out waiting 2PC transactions, and prune them fully after an hour
PROC _ws_xrpc_prune() : void
{
    var lim      := usec();
    var timeout  := mirror(xrpc_statuses.ord_select("wait")).leftfetchjoin(xrpc_timeouts).ord_uselect(0LL,lim).project("timeout");
    var aborted  := mirror(xrpc_locks.ord_uselect(lock_nil,lock_nil)).leftfetchjoin(xrpc_statuses).ord_select("abort");

    kunion(aborted,timeout)@batloop() {
       _ws_xrpc_abort($h,$t);
    }
    var relevant := xrpc_timeouts.ord_uselect(lim - 3600000000LL,lng_nil).hmark(0@0);
    if (count(relevant) < count(xrpc_timeouts)) {
        # pruned xrpc requests should all have destroyed wslock and ws (we let them leak here)
        var relevant_qids     := relevant.leftfetchjoin(xrpc_qids);      xrpc_qids.delete().append(relevant_qids);
        var relevant_timeouts := relevant.leftfetchjoin(xrpc_qtimeouts); xrpc_timeouts.delete().append(relevant_timeouts);
        var relevant_wsbats   := relevant.leftfetchjoin(xrpc_wsbats);    xrpc_wsbats.delete().append(relevant_wsbats);
        var relevant_locks    := relevant.leftfetchjoin(xrpc_locks);     xrpc_locks.delete().append(relevant_locks);
        var relevant_statuses := relevant.leftfetchjoin(xrpc_statuses);  xrpc_statuses.delete().append(relevant_statuses);
    }
}

PROC xrpc_status() : void {
    lock_set(xrpc_lock);
    var err := CATCH(print(xrpc_qids,xrpc_statuses,xrpc_timeouts,xrpc_locks,xrpc_wsbats));
    lock_unset(xrpc_lock);
    if (not(isnil(err))) GDKerror(err);
}


# transaction debugging / performance profiling 
var ws_log_lock := lock_create();
var ws_log := bat(lng,str,1000000).rename("ws_log");
var ws_log_id := 0LL;
var ws_log_wsid := 0LL;
var ws_log_msg := "";
var ws_log_active := false;

PROC ws_log(lng wsid, str msg) : void {
    var id := (usec() * 1000LL) + lng(clientid()); 
    lock_set(ws_log_lock);
    ws_log.insert(ws_log_id := id, ws_log_msg := str(wsid) + " " + msg);
    lock_unset(ws_log_lock);
}
PROC ws_log(BAT[void,bat] ws, str msg) : void { ws_log(ws_id(ws), msg); }

var ws_log_stamp, ws_log_txt, ws_log_tid, ws_log_lck, ws_log_exc;

# analyze the last run 
PROC ws_log_analyze() : void {
    ws_log_active := true;
    ws_log_stamp := ws_log.hmark(0@0);
    ws_log_txt := ws_log.tmark(0@0);
    ws_log_tid := [lng](ws_log_txt);
    ws_log_lck := [search](ws_log_txt, "lock-").ord_uselect(0,int_nil).mirror().leftfetchjoin(ws_log_txt);
    ws_log_lck := [lng](ws_log_lck.[string](ws_log_lck.[search]("lock-").[+](5))).ord_uselect(1000LL,lng_nil).mirror().leftfetchjoin(ws_log_txt);
    ws_log_exc := [search](ws_log_txt, "exec-").ord_uselect(0,int_nil).mirror().leftfetchjoin(ws_log_txt);
    ws_log_exc := [lng](ws_log_exc.[string](ws_log_exc.[search]("exec-").[+](5))).ord_uselect(1000LL,lng_nil).mirror().leftfetchjoin(ws_log_txt);
    ws_log_stamp.rename("ws_log_stamp");
    ws_log_txt.rename("ws_log_txt");
    ws_log_tid.rename("ws_log_tid");
    ws_log_lck.rename("ws_log_lck");
    ws_log_exc.rename("ws_log_exc");
    ws_log.delete();
    ws_log_exc.sample(30).mirror().leftfetchjoin(ws_log_stamp).table(ws_log_txt);
}

# print transaction sequence
PROC ws_log_print(lng t) : void {
   var seq := ws_log_tid.ord_uselect(t).mirror().leftfetchjoin(ws_log_stamp);
   var lo := seq.fetch(0);
   var hi := seq.fetch(seq.count() - 1);
   var lst := ws_log_stamp.ord_select(lo,hi);
   table(lst, [substitute](ws_log_txt,str(t),"=================", false));
   table([/]([-](seq,lo),1000LL), ws_log_txt);
}


@= ws_nme
    .append(toLower("@1"))
@= ws_tpe
    .append(@4)
@= ws_htp
    .append(@5)
@= ws_ttp
    .append(@6)
@= ws_seq
    .append(@7)
@mil
# get a handle to the global locks
var pf_short := pflock_get(0); # *NOTE* all PROCs _X() (starting with underscore) hold pf_short
var pf_wal   := pflock_get(1); # lock that must be held while writing into the WAL
var pf_free  := pflock_get(2); # lock held while freeing ws-es
var pf_free_held := 0LL;       # contains wsid that has the lock (for robust resource release)

# [coll-lock,barrier] bat for getting non-exclusive/exclusive coll-locks 
var shortlock_barrier := bat(lock,sema,10000).rename("shortlock_barrier"); 
var pf_collbarrier_lock := lock_create(); # proptector for shortlock_barrier bat

# master bat *READS* must protect themselves against *independent* master bat updates, and
# in particular extends. Such BATextends may reallocate the base address of the heaps.
# Thus readers take a nonexclusive lock, and appends (that potentially extend) take
# an exclusive extend-lock. We would preferably have had this per-collection, but for now
# there is just a global lock for this.
var pf_extend := pflock_get(3); # protects the counter
var pf_extend_cnt := 0; # counter is used by nonexclusive acces (first gets barrier, last releases)
var pf_extend_barrier := sema(ptr(pflock_get(7))); # the barrier
var pf_commit_barrier := sema(ptr(pflock_get(8))); # the barrier

# master bat *UPDATES* must be protected against checkpoints (global BBP subcommits + log restart)
# master updates take a non-exclusive pf_chkpt lock; pf_checkpoint() takes an exclusive access
var pf_chkpt := lock_create(); # protects the counter
var pf_chkpt_cnt := 0; # counter is used by nonexclusive acces (first gets barrier, last releases)
var pf_chkpt_barrier := sema_create(1); # the barrier 

# all update bats in the ws (tail-type + name)
const ws_update := bat(int,str)
	.insert(bat,"map_pid")
	.insert(bat,"rid_size")
	.insert(bat,"rid_level")
	.insert(bat,"rid_kind")
	.insert(bat,"rid_prop")
	.insert(bat,"rid_nid")
	.insert(bat,"nid_rid")
	.insert(bat,"attr_qn")
	.insert(bat,"attr_prop")
	.insert(bat,"attr_own")
	.insert(bat,"prop_val")
	.insert(bat,"prop_text")
	.insert(bat,"prop_com")
	.insert(bat,"prop_ins")
	.insert(bat,"prop_tgt")
	.insert(bat,"qn_histogram")
	.insert(bat,"qn_prefix_uri_loc")
	.insert(bat,"qn_uri_loc")
	.insert(bat,"qn_prefix")
	.insert(bat,"qn_uri")
	.insert(bat,"qn_loc")
	.insert(oid,"text")
	.insert(bat,"nid_qn_ins")
	.insert(bat,"nid_qn_del")
	.insert(lng,"modified_nid")
	.insert(oid,"ancestor_nid")
	.insert(oid,"modified_attr")
	.insert(oid,"modified_page")
	.insert(oid,"new_page")
	.insert(oid,"del_page")
	.insert(oid,"deleted_nid")
	.insert(oid,"added_nid")
	.insert(oid,"added_attr")
	.insert(oid,"deleted_text_nid")
	.insert(oid,"added_text_nid")
	# the below are only used for XRPC updates
	.insert(lng,"collected_command")
	.insert(oid,"collected_pre_tgt")
	.insert(oid,"collected_pre_cont_tgt")
	.insert(oid,"collected_attr_tgt")
	.insert(oid,"collected_attr_cont_tgt")
	.insert(str,"collected_replace_strings")
	.insert(str,"collected_rename_qn_uri")
	.insert(str,"collected_rename_qn_prefix")
	.insert(str,"collected_rename_qn_local")
	.insert(oid,"collected_pre_ins")
	.insert(oid,"collected_pre_cont_ins")
	.insert(oid,"collected_attr_ins")
	.insert(oid,"collected_attr_cont_ins")
	.access(BAT_READ).rename("ws_update");

# the bats that get changed using the logger in case of updates reach up until qn_loc
# (i.e. these are the ones that need to be registered in the logger)
const logger_bats := [+]("_", ws_update.tmark(0@0).slice(0,20)).rename("logger_bats");

PROC pf_checkpoint(BAT[void,str] commitBAT, bit start_trans) : bit 
{
    if (start_trans and (count(commitBAT) = 0)) return false;
    commitBAT := commitBAT.access(BAT_WRITE);
    commitBAT.append("collection_name");
    commitBAT.append("collection_size");
    commitBAT.append("doc_name");
    commitBAT.append("doc_location");
    commitBAT.append("doc_collection");
    commitBAT.append("doc_timestamp");
    commitBAT.append("uri_lifetime");
    if (start_trans)
        lock_set(pf_wal);
    var err := CATCH({ if (start_trans) log_trans_start(pf_logger);
                       log_trans_end_extra(pf_logger, commitBAT); });
    if (start_trans)
        lock_unset(pf_wal);
    if (not(isnil(err))) {
        var msg := "XQDY0062: checkpoint failed (in pf_checkpoint), query aborted.\n";
        if (not(isnil(err))) msg := err + msg;
        ERROR(msg);
    }
    return true;
}

# some crucial actions; require a server restart to guarantee data sanity
PROC pf_assert(str err, str msg) : void
{
    if (not(isnil(err))) { 
        printf("%s\n!ERROR: %s; please restart Mserver.\n", err, msg); 
        quit(); 
    }
}

PROC pf_logflush(lng wsid) : bit {
    if (not(pflock_trycommit()))
        return false;
    var ws_logtime := usec(); 
    lock_set(pf_wal);
    sema_down(pf_chkpt_barrier);
    if (ws_log_active) 
        ws_log(wsid, "checkpoint-begin"); 
    pf_assert(CATCH(logger_restart(pf_logger)), "background checkpoint failed");
    if (ws_log_active) 
        ws_log(wsid, "checkpoint-end exec" + str(ws_logtime - usec()));
    sema_up(pf_chkpt_barrier);
    lock_unset(pf_wal);
    logger_cleanup(pf_logger);
    sema_up(pf_commit_barrier);
    return true;
}

PROC commit() : void { # redefine commit to alert illegal usage
    ERROR("global commits are not allowed (using WAL-based commit instead)");
}

PROC exit() : void { # redefine exit to do a checkpoint first
    # exit with clean log makes server restart significantly faster 
    var i := 0;
    while(not(pf_logflush(1LL)) and ((i :+= 1) <= 10)) sleep(1); 
    quit(); 
}


# the logger background thread
var logger_base := 0;           # this var only used by XRPC admin
PROC pf_logmanager() : void {
    logger_base := logger_changes(pf_logger);
    var cnt := 0;
    while(true) {
        var changes := logger_changes(pf_logger);
        if (((cnt :+= 1) % 30) = 0) { 
            cleantmpdir((msec() / 1000LL) - 3600LL);
        }
        #if ((changes > 100000) and (cnt > 30)) { 
        if (cnt > 30) { 
            if (pf_logflush(0LL)) { # only flush if no queries active
                logger_base := changes;
                cnt := 0;
            }
        } 
        sleep(10);  

        # remove working sets of non-busy XRPC requests that timed out or aborted
        lock_set(xrpc_lock);
        CATCH(_ws_xrpc_prune());
        lock_unset(xrpc_lock);
    }
}

# the set of document collections (may only be accessed holding pf_short)
var collection_name;  # bat[oid,str] collection name 
var collection_size;  # bat[oid,lng] collection size in bytes
var collection_zombie := bat(oid,lng).rename("collection_zombie"); 
                      # bat[oid,lng] contains ws still using the now defunct collection (TRANSIENT)


var doc_collection;   # bat[oid,oid] collection-id 
var doc_name;         # bat[oid,str] document name
var doc_location;     # bat[oid,str] document URI
var doc_timestamp;    # bat[oid,timestamp] caching limit (nil if none)
var doc_undo := bat(oid,lng).rename("doc_undo"); 
                      # bat[oid,lng] contains ws that created this document (TRANSIENT)

var uri_lifetime;     # caching rules


# runtime collection acccess control (TRANSIENT)
var colname_runtime  := bat(str,bat).rename("colname_runtime"); 
                        # bat[str,bat] runtime index/lock bat-of-bat 
var colname_pins     := bat(str,lng).rename("colname_pins"); 
                        # bat[str,lng] queries that are using a collection (negative numbers are shreds)
var colname_shredlock:= bat(str,lock).rename("colname_shredlock"); 
                        # bat[str,lng] query that is shredding into a collection 
var colname_locks    := bat(str,lng).rename("colname_locks"); 
                        # bat[str,lng] queries that are using a collection 

var ws_overlaps_ws := bat("ws_overlaps_ws"); 
                        # bat[lng,lng] execution of query X overlapped query Y and they used a common collection

var empty_bat        := bat(void,oid).seqbase(0@0).access(BAT_READ).rename("empty_bat"); 
var empty_page       := bat(void,int,REMAP_PAGE_SIZE).rename("empty_page"); 
                        # empty pages to append onto rid_* master bats
{   # bat[void,int] of size REMAP_PAGE_SIZE counting down from REMAP_PAGE_SIZE-1
    var m := int_nil, i := int(REMAP_PAGE_SIZE);
    var bak := debugmask();
    debugmask(0);
    while((i :-= 1) >= 0) { empty_page.append(nilor(i, m)); }
    debugmask(bak);
    empty_page.seqbase(0@0).access(BAT_READ);
}

# the ws_* bats contain a recipe for creating a new ws
var ws_nme := bat(void,str,WS_SIZE)@:ws(nme)@.seqbase(0@0).access(BAT_READ).rename("ws_nme");
              # name (prefix) of ws entry as persistent bat
var ws_tpe := bat(void,int,WS_SIZE)@:ws(tpe)@.seqbase(0@0).access(BAT_READ).rename("ws_tpe");
              # type of ws entry (often bat)
var ws_htp := bat(void,int,WS_SIZE)@:ws(htp)@.seqbase(0@0).access(BAT_READ).rename("ws_htp");
              # (if tpe=bat) head-type of nested bat
var ws_ttp := bat(void,int,WS_SIZE)@:ws(ttp)@.seqbase(0@0).access(BAT_READ).rename("ws_ttp");
              # (if tpe=bat) tail-type of nested bat
var ws_seq := bat(void,oid,WS_SIZE)@:ws(seq)@.seqbase(0@0).access(BAT_READ).rename("ws_seq");
              # (if tpe=bat) seqbase of nested bat
var ws_col := ws_tpe.ord_uselect(bat).mirror().leftfetchjoin(ws_nme).rename("ws_col");
              # all ws entries that are real bats, and their names
var ws_dsk := ws_nme.reverse().ord_select(oid(_MAP_PID), oid_nil).reverse().rename("ws_dsk");
              # of those, all persistent bats on disk for a document container , and their names
var ws_mem := [oid]([int](mirror(ws_dsk)).[-](_RID_SIZE - PRE_SIZE)).access(BAT_WRITE);
              # of those, all master non-rid_* bats, and their (copy-on-write) updatable views 
var ws_rid := ws_mem.slice(PRE_SIZE, PRE_NID).rename("ws_rid");
              # of those, all master rid_* bats, and their (remapped) pre_* views
    ws_mem := ws_mem.slice(NID_RID, count(ws_mem)).rename("ws_mem");


# scans for empty collections, and produces a list of kill-bats (fast - done inside the short lock)
PROC _collection_cleanup() : BAT[str,str]
{
    var commitBAT := bat(str,str);
    var cnt := {count_no_nil}(reverse(doc_collection).leftjoin(doc_name), collection_name);
    var del := cnt.uselect(0).mirror().join(collection_name);
    collection_zombie.delete();
    del@batloop() {
        var coll_oid := $h;
        var doc_oids := doc_collection.uselect(coll_oid);

        # only do something if this collection is not pinned!!
        var users := reverse(colname_pins).uselect($t);
        if (count(users) = 0) {
            # delete all documents from the collection
            doc_collection.delete(doc_oids);
            doc_name.delete(doc_oids);
            doc_location.delete(doc_oids);
            doc_timestamp.delete(doc_oids);

            # delete empty collection 
            if (colname_runtime.exist($t)) {
                var runtime := colname_runtime.find($t);
                var coll_rlock := reverse(runtime).fetch(RT_LOCK_FREELIST);
                var coll_wlock := reverse(runtime).fetch(RT_NID_FREELIST);
                coll_wlock.lock_destroy();
                coll_rlock.lock_destroy();
                colname_runtime.delete($t);
            }
            colname_shredlock.reverse().select($t)@batloop() lock_destroy($h);
            colname_shredlock.delete($t);
            colname_pins.delete($t);
            colname_locks.delete($t);

            collection_name.delete(coll_oid); 
            collection_size.delete(coll_oid);
            commitBAT.insert([+](str(int(coll_oid)), ws_dsk.reverse().mirror()));
            commitBAT.insert("_qn_nid", +(str(int(coll_oid)), "_qn_nid"));
            commitBAT.insert("_vx_hsh_nid", +(str(int(coll_oid)), "_vx_hsh_nid"));
         } else {
            # recall the ws-ids that blocked deletion of a collection 
            collection_zombie.append([abs](reverse(project(users,$h))));
         }
    }
    return commitBAT.access(BAT_READ); # return a list of garbage bats
}

# make a list of victim bats transient, the commit (slow - done outside the short lock)
PROC collection_cleanup(BAT[str,str] delBAT) : void
{
    var cnt := count(delBAT);
    if (bit(cnt)) {
        var commitBAT := bat(void,str,cnt);
        # bats that are likely known by the logger
        var logger_delBAT := logger_bats.join(delBAT);
        # bats that are definitely not known by the logger
        var b := tdiff(delBAT, logger_delBAT).tmark(0@0).access(BAT_WRITE);
        # first remove the bats under control of the logger 
        lock_set(pf_wal);
        # bats not known by the logger after all (probably result of doc() queries of unknown documents)
        var d := [logger_find_bat](pf_logger, logger_delBAT).ord_uselect(0).mirror().join(logger_delBAT);
        # bats definitely known by the logger
        logger_delBAT := logger_delBAT.tdiff(d);
        # bats definitely unknown to the logger
        b.append(d);
        log_trans_start(pf_logger);
        if (logger_delBAT.count() > 0) {
            [logger_del_bat](pf_logger, [logger_find_bat](pf_logger, logger_delBAT));
            [log_bat_transient](pf_logger, logger_delBAT);
        }
        b@batloop() {
            if (isnil(CATCH(persists(bat($t), false)))) commitBAT.append($t); 
        }
        var err := CATCH(pf_checkpoint(commitBAT, false));
        lock_unset(pf_wal);
        if (not(isnil(err))) ERROR(err);
    }
}

@+ locking approach
@T
MonetDB/XQuery uses optimistic concurrency control with snapshot
isolation, so updates do not use locks as such. Instead, conflicting 
queries are aborted at commit time (see later in "Query Types"). 

Still, some locking still needs to be done, to prevent crashes.
Monet BATs may never be modified (e.g. from MIL) while other  
concurrent queries are reading them (e.g. from MIL). This holds
both from the meta-bats, as well as the data bats.

The goal is to allow as much concurrency as possible (after all, the
strength of optimistic CC).

@- short lock

There is a short lock (pf_short) that protects access to the collection_*,
doc_* and colname_* meta-tables. Note that the pf:add-doc(), pf:del-doc()
as well as the fn:doc() function (which does implicit shredding and caching)
modify these tables. All types of queries read them. Thus all reads and
writes to these tables need to be locked, with the short lock. 

It is good practice to carry out all work while holding the short lock inside 
a CATCH(). Otherwise a run-time error may leave the short lock taken and the 
whole system blocked forever.  Thus, all MIL PROCs X() that need the lock, 
have a worker function _X() that is called inside the locked execution:

PROC X() : void {
    lock_set(pf_short);
    var err := CATCH(_X());
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
}

As a general convention all MIL PROCs _X(), i.e. whose name starts with an
underscore, assume the pf_short lock is taken.

@- collection locks and the extend lock 

Apart from the global short lock, each XML collection (i.e. document container) 
has two coll_locks: long and short. 
These locks can be found via the colname_runtime BAT[str,bat].  Its head contains 
collection names. Its tail a so-called 'runtime' BAT[lock,bat]. 
The first entry (named RT_LOCK_FREELIST) in this runtime-BAT is 
[coll_shortlock,pagefree], and the second named RT_NID_FREELIST) contains
[coll_longlock,freenids].
The tails of these BUNs hold free-lists for pages and NIDS, respectively.

The other entries in the runtime-BAT relate to indices on the collection.
More information can be found in the QN_NID paragraph. Also, the StandOff
extensions use entries in the runtime-BAT.

The coll_short lock is the most critical lock that guarantees that
no changes at all (except new pages allocated to other transactions).
occur. It is taken by read-transactions when they want to read stable
master bats. It must also be taken by update transactions that modify
stable portions of the master bats.

Writers just lock coll_lock short to get exclusive access. Readers can 
follow a non-exclusive mechanism wrt to other readers. This policy
is used when opening collections (the lock is needed while the remap/rcopy
isolation is done on the master bats). The non-exclusive variant releases
the lock immediately, but after insert an entry [shortlock,nil] in the
shortlock_barrier bat, and remove this when they are done. Exclusive writers 
examine this BAT, and if [shortlock,nil] is present, create a new
barrier and insert this [shortlock,sema] in the BAT, and block on the 
semaphore. The last reader exiting wakes up all these semaphores.

The long lock is taken by updating actions only. Holding it prevents
other updates from proceeding on a commit to that collection. However, 
it does allow read-transactions to access the master BATs freely.
In the commit sequence, the long-lock is taken during the logging
phase. After that succeeds, the short lock is added, and then the
mater BATs are updated.

So, the collection shortlock should be used to get exclusive access 
to the master bats of a collection. This is required when:

- a collection is opened and isolated copies of the master bats need to 
  be made. This includes the time taken for any nsloc index construction,
  which must be done on the master copies.

- while an incremental shred operation is *extending* the bat collection 
  -- though it is not needed while shredding in general.

- while playing the update tape, and determining which updates
  to make, certain append-only tables are extended (qn_*,  prop_*).

- while applying the updates of a committed query to the master bat.
  Note that in a commit, first all locks of all documents must be acquired
  (in ascending order - to prevent deadlock), then all changes committed. 

There are some situations where access to the master BATs is allowed
without acquiring any of these collection locks. That for pages 
not part of the stable master bats (i.e. newly allocated pages).
This access (reading and writing in newly allocated RID_* pages during 
updates) is protected by pf_extend_[un]protect(). It sets a non-exclusive
counting lock. Its function is to prevent base reallocation. 

Similar to the short_lock PROC naming convention, __X() PROCs, i.e.
with double underscore are those that execute while holding a collection
lock.

@- shred lock 

Note that we will not use the collection lock while shredding (pf:add-doc())
continuously, only when extending the master bats. The code of shredding is 
carefully screened only to touch *new* parts of the BATs thus not affecting 
the master copies in places that are shared with ongoing queries. BAT-extends 
must be locked with the collection lock, as these may lead to base-relocations,
that affect other users of the master bats.

However, we must ensure that only one shred into the same collection is
happening at a time, as both need exclusive append access. For this
purpose, an extra "shred-lock" in the colname_shredlock BAT[str,lock] is taken 
by shred operations.

This "shred-lock" thus protects the process of making the data bats longer.
As such, also the insert operations that allocate a new page (ws_newpage) 
or a new set of NIDs (ws_newnids) must acquire this lock. These two operations
always request a larger batch of (pages, NIDs) in one go, adding them to a 
free-list. Thus, many insert actions will actually not need to take the 
shred-lock (as they can get their new pages/NIDs from a freelist).

@- deleting documents 

Deleting documents simply marks them as unused by changing document and
collection names to nil. This ensures that newly entering queries will
never find and thus use those documents. When the last document in a 
collection is deleted, the collection should be removed. Only then, 
the BAT resources may be freed.

Deleting documents is separated in two phases, where the setting 
to nil of the name is the critical part. This action does not throw
away any data, so can be easily rolled back. A single query can
delete many documents and shred many new ones, but should commit
atomically. If some action fails, everything is rolled back.
The "doc_undo" meta bat is used for determining what has to be rolled back.

After a successful document management query, empty document collections 
are pruned. That is, they are removed from the meta-tables, and 
their data tables physically removed.  This may not be possible if there
are concurrent users of documents in the collections (these 
transactions must have started before the document name was 
set to nil, or will shred a new document into the now empty collection).
Those collections are not pruned, but when the transaction that
blocked it finishes, it retries empty collection pruning in its 
ws_destroy() sequence.
The "collection_zombie" bat is used for to trigger collection pruning
when such transactions finish.

@- collection pins and ws-IDs

To detect concurrent use of collections, all document-using functions 
(fn:doc() but also pf:add-doc()) put a so-called 'pin' on the collection 
they are intending to manipulate.  These pins are gathered in the BAT[str,lng] 
colname_pins (it contains document names and ws-IDs). 
This BAT is also used for recording which queries currently hold 
a shred-lock. This is marked by storing the *negated* ws-id. 

ws-ID: a transaction (query) is characterized by its ws-id. The 
ws-id is a unique OID, and is kept as the name of the working set 
bat[void,bat]. ws-IDs are globally and temporally unique, i.e. 
we can talk about a ws-ID even after the transaction has finished.

The collection pins record which transactions (wsbats) are using 
which bats. This information is deleted only at the end of
the transaction. 

@- conflict detection & isolation 

This relates to the _ws_precommit() function. 

From the collection pins we also derive a 'ws_overlaps_ws'
BAT that captures which update transactions executed concurrently
and had some common XML collection open (in common). These
transactions potentially conflict with each other (which
may lead to an abort on one of them).

For conflict detection, we remember all page-ids in the rid_* 
table that a transaction modified, as well as all attribute 
oids (the other tables, qn_* and prop_* are append-only so
never cause any conflicts).

(note that for structural updates in a page, all pages containing 
 ancestors (whose size field will be affected by the update) 
 are *not* considered modified. Concurrent ancestor size-changes 
 are compensated at commit time by keeping track of the difference 
 in size, caused by a transaction, rather than absolute values.)

A related issue is isolation. At commit time, changes from
the update list (that were successfully logged in the WAL) are
applied to the master BATs. Concurrent queries should be
isolated from these changes in the masters. For this purpose, 
we touch the affected pages in their copy-on-write memory maps
first. 

To establish which transactions (working sets) need to be 
isolated, we again use the colname_pins.
The isolating may take significant time, therefore it
is done outside the short lock, but inside a free-lock.
This free-lock is also taken during the ws_destroy.

@- WAL, subcommit and checkpoints

For committing, queries write into the WAL. The WAL is therefore also
protected by a separate pf_wal WAL lock (similar to, but separate from the 
short lock).

However, for shredding, we use checkpoints. A partial checkpoint 
causes certain BATs to be saved to disk; all WAL entries for that BAT
before the subcommit time can be ignored.

Additionally, there is a checkpoint background thread in the logger, that 
performs periodical checkpoints, and flushes the old logfiles.  

The points in the code were the master bats are updated must be 
protected against the checkpoint thread. That is, the checkpoint 
thread should commit a consistent master image to disk. For this
purpose, there is (similar to the extend lock) a nonexclusive/exclusive
checkpoint lock.

The places where master bats of some collection are updated
take a nonexclusive checkpoint lock. The checkpointing thread
itself takes an exclusive lock.

The checkpoint locking structure consists of:
- a counter (pf_chkpt_cnt) that counts how many nonexclusive users are 
  inside the critical section.
- a lock (pf_chkpt) that protects the counter
- a semaphore (pf_chkpt_barrier) that is taken by the first nonexclusive 
  user and released by the last. Exclusive checkpoint locks always take 
  the semaphore.


@+ Query types

Besides read-only queries, there are two other types: updating
queries (that rely on a WAL commit) and document management
queries that shred/delete documents (and rely on a checkpoint).

Because of their reliance on different commit mechanism, updating
queries cannot do document management. Otherwise we would not be
able to provide an atomic commit. 

@- Updating Queries

An update query goes through the following phases:
(0) ws_create(2), indicating an updating query. If the parameter value 
    is 3, it is a rerun of an aborted transaction (in serial mode). 
(1) query execution. this produces an update tape.
(2) ordering and playing the update tape for each kind of update
    (delete, insert, rename). This produces intentional updates
    aka "delta bats" to the (rid_*, nid_* and attr_* master tables),
    but *directly* carries out modifications to the qn_*,attr_* and prop_* 
    masters.  For the latter direct modifications coll_shortlocks
    are used temporarily (i.e. lock, modify, unlock). 
(3) taking the long coll_lock for all affected containers (in a fixed order)
    _ws_precommit() then performs conflict detection:
    this will trigger a MIL ERROR, which will result in an abort.
(4) All updates are logged in the WAL and a commit record is written. 
    This step is protected by a separate global WAL-lock.
    Reaching this point determines transaction commit/abort.
    Note that computing the deltas to the rid_size column is special,
    because for affected ancestors, we keep not the new value, but the difference. 
    So, the final sizes have to be computed when logging into the WAL
    by we reading the current value in the master, and adding the difference kept 
    in the delta-bat, and logging those deltas.  Thanks to this, we can keep 
    the affected ancestors out of the modified-page administration (otherwise, 
    the page containing the root node would always be modified, inhibiting any concurrency).
(5) Upgrade the long-locks to a both lock (i.e. taking the short lock also)
     __ws_isolate() then isolates all concurrent
    transactions from the intended updates (VM shadow paging).
(6) Update all rid_*, nid_* and attr_* masters with the 
    intentional updates (delta bats) produced in phase 2 (and 4).
(7) __ws_postcommit() maintains the index structures.
    Finally, both coll_locks (long and short) are released.
(8) ws_destroy(). In case of an error in any of the stages >0, 
    we get here.  It releases all locks (these actions are protected 
    by pf_short, the only lock that must never remain taken when 
    an error occurs)

Optimistic CC may suffer from high abort rates, if many updating queries 
modify the same page. With our page-wise deadlock detection (and pages
being rather large), this is a serious threat to update throughput.

This threat is reduced somewhat by adaptively handling transaction aborts.
When a transaction aborts (due to concurrent conflict only), we immediately 
reschedule the transaction it and run it in serial mode.

A rerun of an update transaction starts with ws_create(3); so it is easily
recognized as such.

We actually schedule a "serial convoy" of e.g. size 5.  This serial convoy 
means that the next 5 incoming update transactions will be executed in 
serial mode.  This policy ensures that conflict aborts never bother the user, and
highly "conflicting" query loads avoid putting the system in "thrashing" mode.

(see the pflock_* COMMANDS in pf_support.mx)

@- Document Management Queries 
A read-only query executes the following phases:
(0) ws_create(1)
(1) query execution. this produces an statement tape.
(2) ordering and playing the statement tape for each kind of 
    statement (first del_doc, then add_doc).
(3) pf_checkpoint. All affected document containers, plus the
    meta bats (collection_*, doc_*) are flushed to disk.
(4) ws_destroy()

@- Read-only Queries 
A read-only query just executes the following phases:
(0) ws_create(0)
(1) query execution. 
(2) ws_destroy()

Note that for all types of queries, phase (1) "query execution"  
may also trigger checkpoints. That is, each occurrence of a fn:doc() 
function may trigger a checkpoint, if the document (identified by URI) 
was not in the database and the caching policy indicates that 
the document should be cached. 

Note that we loop-lifted fn:doc() (ws_opendoc), in the sense that 
each fn:doc() application will construct exactly one single collection 
with all documents that had to be shredded in the loop. A checkpoint
is done only if these documents are exported visibly (to other queries) 
in the database as cached URIs. This is determined by the caching rules. 
This is only done if *all* documents in the loop are cachable. Otherwise, 
the shredded collection is a temporary collection that does not need a 
checkpoint. 

@mil
PROC _shredlock_set(str colname, 
                    lng wsid) : void
{
    # xlocks use a string collection key as it may not yet exist in the table
    var shred_lock;
    if (colname_shredlock.exist(colname)) 
        shred_lock := colname_shredlock.find(colname);
    else
        shred_lock := lock_create();
    colname_shredlock.insert(colname, shred_lock);
    lock_unset(pf_short);
    lock_set(shred_lock); # can take a long time, do so with short lock released 
    lock_set(pf_short);
    colname_pins.insert(colname, -(wsid));
}

PROC _shredlock_unset(str colname, 
                      lng wsid) : void
{
    # xlocks use a string collection key as it may not yet exist in the table
    var shred_lock := colname_shredlock.find(colname);
    lock_unset(shred_lock);
    colname_pins.delete(colname, -(wsid));
    colname_shredlock.delete(colname, shred_lock);
    if (not(colname_shredlock.exist(colname))) lock_destroy(shred_lock); 
}

PROC extend_protect(BAT[void,bat] ws,
                    oid cont) : void
{
    lock_set(pf_extend);
    if (pf_extend_cnt = 0) sema_down(pf_extend_barrier);
    pf_extend_cnt :+= 1;
    lock_unset(pf_extend);
    ws.fetch(CONT_LOCKED).append(pf_extend);
}

PROC extend_unprotect(BAT[void,bat] ws,
                      oid cont) : void
{
    lock_set(pf_extend);
    if ((pf_extend_cnt :-= 1) = 0) sema_up(pf_extend_barrier);
    lock_unset(pf_extend);
    ws.fetch(CONT_LOCKED).reverse().delete(pf_extend);
}

PROC checkpoint_protect(BAT[void,bat] ws) : void
{
    lock_set(pf_chkpt);
    if (pf_chkpt_cnt = 0) sema_down(pf_chkpt_barrier);
    pf_chkpt_cnt :+= 1;
    lock_unset(pf_chkpt);
    ws.fetch(CONT_LOCKED).append(pf_chkpt);
}

PROC checkpoint_unprotect(BAT[void,bat] ws) : void
{
    lock_set(pf_chkpt);
    if ((pf_chkpt_cnt :-= 1) = 0) sema_up(pf_chkpt_barrier);
    lock_unset(pf_chkpt);
    ws.fetch(CONT_LOCKED).reverse().delete(pf_chkpt);
}

PROC coll_lock_set(BAT[void,bat] ws, 
                   oid cont, 
                   int mode, 
                   str nme) : void
{
    if (bit(cont)) {
        var ws_logtime := usec();
        if (bit(mode and COLL_LONGLOCK)) {
            var coll_longlock := reverse(ws.fetch(CONT_RUNTIME).fetch(cont)).fetch(RT_NID_FREELIST);
            ws.fetch(CONT_LOCKED).append(coll_longlock);
            lock_set(coll_longlock);
            if (ws_log_active) 
                ws_log(ws, nme + " (coll_lock_set) longlock" + str(ws_logtime - (ws_logtime := usec())));
        }
        if (bit(mode and COLL_SHORTLOCK)) {
            var coll_shortlock := reverse(ws.fetch(CONT_RUNTIME).fetch(cont)).fetch(RT_LOCK_FREELIST);

            lock_set(coll_shortlock);
            if (ws_log_active)
                ws_log(ws, nme + " (coll_lock_set) shortlock" + str(ws_logtime - (ws_logtime := usec())));
            lock_set(pf_collbarrier_lock);
            if (shortlock_barrier.exist(coll_shortlock,sema_nil)) { # there are readers in ws_opencoll!!
                var barrier := sema_create(0);
                shortlock_barrier.insert(coll_shortlock,barrier); 
                while(shortlock_barrier.exist(coll_shortlock,sema_nil)) {
                    lock_unset(pf_collbarrier_lock);
                    lock_unset(coll_shortlock);
                    sema_down(barrier); # wait for the readers
                    if (ws_log_active)
                        ws_log(ws, nme + " (coll_lock_set) sema" + str(ws_logtime - (ws_logtime := usec())));
                    lock_set(coll_shortlock);
                    lock_set(pf_collbarrier_lock);
                    if (ws_log_active)
                        ws_log(ws, nme + " (coll_lock_set) shortlock-redo" + str(ws_logtime - (ws_logtime := usec())));
                }
                shortlock_barrier.delete(coll_shortlock,barrier); 
                sema_destroy(barrier); 
            }
            lock_unset(pf_collbarrier_lock);

            ws.fetch(CONT_LOCKED).append(coll_shortlock);
            checkpoint_protect(ws);
            if (ws_log_active)
                ws_log(ws, nme + " (coll_lock_set) checkpoint_lock" + str(ws_logtime - usec()));
        }
    }
}

PROC coll_lock_unset(BAT[void,bat] ws, 
                     oid cont, 
                     int mode, 
                     str nme, lng ws_logtime) : void
{
    if (bit(cont)) {
        if (bit(mode and COLL_SHORTLOCK)) {
            checkpoint_unprotect(ws);
            var coll_shortlock := reverse(ws.fetch(CONT_RUNTIME).fetch(cont)).fetch(RT_LOCK_FREELIST);
            ws.fetch(CONT_LOCKED).reverse().delete(coll_shortlock);
            lock_unset(coll_shortlock);
        }
        if (bit(mode and COLL_LONGLOCK)) {
            var coll_longlock := reverse(ws.fetch(CONT_RUNTIME).fetch(cont)).fetch(RT_NID_FREELIST);
            ws.fetch(CONT_LOCKED).reverse().delete(coll_longlock);
            lock_unset(coll_longlock);
        }
        if (ws_log_active)
            ws_log(ws, nme + " (coll_lock_unset) exec" + str(ws_logtime - usec()));
    }
}


@+ working sets and ws-IDs

The working set is the runtime context of a query. For each collection, two
sets of bats are visible: updatable bats and masters. The masters
have names in the working set that start with an underscore.

When a working-set is destroyed, we take care to release all
collection locks and pins. These may be open as the query may have terminated
early with an error (these are caught and ws_destroy() is executed always).

ws-IDs are persistent references to working sets. As a working set is
a BAT that is freed and recycled after commit, is is not persistent.
The combination of a *unique* ID (int) with a working-set ID (bat) is
fit into a lng and is used as the ws-ID.
@mil
PROC ws_new(oid id, 
            int tpe, 
            int htp, 
            int ttp, 
            oid seq) : BAT[void,any]
{
    if ((ttp != void) and (int(id) < PID_MAP)) { # a bat of bats (persistent or view)
        var b := bat(htp,ttp,10000);
        if (not(isnil(seq))) b.seqbase(seq); 
        return bat(void,bat).seqbase(PRE_BASE).append(b);
    } 
    return bat(void,tpe).seqbase(PRE_BASE); # a constant bat
}

PROC ws_cache_expr(bat ws, str id) : bit {
    return not(ws.fetch(CACHE_ID).texist(id));
}

PROC ws_cache_get(bat ws, str id) : bit {
    var idx := reverse(ws.fetch(CACHE_ID)).find(id);
    inplace(ws.fetch(CACHE_LRU), idx, usec(), true);
    return ws.fetch(CACHE_VAL).fetch(idx);
}

PROC ws_cache_put(bat ws, str id, bat val) : void {
    ws.fetch(CACHE_ID).append(id);
    ws.fetch(CACHE_VAL).append(val);
    ws.fetch(CACHE_SIZE).append(sum([batsize](val)));
    ws.fetch(CACHE_LRU).append(0LL);
#print(ws.fetch(CACHE_ID), ws.fetch(CACHE_VAL), ws.fetch(CACHE_SIZE), ws.fetch(CACHE_LRU));
}

PROC ws_cache_end(bat ws) : void {
    if (count(ws.fetch(CACHE_SIZE)) = 0) return; 
    var totsize, now := usec(), prune := false;
    while ((totsize := sum(ws.fetch(CACHE_SIZE))) > xquery_cache_lim) {
        var lru := min(ws.fetch(CACHE_LRU));
        var idx := reverse(ws.fetch(CACHE_LRU)).find(lru);
        ws.fetch(CACHE_LRU).inplace(idx,LNG_MAX, true);
        ws.fetch(CACHE_SIZE).inplace(idx,0LL, true);
        prune := true;
    }
    if (prune) {
        var pivot := ws.fetch(CACHE_SIZE).uselect(1LL,lng_nil).hmark(0@0);
        ws.inplace(oid(CACHE_ID), pivot.leftfetchjoin(ws.fetch(CACHE_ID)).access(BAT_APPEND), true);
        ws.inplace(oid(CACHE_VAL), pivot.leftfetchjoin(ws.fetch(CACHE_VAL)).access(BAT_APPEND), true);
        ws.inplace(oid(CACHE_LRU), pivot.leftfetchjoin(ws.fetch(CACHE_LRU)).access(BAT_APPEND), true);
        ws.inplace(oid(CACHE_SIZE), pivot.leftfetchjoin(ws.fetch(CACHE_SIZE)).access(BAT_APPEND), true);
    }
#print(ws.fetch(CACHE_ID), ws.fetch(CACHE_VAL), ws.fetch(CACHE_SIZE), ws.fetch(CACHE_LRU));
}


# check the working-set cache for multi-request XRPC queries (repeatable reads)
# returns a integer "xrpc" indexing into xrpc_* bats (idx = abs(xrpc) - 1);
#  0 = no xrpc ws caching needed, 
# >0 = ws is already cached at idx
# <0 = create new ws and cache it at idx
PROC _ws_xrpcget() : int
{
    var xrpc := 0;
    xrpc_seqnr := (xrpc_querynr :+= 1LL);
    xrpc_coord := false;
    if (xrpc_mode.search("repeatable") >= 0) {
        xrpc := -(int(count(xrpc_qids))+1); # default: create a new ws
        if (xrpc_qid = "") {
            xrpc_qid := xrpc_hostport + ":" + str(xrpc_seqnr); #init
            xrpc_coord := true;
        } else if (xrpc_qids.texist(xrpc_qid)) {
            xrpc := int(reverse(xrpc_qids).find(xrpc_qid)); # reuse an old ws
        }
    }
    return xrpc;
}

# create a new ws if necessary (res <= 0)
PROC _ws_new(int xrpc, oid idx, int update) : BAT[void,bat]
{
    var ws;
    if (xrpc > 0) {
        ws := xrpc_wsbats.find(idx);
        if (xrpc_mode.search("cache") >= 0) { # extend lifetime of session
            var timeout := 0LL; 
            if (bit(xrpc_timeout)) timeout := usec() + *(1000LL, xrpc_timeout);
            xrpc_timeouts.inplace(idx, timeout);
            xrpc_statuses.inplace(idx, "reserved");
            if (xrpc_mode.search("use") < 0) return ws;
        }
        # use a copy of the ws so it is not affected by concurrent queries; give it a private cache
        ws := [access]([copy](ws), BAT_WRITE);

        # the tempdoc container must be deep copied, as read-only queries change it
        var temp_doc_cont := [access]([copy]([fetch](ws.slice(0,ATTR_PROP), WS)), BAT_WRITE);
        temp_doc_cont.mark(oid(MAP_PID)).tmark(0@0).leftfetchjoin(ws).[inplace](WS, temp_doc_cont.tmark(0@0));
        temp_doc_cont.mark(oid(_MAP_PID)).tmark(0@0).leftfetchjoin(ws).[inplace](WS, temp_doc_cont.tmark(0@0));
        temp_doc_cont := [access]([copy]([fetch](ws.slice(QN_NID,ATTR_OWN_SHARED), WS)), BAT_WRITE);
        temp_doc_cont.mark(oid(QN_NID)).tmark(0@0).leftfetchjoin(ws).[inplace](WS, temp_doc_cont.tmark(0@0));
        ws.fetch(ATTR_OWN_PRIVATE).inplace(WS, ws.fetch(ATTR_OWN).fetch(WS));
    } else {
        ws := [ws_new](mirror(ws_tpe), ws_tpe, ws_htp, ws_ttp, ws_seq);
        if (xrpc < 0) {
            var wslock := lock_create(); 
            xrpc_qids.append(xrpc_qid);
            xrpc_timeouts.append(usec() + *(1000LL, xrpc_timeout));
            xrpc_wsbats.append(ws);
            xrpc_locks.append(wslock);
            xrpc_statuses.append("reserved");
            xrpc_mode := "cache-repeatable";
        }
    }
    var id := and(lng(newoid(1)), 2147483647LL);
    var wsid := <<(id, 32) + (lng(update) << 30) + lng(ws);
    ws.access(BAT_READ).rename(str(wsid));
    return ws;
}

PROC ws_end(BAT[void,bat] ws, str err) : int 
{
    ws_log_wsid := ws_id(ws);
    if (not(isnil(err))) ws_log(ws, err);
    if ((xrpc_qid = "") or (xrpc_mode = "use-cache-repeatable")) {
        ws_destroy(ws);
    } else {
        if (count(ws.fetch(PRE_SIZE).fetch(WS)) > 1000000) { # limit max tempdoc copy time to ~100ms
            xrpc_timeout := 0LL; # if tempdoc container gets too big, we have to kill the ws
        } else {
            ws_cache_end(ws); # apply LRU to the cached subsequences
        }
        lock_set(xrpc_lock);
        err := CATCH(_ws_xrpc_end(xrpc_qid, err));
        lock_unset(xrpc_lock);
        if (not(isnil(err))) ERROR(err);
    }
    # do not auto-restart XRPC updates (deadlock danger)
    if (xrpc_method != "") return 2;
    return 1;
}

PROC ws_create(int update) : BAT[void,bat]
{
    if (bit(and(update,2)) and (xrpc_mode.search("cache") >= 0)) {
        if (xrpc_mode.search("use") >= 0) ERROR("ws_create(): cannot update in a pf:session-use session");
        xrpc_timeout := 0LL; # updates in a session terminate the session
    }

    # NOTE: use pre-query MIL variables xrpc_qid/xrpc_timeout to possibly re-use an existing ws
    lock_set(xrpc_lock);
    var wslock := lock_nil, ws, xrpc := 0, idx, err := CATCH(ws := _ws_new(xrpc := _ws_xrpcget(), idx := oid(abs(xrpc)), update));
    if (xrpc != 0) wslock := xrpc_locks.find(idx);
    lock_unset(xrpc_lock);
    if (not(isnil(err))) ERROR(err);

    # each xrpc request must lock its ws; read-only use directly unlocks again
    if (not(isnil(wslock))) {
        lock_set(wslock);

        lock_set(xrpc_lock);
        var aborted := (xrpc_statuses.find(idx) = "abort");
        if (aborted) {
            lock_unset(wslock);
        } else if (xrpc_mode = "use-cache-repeatable") {
            lock_unset(wslock);
            xrpc_statuses.inplace(idx, "wait");
        } else {
            xrpc_statuses.inplace(idx, "exec");
        }
        lock_unset(xrpc_lock);
        if (aborted) ERROR("ws_create: session was aborted by a previous query"); 
    }

    if (bit(and(update,2))) {
        # for update queries, create additional bats that will hold the deltas
        var tpe := ws_update.hmark(oid(WS_SIZE));
        var upd := [new](oid,tpe);
        var unq := [=](tpe,bat);
        [set](mirror(unq.uselect(false)).leftfetchjoin(upd), true);
        [key](mirror(unq.uselect(true)).leftfetchjoin(upd), true);
        ws := ws.access(BAT_WRITE).insert(upd).access(BAT_READ);
    }
    if (xrpc > 0) return ws;

    # only instantiates the default views of the ws-bats (not the master bats)
    ws.fetch(PID_MAP).append(ws.fetch(MAP_PID).fetch(WS));
    ws.fetch(_MAP_PID).append(ws.fetch(MAP_PID).fetch(WS));
    ws.fetch(ATTR_OWN_SHARED).append(empty_bat);
    ws.fetch(ATTR_OWN_PRIVATE).append(ws.fetch(ATTR_OWN).fetch(WS));

    # the temporary document container has some bogus values here
    ws.fetch(CONT_COLL).seqbase(0@0).insert(0@0,oid_nil);
    ws.fetch(CONT_NAME).seqbase(0@0).insert(0@0,str_nil);
    ws.fetch(CONT_RUNTIME).seqbase(0@0).insert(0@0,runtime(lock_nil,lock_nil));
    ws.fetch(OPEN_DOCID).append(WS);
    ws.fetch(OPEN_CONT).append(WS);
    ws.fetch(OPEN_NAME).append("");

    # add empty region index entries
    ws.fetch(REGION_PRE).append(empty_bat);
    ws.fetch(REGION_START).append(empty_bat);
    ws.fetch(REGION_END).append(empty_bat);

    # add a (bogus) collection root pre=0@0 to the temporary document container
    ws.fetch(PRE_SIZE).fetch(WS).append(int_nil).append(0);
    ws.fetch(PRE_LEVEL).fetch(WS).append(chr(-2)).append(chr(-1));
    ws.fetch(PRE_PROP).fetch(WS).append(oid_nil).append(oid_nil);
    ws.fetch(PRE_KIND).fetch(WS).append(COLLECTION).append(DOCUMENT);
    ws.fetch(PRE_NID).fetch(WS).append(oid_nil).append(oid_nil);
    ws.fetch(PRE_CONT).fetch(WS).append(oid_nil).append(oid_nil);
    ws.fetch(NID_RID).fetch(WS).append(0@0).append(1@0);
    ws.fetch(FRAG_ROOT).fetch(WS).insert(0@0,1@0); # first root is 1@0

    # add identical bat references as 'master' bats just to make the transient container complete  
    mirror(ws_rid).leftfetchjoin(ws).[insert](0@0, [fetch](ws_rid.leftfetchjoin(ws), 0));
    mirror(ws_mem).leftfetchjoin(ws).[insert](0@0, [fetch](ws_mem.leftfetchjoin(ws), 0));

    var wsid := ws_id(ws);
    pflock_begin(wsid);

    if (ws_log_active)
        ws_log(ws, "===================== START TRANSACTION update-" + str(update));
    return ws;
}


# retrieve wsid from name of BAT ws 
PROC ws_id(BAT[void,bat] ws) : lng
{
    return lng(bbpname(ws));
}

PROC old_sum(BAT[any,lng] b) : lng 
{
        var s := sum(b);
        if (isnil(s)) {
                return lng(0);
        } else {
                return s;
        }
}

PROC _ws_free(lng wsid) : BAT[void,any]
{
    # remove the dependencies of this query (modifies ws_overlaps_ws) 
    pflock_end(wsid); 
   
    # check for failed doc_adds (should happen infrequently - we don't care about performance)
    var del := doc_undo.uselect(wsid).project(str_nil);
    if (count(del) > 0) _del_doc_replace(del, del); # delete them again

    # release all claimed locks (error recovery)
    colname_pins.uselect(-(wsid))@batloop() _shredlock_unset($h, wsid);
    reverse(colname_pins).delete(wsid);
 
    # determine whether a cache flush is desired (only count cached bats, those with a timestamp)
    var cursize := old_sum(doc_timestamp.uselect(timestamp_nil,timestamp_nil).mirror().join(collection_size));
    var maxsize := (1024LL * 1024LL) * lng(monet_environment.find("xquery_cacheMB"));
    if (cursize > maxsize) del_doc_base(true, doc_name, false);

    # try to flush the document collection, if we held up removal of a now empty collection
    if (count(collection_zombie.uselect(wsid)) > 0)
        return _collection_cleanup(); # garbage collect empty collections

    return new(str,str);
}

PROC ws_free(lng wsid) : void
{
    var commitBAT; # always make sure not to cause errors while holding the lock
    if (pf_free_held != wsid) lock_set(pf_free);
    lock_set(pf_short);
    pf_assert(CATCH(commitBAT := _ws_free(wsid)), "query cleanup failed"); 
    lock_unset(pf_short);
    lock_unset(pf_free);
    CATCH(collection_cleanup(commitBAT)); 
}

PROC ws_destroy(any ws) : void {} # dummy that gets called if ws_create() failed
PROC ws_destroy(BAT[void,bat] ws) : void
{
    # free any collection, extend and checkpoint locks that this query holds 
    var coll_locks := ws.fetch(CONT_LOCKED);
    if (bit(count(coll_locks))) {
        coll_locks@batloop() {
            if (isnil($t)) {
                sema_up(pf_extend_barrier); # release extend-write-lock
            } else if ($t = pf_chkpt) {
                # release checkpoint-nonexclusive-lock
                lock_set(pf_chkpt);
                if ((pf_chkpt_cnt :-= 1) = 0) sema_up(pf_chkpt_barrier);
                lock_unset(pf_chkpt);
            } else if ($t = pf_extend) {
                # release extend-nonexclusive-lock
                lock_set(pf_extend);
                if ((pf_extend_cnt :-= 1) = 0) sema_up(pf_extend_barrier);
                lock_unset(pf_extend);
            } else {
                lock_unset($t); # release coll_lock
            }
        }
    }
    ws_free(ws_id(ws)); # deregister working set from meta tables
    if (ws_log_active)
        ws_log(ws, "===================== END TRANSACTION");
}

PROC __ws_opencoll(BAT[void,BAT] ws,
                   BAT [str,bat] docBAT, 
                   str colname, oid coll_oid, oid cont) : BAT[oid,oid]
{
    # get the bats, isolate them, and add them to the working set
    var dsk := docBAT.tmark(oid(_MAP_PID));
    var map_pid := copy(dsk.fetch(MAP_PID));
    var pid_map := map_pid;
    var pre, mem, isolate := (ttype(map_pid) = oid);
    var free_pages := empty_bat;

    if (isolate) {
        # on each document load, we update the free page list (i.e. garbage collection)
        free_pages := map_pid.ord_uselect(oid_nil).access(BAT_WRITE).revert(); # list of free pages
        if ((free_pages.count() = 0) and (free_pages.htype() = void)) free_pages.seqbase(0@0); # give it a non-nil head

        # sort the non-nil entries to get used-page-list (avoids sorting using positional insert trick)
        pid_map := map_pid.slice(0,count(map_pid)-(1+count(free_pages))).copy().access(BAT_WRITE);
        pid_map.replace(map_pid.reverse()).access(BAT_READ);

        # use the map to get transaction isolation: pre's are remapped logical views, and mem are rcopy logical views
        pre := [remap](reverse(ws_rid).leftjoin(dsk), const pid_map, false);
        mem := [rcopy](reverse(ws_mem).leftjoin(dsk)); 
    } else {
        # read-only doc; isolation is also needed due to concurrent add-doc()s
        pre := [rcopy](reverse(ws_rid).leftjoin(dsk));
        mem := [rcopy](reverse(ws_mem).leftjoin(dsk));
    }

    # register new container in ws, and append all bats to it
    var cont_bat := constant2bat(cont);
    ws.fetch(MAP_PID).append(map_pid);
    ws.fetch(PID_MAP).append(pid_map);
    ws.slice(PRE_SIZE,PRE_NID).[append](pre);
    ws.slice(NID_RID,ATTR_PROP).[append](mem);
    ws.fetch(PRE_CONT).append(cont_bat);
    ws.fetch(ATTR_CONT).append(cont_bat);
    ws.fetch(CONT_COLL).append(coll_oid);
    ws.fetch(CONT_NAME).append(colname);
    mirror(ws_dsk).leftfetchjoin(ws).[append](dsk);

    # region index (start/end attributes, standoff steps) left empty until first use
    ws.fetch(REGION_PRE).append(empty_bat);
    ws.fetch(REGION_START).append(empty_bat);
    ws.fetch(REGION_END).append(empty_bat);

    return free_pages;
}

# create a runtime index on persistent documents (we do not deem index creation
# worthwhile on temporary documents that only exist during this query).
PROC __ws_indexcoll(BAT[void,bat] ws, 
                    BAT[lock,bat] runtime, 
                    BAT[oid,any] free_pages, str colname, oid coll_oid, oid cont) : void
{
    var ws_logtime := usec(); 
    var prefix := "runtime" + str(abs(int(runtime))) + "_" + str(int(coll_oid));
    var idx, ins, del, own, unq, vxm, vxi, vxd;
    var isolate := (ttype(ws.fetch(MAP_PID).fetch(cont)) = oid);

    if (count(runtime) > RT_QN_NID) {
        # a previous query already created the indices, just grab them
        own := runtime.fetch(RT_ATTR_OWN);
        idx := runtime.fetch(RT_QN_NID);
        ins := runtime.fetch(RT_QN_NID_INS);
        del := runtime.fetch(RT_QN_NID_DEL);
        unq := runtime.fetch(RT_QN_NID_UNQ);
        vxm := runtime.fetch(RT_VX_HSH_NID);
        vxi := runtime.fetch(RT_VX_HSH_NID_INS);
        vxd := runtime.fetch(RT_VX_HSH_NID_DEL);
    } else {
        # insert list of free pages
        free_pages.bbpname(prefix + "_free_pages");
        var coll_shortlock := reverse(runtime).fetch(RT_LOCK_FREELIST);
        runtime.replace(coll_shortlock, free_pages);

        # only index element qnames that occur <M times, such that M.log(M) < N
        var cnt   := ws.fetch(QN_HISTOGRAM).fetch(cont);
        # DISABLE for PFROX var N     := count(ws.fetch(PRE_SIZE).fetch(cont));
        # DISABLE for PFROX var MlogM := [*](cnt,[log]([dbl](cnt)));
        # DISABLE for PFROX  unq := reverse(ord_uselect(MlogM, dbl_nil, dbl(N)));
        unq := cnt.hmark(0@0);

        # create the QN_NID and HSH_NID indices
        idx := __runtime_index(coll_oid, ws.fetch(PRE_SIZE).fetch(cont),
                                         ws.fetch(PRE_NID).fetch(cont),
                                         ws.fetch(PRE_KIND).fetch(cont),
                                         ws.fetch(PRE_PROP).fetch(cont),
                                         unq,
                                         ws.fetch(NID_RID).fetch(cont),
                                         ws.fetch(ATTR_OWN).fetch(cont),
                                         ws.fetch(ATTR_QN).fetch(cont),
                                         ws.fetch(ATTR_PROP).fetch(cont),
                                         ws.fetch(PROP_VAL).fetch(cont),
                                         ws.fetch(PROP_TEXT).fetch(cont),
                                         ws.fetch(MAP_PID).fetch(cont), 1@0, 0@0, isolate);
        vxm := idx.fetch(1);
        idx := idx.fetch(0);
        ins := empty_bat;
        del := empty_bat;
        own := ws.fetch(_ATTR_OWN).fetch(cont); 
        vxi := empty_bat;
        vxd := empty_bat;

        var page_ws := empty_bat;
        var attr_ws := empty_bat;

        # persistent document ?
        if (coll_oid >= DOCID_MIN) { 
            var suffix := str(abs(int(idx)));
            var free_nids := empty_bat;

            # create the shared hash table on ATTR_OWN (essential during serialization of anything)
            reverse(own := copy(own)).accbuild("hash");

            # updatable document ?
            if (isolate) { 
                free_nids := ws.fetch(NID_RID).fetch(cont).uselect(oid_nil).access(BAT_WRITE).revert();
                if ((free_nids.count() = 0) and (free_nids.htype() = void)) {
                  free_nids.seqbase(0@0);
                } else {
                   # BEWARE DELETED NODES: our impl leaves attrs of deleted nodes in place; filter those out now
                   var garbage := reverse(reverse(free_nids).leftjoin(reverse(own))).project(oid_nil);

                   # the above leftjoin is fast thanks to the hashtable; these replaces are positional (thus fast also)
                   own.replace(garbage,true); # delete from idx
                   ws.fetch(_ATTR_OWN).fetch(cont).replace(garbage,true); # delete from master

                   # with mmapped bats, the master changes will directly be visible in the private copy
                   # however for small, non memory mapped bats this is not the case
                   # in the mmap-case we do not want to touch (needless copy-on-write), so filter first
                   garbage := garbage.mirror().leftfetchjoin(ws.fetch(ATTR_OWN).fetch(cont)).uselect(oid_nil,oid_nil);
                   if (bit(count(garbage))) { # only non-empty for malloced (non -mmap) bats 
                       ws.fetch(ATTR_OWN).fetch(cont).replace(garbage.project(oid_nil),true); # delete from private copy
                   }
                }
                ins := bat(oid,oid);
                del := bat(oid,oid);
                vxi := bat(int,oid);
                vxd := bat(int,oid);
                page_ws := bat(oid,lng);
                attr_ws := bat(oid,lng);

                # to ease debugging and monitoring, we give these potentially long-lived bats a name
                free_nids.bbpname(prefix + "_free_nids" + suffix);
                idx.bbpname(prefix + "_qn_nid" + suffix);
                ins.bbpname(prefix + "_qn_nid_ins" + suffix);
                del.bbpname(prefix + "_qn_nid_del" + suffix);
                vxm.bbpname(prefix + "_vx_hsh_nid" + suffix);
                vxi.bbpname(prefix + "_vx_hsh_nid_ins" + suffix);
                vxd.bbpname(prefix + "_vx_hsh_nid_del" + suffix);
                page_ws.bbpname(prefix + "_page_ws" + suffix);
                attr_ws.bbpname(prefix + "_attr_ws" + suffix);
            }
            own.bbpname(prefix + "_attr_own" + suffix);
            unq.bbpname(prefix + "_qn_nid_unq" + suffix);

            # insert QN_NID index (nsloc) to the shared runtime bat
            runtime.replace(reverse(runtime).fetch(RT_NID_FREELIST),free_nids); 
            runtime.insert(lock_nil,page_ws);
            runtime.insert(lock_nil,attr_ws);
            runtime.insert(lock_nil,own);
            runtime.insert(lock_nil,idx);
            runtime.insert(lock_nil,ins);
            runtime.insert(lock_nil,del);

            # we retain the index sizes for cache control
            # it must be part of the runtime structure, protected by coll_shortlock
            # but where to store such a number?
            # the hack is to misuse the (unused) tail seqbase of unq
            unq.seqbase(oid(batsize(idx) + batsize(vxm))); 
            runtime.insert(lock_nil,unq);

            # add value index (text/attr) to the shared runtime bat
            runtime.insert(lock_nil,vxm);
            runtime.insert(lock_nil,vxi);
            runtime.insert(lock_nil,vxd);

            # Set a unique number (mangled from the wsid in the runtime).
            # it will be zapped by any update. therefore, precense of this id 
            # proves absence of updates. We enforce an odd mangled number (or with 1)
            # to avoid ever confusing such numbers with real lock values
            # (the lock values in RT_LOCK_FREELIST and RT_NID_FREELIST runtime BUNs are 
            #  sometimes used as lookup keys -- should not collide with this mangled id). 
            reverse(runtime).replace(colname_locks, lock((ws_id(ws) >> 32) or 1LL));
        }
    }
    # add the indices to the private transaction environment (ie ws)
    if (isolate) { # copy ins/del for isolation
        ins := ins.copy().access(BAT_WRITE);
        del := del.copy().access(BAT_WRITE);
        unq := unq.copy().access(BAT_WRITE);
        vxi := vxi.copy().access(BAT_WRITE);
        vxd := vxd.copy().access(BAT_WRITE);
    }
    
    # store the indices in the working set
    ws.fetch(QN_NID).append(idx);
    ws.fetch(QN_NID_INS).append(ins);
    ws.fetch(QN_NID_DEL).append(del);
    ws.fetch(QN_NID_UNQ).append(unq);
    ws.fetch(VX_HSH_NID).append(vxm);
    ws.fetch(VX_HSH_NID_INS).append(vxi);
    ws.fetch(VX_HSH_NID_DEL).append(vxd);

    # for accessing ATTR_OWN, we split it in an old (shared, hash-indexed) and new part.
    ws.fetch(ATTR_OWN_SHARED).append(own);
    ws.fetch(ATTR_OWN_PRIVATE).append(ws.fetch(ATTR_OWN).fetch(cont).slice(count(own),int_nil));
}

# returns all other queries that have this collection open
PROC _ws_pinnedcoll(lng wsid,
                    str colname) : BAT[lng,lng]
{
    return colname_pins.reverse().ord_uselect(colname).reverse() # get the basic list of pins
                       .[abs]()                                  # shreds have negated numbers here, normalize
                       .ord_select(0LL,WS_MAXID).reverse()       # exclude artificial ws-es (MIL invoked commands)
                       .access(BAT_WRITE).delete(wsid)           # do not report self
                       .project(wsid).reverse();                           
}

# throw away all index structures! Running queries keep referencing all runtime bats they already opened.
PROC _runtime_flush() : void
{
    var shortlocks := [fetch]([reverse](colname_runtime), RT_LOCK_FREELIST);
    var longlocks := [fetch]([reverse](colname_runtime), RT_NID_FREELIST);
    var runtimes := [_runtime_create](colname_runtime.mirror(), shortlocks, longlocks);
}

PROC _ws_opencoll(lng wsid, 
                  oid coll_oid) : BAT[lock,bat]
{
    var colname := collection_name.find(coll_oid);
    var runtime := _runtime_get(colname);

    if (and(wsid, WSID_UPDATE) != 0LL) {
        # all queries using colname constitute new dependencies
        var dep := _ws_pinnedcoll(wsid, colname);
        ws_overlaps_ws.insert(dep);           # A overlaps B
        ws_overlaps_ws.insert(reverse(dep));  # B overlaps A (overlapping is reflexive)

        # clean up the query change lists (pages, attributes) by removing no longer overlapping queries
        if (count(runtime) > RT_ATTR_WS) {
            var page_ws := runtime.fetch(RT_PAGE_WS);
            var all_ws := reverse(kunique(ws_overlaps_ws).kunion(reverse(ws_overlaps_ws)));
            if (bit(count(page_ws))) {
                page_ws.deleteBuns(tdiff(page_ws, all_ws));
            }
            var attr_ws := runtime.fetch(RT_ATTR_WS);
            if (bit(count(attr_ws))) {
                attr_ws.deleteBuns(tdiff(attr_ws, all_ws));
            }
        }
    }

    # index memory control, done when a persistent collection is opened for the first time  
    if ((coll_oid >= DOCID_MIN) and (count(runtime) <= RT_QN_NID)) {
        # check sizes of all indices, stuffed in the seqbase of the UNQ bats
        var indices := [count](reverse(colname_runtime).mirror()).ord_uselect(RT_QN_NID_UNQ, int_nil);
        if (bit(count(indices))) {
            var totsize := [fetch](reverse(indices), RT_QN_NID_UNQ).[seqbase]().[lng]().old_sum();
            if (totsize > (mem_maxsize()/4LL)) _runtime_flush();
        }
    }
    return runtime;
}


PROC ws_opencoll(BAT[void,BAT] ws,
                 BAT[str,bat] docBAT, 
                 str colname, oid coll_oid) : oid
{
    var err, ws_logtime := usec(), runtime;

    # get a handle to the shared runtime bat (create if first access)
    if (coll_oid >= DOCID_MIN) {
         lock_set(pf_short);
         if (ws_log_active) 
             ws_log(ws, sprintf("ws_opencoll(%s) shortlock%s\n", colname, str(ws_logtime - usec())));
         err := CATCH(runtime := _ws_opencoll(ws_id(ws), coll_oid));
         lock_unset(pf_short);
         if (not(isnil(err))) ERROR(err);

         if (ws_log_active)
             ws_log(ws, sprintf("_ws_opencoll(%s) exec%s\n", colname, str(ws_logtime - (ws_logtime := usec()))));
    } else {
         runtime := runtime(lock_nil,lock_nil);
    }
    ws.fetch(CONT_RUNTIME).append(runtime);

    # first opener creates a collection barrier that will block write access (coll_lock_set)
    var coll_shortlock := reverse(runtime).fetch(RT_LOCK_FREELIST);
    var cont := oid(count(ws.fetch(CONT_COLL)));

    # get non-exclusive read-only access
    if (coll_oid >= DOCID_MIN) {
        lock_set(coll_shortlock);
        lock_set(pf_collbarrier_lock);
        shortlock_barrier.insert(coll_shortlock, sema_nil);
        lock_unset(pf_collbarrier_lock);
        lock_unset(coll_shortlock); # allow concurrent __ws_opencoll()
    }

    # first get the isolated master bat images (NOTE: avoid coll_shortlock this bat access)
    var free_pages;
    err := CATCH(free_pages := __ws_opencoll(ws, docBAT, colname, coll_oid, cont));

    # now add the indices. is fast if already created. But here we need exclusive collection access
    if (coll_oid >= DOCID_MIN) lock_set(coll_shortlock); 
    var ws_exectime := usec();
    if (isnil(err))
        err := CATCH(__ws_indexcoll(ws, runtime, free_pages, colname, coll_oid, cont));

    # last non-exclusive accesser wakes up exclusive requests (coll_lock_set)
    if (coll_oid >= DOCID_MIN) {
        var barriers := empty_bat;
        lock_set(pf_collbarrier_lock);
        shortlock_barrier.delete(coll_shortlock, sema_nil);
        if (not(shortlock_barrier.exist(coll_shortlock, sema_nil))) # is last?
             barriers := reverse(shortlock_barrier).select(coll_shortlock);
        lock_unset(pf_collbarrier_lock);
        if (bit(count(barriers))) [sema_up](reverse(barriers));
        lock_unset(coll_shortlock); 
    }

    # we made sure __ws_opencoll/__ws_indexcoll errors were caught to free the locks anyway.
    if (not(isnil(err))) ERROR(err);
    if (ws_log_active)
        ws_log(ws, sprintf("__ws_opencoll(%s) shortlock%s exec %s\n", 
                           colname, str(ws_logtime - ws_exectime), str(ws_exectime - usec())));
    return cont;
}


@+ Collection Free-Lists and Indexing

These run-time structures are available in the colname_runtime meta 
BAT[str,bat]. For non-loaded collections, this BAT does not contain
any data. It only gets an entry the first time a collection is loaded.
Currently, this entry remains RAM resident from then on.

The runtime-BATs are BAT[lock,bat]s, in which order does matter. We fetch
BUNs by position using RT_* constants. The idea is to store per
collection a number of specific locks and index structures (BATs).

  fetch constant    runtime tuple 
  ================  ==========
0  RT_LOCK_FREELIST [lock,bat] 
                    the lock and a BAT with a list of free page ids.
1  RT_NID_FREELIST  [lock,bat] 
                    nil lock and a BAT with a list of free NIDs
2  RT_REGION_WSID   [wsid,colname_locks] 
                    lock(wsid) that last opened this collection (pf_standoff)
                    colname_locks does not have a real role here (just used as an id)
3  RT_REGION_PRE    [lock,bat] 
                    nil lock and a BAT with sorted PRE list (pf_standoff)
4  RT_REGION_START  [lock,bat] 
                    nil lock and a BAT with start attr values (pf_standoff)
5  RT_REGION_END    [lock,bat] 
                    nil lock and a BAT with end attr values (pf_standoff)
------------------------- start of optional part ---------------------
6  RT_PAGE_WS       [lock,bat] 
                    nil lock and a BAT with (cont,modified pages)
7  RT_ATTR_WS       [lock,bat] 
                    nil lock and a BAT with (cont,modified attrs)
8  RT_ATTR_OWN      [lock,bat] 
                    nil lock and a copy of the ATTR_OWN with hash table
9  RT_QN_NID        [nil, bat] 
                    the main BAT[oid,oid] object shared by all concurrent queries
10 RT_QN_NID_INS    [nil, bat] 
                    a private BAT[oid,oid] with all nodes that got this QN  during this query
11 RT_QN_NID_DEL    [nil, bat] 
                    a private BAT[oid,oid] with all nodes that no longer have this QN since this query.
12 RT_QN_NID_UNQ    [nil, bat] 
                    a BAT[oid,void] list of QNs which are interesting to index (i.e. MlogM < N).
13 RT_VX_HSH_NID   [nil, bat] 
                    the main BAT[int,oid] holding hash-codes and element nids for the text/attr value index
14 RT_VX_HSH_NID_INS [nil, bat] 
                    a private BAT[int,oid] for all old attr/text values that changed or disappeared 
15 RT_VX_HSH_NID_DEL [nil, bat] 
                    a private BAT[int,oid] for all new attr/text values that changed or were inserted 

The first entry head holds a "short" collection lock (coll_shortlock, in much of the
MIL code). It is used to guard the master BAT images, while isolated
copies are created for read-queries. Update-queries and shred-docs
acquire this lock when committing resp. extending these master bats. 

The second entry head holds a "long" collection lock. It is used during
the commit sequence to keep away other commits on that collection.

The first entry tail holds a page-free list.
The second entry tail holds a NID-free list.
Both are used when a document is being extended by an update query.

@- QN_NID index

The QN_NID index uses the collection lock for all its maintenance and
query actions. The bulk of the data is in the shared QN_NID master bat (thus 
taking coll_shortlock is required when accessing it).  The QN_NID index does not 
use any additional locks, so the other head values are nil.

The QN_NID index is a dynamic, adaptive non-persistent index on NSLOC that
can deliver NID candidate lists. These NIDs must be translated into RIDs
(leftfetchjoin), then swizzled, then (finally) sorted, in order to serve
as input for loop-lifted staircase join (SCJ).

As sorting is on the menu always, the length of the candidate list M
should be small wrt the document size N (i.e. MlogM < N), otherwise the whole
purpose of indexing is defeated and a loop-lifted staircase join that does
a full O(N) scan is more efficient.

Thus, the idea is to solely index those QN items that are small. For this
purpose the shredder now keeps statistics on M in QN_HISTOGRAM, and the 
index keeps the QN_NID_UNQ; a list of all index QN-s.
Notice that in incremental shreds, the decision which qnames to index 
is made on the first document. With incremental shreds, qnames that were 
kept out of the index will stay out, even if they turn out to be frequent 
in later shredded documents. This is necessary because *if* the index
contains info about a qn, it should be complete info.

The index is non-persistent as in update scenarios it may be big and is 
not ordered, which would make checkpointing it expensive. Now our checkpoint
costs are in the order of the updated volume (the checkpointed index cost
would be  in the order of the document collection, instead).
Therefore, it is stored in the runtime-BAT, reachable for new queries
via the colname_runtime meta-BAT.

Isolation in the QN_NID index is provided by committing in the _INS and _DEL 
master bats, which are not seen by any concurrent queries. If there are no 
concurrent queries, _INS and _DEL are flushed with the new changes into the 
QN_NID master.


@- VX index

The VX index is used for attribute-value and text-value indexing. It only 
supports equality tests on string values. It indexes *all* text nodes and
attribute values, linking those to a parent element. It does not replicate
the string values (could be huge), rather stores a hash value. For attributes,
this hash-value also combines a qname-id for the attribute as well as the
parent element (for text nodes, no such info is available).

The VX index is exploited by the new pf:attributes() and pf:text() builtins
that return node candidates based on attribute and text-value equality.

The VX index is not guaranteed to be complete. The rationale is that it
tends not be be very useful to use index access for highly frequent values,
while on the other hand those frequent values may occupy the majority of the 
index memory. Therefore, such frequent values are omitted.  
An [hash,1@0] entry called "tombstone" signals the fact that values with 
hash code 'hash' have been omitted from the index. The 1@0 PRE-id is a document-node 
(in all containers). It was chosen as the special value because it can 
never be returned (normally) by pf:attribute() and pf:text(), which 
return element and text nodes, respectively *and* in XQuery queries we can 
test whether a returned candidate is a document node:

 some $x in pf:text(..) satisfies $x instance of document-node()

This construct is used in the generated XQuery to make a run-time decision
whether to use an indexed access path or not.

The VX index is *also* used to power ID/IDREF access. The shredder marks in the 
qn_* table all QNames of ID/IDREF attributes, using a |id| and |idref| suffix in 
the prefix_uri_loc bat.. By joining against all such marked QNames, we can use
the attribute indes to service ID/IDREF calls. Thus, the VX index has
replaced the previous mechanism, that could not handle updates.

The VX index handles updates in much the same way as the NSLOC index: there
is a large shared bat with a hash index, and individual queries get
ins/del bats. The main bat is updated in batch whenever no other queries run. 
For read-only collections, the VX index is persistent and does not use
a hash table (it is then sorted in hash code).


@mil
# get a handle to the runtime-BAT (and create it if not there yet)
PROC runtime(lock shortlock, lock longlock) : BAT[lock,bat] 
{
    var runtime := bat(lock,bat).insert(shortlock,empty_bat)
                                .insert(longlock, empty_bat)
                                .insert(lock(0),  colname_locks)
                                .insert(RT_REGION_PRE_LOCK, empty_bat)
                                .insert(RT_REGION_START_LOCK, empty_bat)
                                .insert(RT_REGION_END_LOCK, empty_bat);
    runtime.bbpname("runtime" + str(abs(int(runtime))));
    return runtime;
}

PROC _runtime_create(str colname, lock shortlock, lock longlock) : BAT[lock,bat] 
{
    var runtime := runtime(shortlock, longlock);
    colname_runtime.delete(colname);
    colname_runtime.insert(colname, runtime);
    return runtime;
}

PROC _runtime_get(str colname) : BAT[lock,bat] 
{
    if (not(colname_runtime.exist(colname)))
        return _runtime_create(colname, lock_create(), lock_create());
    return colname_runtime.find(colname);
}

const IDX_MAGICAL_CONST := 1024;

PROC vx_reduce(BAT[void,int] vxt, BAT[void,oid] vxp) : BAT[oid,oid]
{
    # sort on hash code, to make next stap bearable
    vxt := vxt.tsort();
    vxp := vxt.hmark(0@0).leftfetchjoin(vxp).copy().access(BAT_WRITE);
    vxt := vxt.tmark(0@0);

    # eliminate too frequent values from the index
    var lim := IDX_MAGICAL_CONST; 
    var sel := histogram(vxt).[<](lim);
        sel := vxt.leftjoin(sel).tmark(0@0).access(BAT_WRITE); # true for infrequent values
    var tmp := reverse(kunique(reverse(vxt))).project(true);   # true for first frequent value only
        vxp := vxp.inplace(mirror(tmp).leftfetchjoin(sel).ord_uselect(false).project(1@0));
        sel := sel.inplace(tmp).ord_uselect(true).mirror(); tmp := nil; # select first hsh occurrence anyway 

    return reverse(sel.leftfetchjoin(vxt)).leftfetchjoin(vxp); 
}

# double-underscore __X() means we must have the RT_LOCK_FREELIST of this collection

# this computes a new QN_NID and HSH_NID indices for a suffix of the PRE-table
PROC __runtime_index(oid coll_oid,
                     BAT[void,int] sze, 
                     BAT[void,oid] nid, 
                     BAT[void,chr] knd, 
                     BAT[void,oid] prp, 
                     BAT[void,oid] unq, 
                     BAT[void,oid] rid, 
                     BAT[void,oid] attr_own, 
                     BAT[void,oid] attr_qn, 
                     BAT[void,oid] attr_prp, 
                     BAT[void,str] prp_val, 
                     BAT[void,str] prp_txt, 
                     BAT[void,oid] map_pid, 
                     oid pre, oid att, bit updatable) : BAT[void,bat] 
{

    # look if we have it already (persistent readonly case)
    var nme := str(lng(coll_oid)) + "_qn_nid";
    var vx_nme := str(lng(coll_oid)) + "_vx_hsh_nid";
    if ((coll_oid >= DOCID_MIN) and not(updatable)) { 
        # hack: reverse/reverse to ensure it gets loaded
        var b, err := CATCH(b := reverse(reverse(bat(nme)))); 
        if (isnil(err)) return new(void,bat).insert(nil,b).insert(nil,reverse(reverse(bat(vx_nme))));
    }

    # compute the nsloc index bat for all nodes > pre (ie the newly shredded nodes)
    var tmp := knd.reverse().ord_select(pre,oid_nil).reverse().ord_uselect(ELEMENT).mirror();   
    var idx := tmp.leftfetchjoin(prp); tmp := nil;  # get [pre,qn], in pre-order
        idx := reverse(idx.leftjoin(reverse(unq).mirror())); # get [qn,pre], in pre-order
        idx := idx.leftfetchjoin(nid);              # [qn,nid] still in pre-order

    # compute the text/attr index, starting with text values 
    var vxm := knd.reverse().ord_select(pre,oid_nil).reverse().ord_uselect(TEXT).hmark(0@0);   
    var vxp := vxm.leftfetchjoin(nid);
    var vxt := vxm.leftfetchjoin(prp); vxm := nil;
        vxt := vxt.leftfetchjoin(prp_txt);
        vxt := [xquery_hash](vxt);
        #vxm := vx_reduce(vxt, vxp); for explicit usage, a full index is more useful
        vxm := reverse(vxt).leftfetchjoin(vxp); 
        vxp := nil; vxt := nil;

    # add the newly shredded attributes. The new attributes are >att and have an attr_own > pre
        tmp := attr_own.reverse().ord_select(att,oid_nil).reverse();
    if (updatable) { 
        # get pre-s from nids, but beware nils in attr_own and pre_nid (left by deletes)
        tmp := tmp.leftjoin(rid); 
        tmp := tmp.ord_select(oid_nil,oid_nil);
        tmp := tmp.[swizzle](map_pid); 
    }
        vxp := tmp.ord_select(pre,oid_nil);
        tmp := vxp.hmark(0@0);
        vxp := [int](vxp.tmark(0@0).leftfetchjoin(prp)).access(BAT_WRITE); 
        vxp := [:rotate_xor_hash=](vxp, 13, tmp.leftfetchjoin(attr_qn));
        vxt := [xquery_hash](tmp.leftfetchjoin(attr_prp).leftfetchjoin(prp_val)).access(BAT_WRITE);
        vxt := [:rotate_xor_hash=](vxt, 13, vxp);
        vxp := tmp.leftfetchjoin(attr_own); tmp := nil;
        #tmp := vx_reduce(vxt, vxp);
        tmp := reverse(vxt).leftfetchjoin(vxp); 
        vxp := nil; vxt := nil;

    if (updatable or (coll_oid < DOCID_MIN)) { # hash-index, created here each time document is first used
        (idx := bat(oid, oid, (count(idx)*8)/7).insert(idx)).accbuild("hash");
        (vxm := bat(int, oid, ((count(vxm)+count(tmp))*8)/7).insert(vxm).insert(tmp)).accbuild("hash");
    } else { # persistent read-only case. indices are sorted (binary search) 
        # idx [qn,pre] lexico-ordered (thanks to stable-sort)
        idx := idx.access(BAT_WRITE).sorder().access(BAT_READ).rename(nme).mmap(1).persists(true);
        vxm.access(BAT_WRITE).insert(tmp).order().access(BAT_READ).rename(vx_nme).mmap(1).persists(true); 
        if (pre = 1@0) {
            CATCH(pf_checkpoint(bat(void,str).append(nme).append(vx_nme), true)); # immediate commit
        }
    }
    return new(void,bat).insert(nil,idx).insert(nil,vxm);
}

# worker function to handle inserts and deletes in the index
PROC __runtime_maintain(lng wsid, 
                        str colname, 
                        BAT[lock,bat] runtime, 
                        BAT[int,oid] ins_vx, 
                        BAT[int,oid] del_vx,
                        BAT[oid,oid] ins_qn_nid, 
                        BAT[oid,oid] del_qn_nid,
                        BAT[void,oid] new_qn,
                        BAT[void,oid] del_pages) : void
{
    lock_set(pf_short);
    var coast_clear := (count(colname_pins.reverse().ord_uselect(colname)) = 1); 
    lock_unset(pf_short);

    var idx := runtime.fetch(RT_QN_NID);
    var ins := runtime.fetch(RT_QN_NID_INS);
    var del := runtime.fetch(RT_QN_NID_DEL);
    var unq := runtime.fetch(RT_QN_NID_UNQ);
    var flush_deletes := ((count(del)*8) > count(idx));

    var vxm := runtime.fetch(RT_VX_HSH_NID);
    var vxi := runtime.fetch(RT_VX_HSH_NID_INS);
    var vxd := runtime.fetch(RT_VX_HSH_NID_DEL);
    var vx_flush_deletes := ((count(vxd)*8) > count(vxm));

    # nsloc index only contains qn-s from unq (ignore the rest)
    unq.append(new_qn, true);
    ins_qn_nid := ins_qn_nid.kintersect(reverse(unq));
    del_qn_nid := del_qn_nid.kintersect(reverse(unq));

    # register the new deltas
    ins.insert(ins_qn_nid);
    del.insert(del_qn_nid);
    vxi.insert(ins_vx);
    vxd.insert(del_vx);

    if (coast_clear) {
        # coast is clear, unite als ins, all del and apply them to the master
        idx.insert(ins);
        ins.delete();

        # do not maintain the hash table on qn under deletes, rather
        # accumulate del-s until a full rebuild of the hash
        if (flush_deletes) { 
           # The problem is not so much finding the BUN to delete (by NID), but
           # maintaining the hash-table on qn. 
           # This hash table has a long coll list that unites all equal qn-s
           # the length is often |N|/8, i.e. linear in #nodes (*really* long)
           # As the collision list is singly linked, deleting in the middle 
           # requires a traverse.
           idx.accdel("hash");
           idx.deleteBuns(del);
           del.delete();
        }
        idx.accbuild("hash"); # the hash table on qn ensures fast nsloc lookup

        # do the same for value index
        var ignores := vxm.uselect(1@0);
        vxi.delete(ignores); # ignore hash-codes marked nil (too frequent)
        vxd.delete(ignores); # idem
        vxm.insert(vxi);
        vxi.delete();
        if (vx_flush_deletes) { 
           vxm.accdel("hash");
           vxm.deleteBuns(vxd);
           vxd.delete();
        }
        vxm.accbuild("hash"); # the hash table on (qn,str) code ensures fast value lookup
    }
    # store the new size in the UNQ seqbase
    var totsize := batsize(idx) + batsize(ins) + batsize(del) + 
                   batsize(vxm) + batsize(vxi) + batsize(vxd);
    unq.seqbase(oid(totsize));

    if (ws_log_active)
      ws_log(wsid, sprintf("maintain(idx-%d unq-%d ins-%d del-%d _unq-%d _ins-%d _del-%d) %s %s",
                   count(idx), 
                   count(unq), count(ins), count(del), 
                   count(new_qn), count(ins_qn_nid), count(del_qn_nid), 
                   str(coast_clear), str(flush_deletes)));

    # mark the region index invalid
    # for now, we don't try to maintain it over updates
    # any subsequent queries need to first recreate the index
    reverse(runtime).replace(colname_locks, lock(0LL));
    runtime.replace(RT_REGION_PRE_LOCK, empty_bat);
    runtime.replace(RT_REGION_START_LOCK, empty_bat);
    runtime.replace(RT_REGION_END_LOCK, empty_bat);

    runtime.fetch(RT_LOCK_FREELIST).reverse().append(del_pages);
}

# used by shred: incremental shred on an already index document must maintain it (i.e. add new nodes)
PROC __runtime_addchunk(lng wsid, 
                        str colname, 
                        oid coll_oid, 
                        BAT[lock,bat] runtime, 
                        BAT[void,int] sze, 
                        BAT[void,oid] nid, 
                        BAT[void,chr] knd, 
                        BAT[void,oid] prp, 
                        BAT[void,oid] rid, 
                        BAT[void,oid] attr_own, 
                        BAT[void,oid] attr_qn, 
                        BAT[void,oid] attr_prp, 
                        BAT[void,str] prp_val, 
                        BAT[void,str] prp_txt, 
                        BAT[void,oid] map_pid, 
                        oid pre, 
                        oid att,
                        bit updatable) : str 
{
    var unq, commitbat := str(lng(coll_oid));

    if (updatable) {
        commitbat := str_nil; # no new persistent bats
        if (count(runtime) <= RT_QN_NID) return commitbat; # no index to maintain..
        unq := runtime.fetch(RT_QN_NID_UNQ);
    } else if (pre = 0@0) {
        # first shred (readonly) automatically creates persistent sorted index
        var cnt := bat(str(lng(coll_oid)) + "_qn_histogram");
        #unq := reverse(ord_uselect([*](cnt,[log]([dbl](cnt))), dbl_nil, dbl(count(knd))));
        unq := cnt.hmark(0@0);
    } else {
        # but we do not maintain read-only indices after additional shreds (too costly to re-sort each time)
        var qn_nid     := commitbat + "_qn_nid";
        var vx_hsh_nid := commitbat + "_vx_hsh_nid";

        # first check whether it has an index at all
        if (not(isnil(CATCH({ bat(vx_hsh_nid).persists(false).rename("old_" + vx_hsh_nid);
                              bat(qn_nid).persists(false).rename("old_" + qn_nid); })))) return str_nil;

        # replace the runtime bat with an empty one to delete all references to the old index bats
        var coll_shortlock := reverse(runtime).fetch(RT_LOCK_FREELIST);
        var coll_longlock := reverse(runtime).fetch(RT_NID_FREELIST);
        lock_set(pf_short);
        var err := CATCH(_runtime_create(colname, coll_shortlock, coll_longlock));
        lock_unset(pf_short);
        if (not(isnil(err))) ERROR(err);

        return "old_" + commitbat; # use commit to *remove* old index bats
    }
    # compute a (partial) [qn,nid] inverted list, and insert in into the index
    var idx := __runtime_index(coll_oid,
                               sze, nid, knd, prp, unq, rid, attr_own, attr_qn, attr_prp, prp_val, prp_txt, map_pid,
                               pre, att, updatable);
    if (updatable)
        __runtime_maintain(wsid, colname, runtime, idx.fetch(1), bat(int,oid,0).access(BAT_READ), idx.fetch(0), empty_bat, empty_bat, empty_bat);
    return commitbat;
}

PROC __runtime_newpage(BAT[lock,bat] runtime) : oid 
{
    var free_pages := runtime.fetch(RT_LOCK_FREELIST);
    var i := count(free_pages);

    # re-use a page from the list; or -if empty- append a new page
    if (i > 0) {
        var last_pid := reverse(free_pages).fetch(i - 1);
        free_pages.delete(last_pid);
        return last_pid;
    }
    return oid_nil;
}

PROC __runtime_newnids(BAT[lock,bat] runtime, int cnt) : BAT[void,oid]
{
    var free_nids := runtime.fetch(RT_NID_FREELIST);
    var i := count(free_nids);
    # re-use a page from the list; or -if empty- append a new page
    if (i > 0) {
        var ret := free_nids.slice(max(0, i - cnt), i - 1).copy();
        free_nids.delete(ret);
        return reverse(ret);
    }
    return bat(void,oid);
}


@- exported runtime support API 

ws_lookup       - lookup nids by qn id
_ws_precommit   - detect conflicts with overlapping queries that committed already
__ws_isolate    - isolate bats in concurrent queries (shadow paging)
__ws_postcommit - maintain indices after updates
ws_newpage      - allocate a new page, during an update
ws_newnids      - allocate a new nids, during an update
@mil
# proc to call from the SCJ with nsloc test: acquire locks and get a candidate list using the index
PROC ws_lookup(BAT[void,bat] ws, 
               oid cont, 
               BAT[oid,oid] qn_ids) : bat[void,oid] 
{
    if (count(qn_ids) = 0) return empty_bat; # unknown qnames => empty result

    var res, valid := false;
    var ins, del, idx := reverse(ws.fetch(QN_NID).fetch(cont));
    if (count(qn_ids) = 1) {
        var qn_id := qn_ids.fetch(0);
        valid := ws.fetch(QN_NID_UNQ).fetch(cont).texist(qn_id);
        if (valid) {
            res := idx.select(qn_id).assert_order();
            ins := reverse(ws.fetch(QN_NID_INS).fetch(cont)).select(qn_id);
            del := reverse(ws.fetch(QN_NID_DEL).fetch(cont)).select(qn_id);
        }
    } else {
        valid := (count(qn_ids.kdiff(reverse(ws.fetch(QN_NID_UNQ).fetch(cont)))) = 0);
        if (valid) {
            res := idx.join(qn_ids);
            ins := reverse(ws.fetch(QN_NID_INS).fetch(cont)).join(qn_ids);
            del := reverse(ws.fetch(QN_NID_DEL).fetch(cont)).join(qn_ids);
        }
    }
    if (not(valid)) ERROR("index_lookup: qn_nid not indexed");
    if (bit(count(ins) + count(del))) {
        # avoid doing this when ins/del are empty: res maybe a view on idx (readonly case)
        res := res.access(BAT_WRITE).insert(ins).deleteBuns(del).access(BAT_READ);
    }
    # SCJ must catch error and use sequential post-filter instead.
    return ws_docfilter(ws, sort(res).hmark(oid_nil), cont, true).tmark(0@0);
}


# figure out the set of qualifying qnames for attribute access (matching on uri/loc)
PROC vx_lookup_qns(BAT[void,bat] ws, oid cont, str uri, str loc) : BAT[oid,oid]
{
    var prefix_uri_loc := ws.fetch(QN_PREFIX_URI_LOC).fetch(cont);
    var uri_loc        := ws.fetch(QN_URI_LOC).fetch(cont);
    var qns;
    if (isnil(uri)) { # ID access: get any qname that is marked as ID
        qns := prefix_uri_loc.[endsWith](NS_ACCEL_SEP+"id"+NS_ACCEL_SEP).ord_uselect(true);
    } else if (isnil(loc)) { # IDREF access: get any qname that is marked as IDREF
        qns := prefix_uri_loc.[endsWith](NS_ACCEL_SEP+"idref"+NS_ACCEL_SEP).ord_uselect(true);
    } else if ((uri = "*") and (loc = "*")) { 
        qns := uri_loc;
    } else if (uri = "*") {
        qns := [endsWith](uri_loc, NS_ACCEL_SEP + loc).ord_uselect(true);
    } else if (loc = "*") { 
        qns := [startsWith](uri_loc, uri + NS_ACCEL_SEP).ord_uselect(true);
    } else {
        qns := uri_loc.ord_uselect(uri + NS_ACCEL_SEP + loc);
    }
    return mirror(qns);
}

PROC vx_lookup_qns(BAT[void,bat] ws, oid cont, str elt_uri, str elt_loc, str attr_uri, str attr_loc) : BAT[oid,int]
{
    var elt_qns  := vx_lookup_qns(ws, cont, elt_uri, elt_loc);
    var attr_qns := vx_lookup_qns(ws, cont, attr_uri, attr_loc);
    var pivot    := cross(reverse(elt_qns), attr_qns);
    var attr     := pivot.tmark(0@0); 
    return reverse(attr).leftfetchjoin([int](pivot.hmark(0@0)).access(BAT_WRITE).[:rotate_xor_hash=](13, attr));
}

# loop-lifted (attr/text) key lookup, with iteration dependent 'val': thus can also power joins!  
PROC vx_lookup(BAT[void,bat] ws, oid cont, BAT[void,int] iter_val, BAT[oid,int] qns, bit getattr) : BAT[oid,oid] 
{
    var nid_rid        := ws.fetch(NID_RID).fetch(cont);
    var vx_hsh_nid     := ws.fetch(VX_HSH_NID).fetch(cont);
    var vx_hsh_nid_ins := ws.fetch(VX_HSH_NID_INS).fetch(cont);
    var vx_hsh_nid_del := ws.fetch(VX_HSH_NID_DEL).fetch(cont);
    var map_pid        := ws.fetch(MAP_PID).fetch(cont);
    var iter_cand;

    if (count(qns) = 0) return empty_bat; # qn not present in the document, so there cannot be any matches
    if (cont = WS) {
        return cross(iter_val, reverse(ws.fetch(PRE_KIND).fetch(WS).uselect(getattr.ifthenelse(ELEMENT,TEXT))));
    }

    # for each iter, create a hash number (for each tuple in qns -- usually one of course)
    var iter_hsh  := cross(iter_val, qns);
    var key_iter  := iter_hsh.hmark(0@0);
    var key_hsh   := key_iter.leftjoin(iter_val).tmark(0@0).access(BAT_WRITE);
    if (getattr)
        key_hsh   := [:rotate_xor_hash=](key_hsh, 13, iter_hsh.tmark(0@0)); 
        iter_hsh  := reverse(key_iter).leftfetchjoin(key_hsh);

    # now we are going to assemble a list of candidate NIDs (of the elements we are looking for)
        iter_cand := iter_hsh.leftjoin(vx_hsh_nid); # main lookup

    # ins/del data structures are private to the transaction (no lock needed)
    var iter_ins  := empty_bat;
    var iter_del  := empty_bat;
    if (count(vx_hsh_nid_ins) > 0) iter_ins := iter_hsh.leftjoin(vx_hsh_nid_ins);
    if (count(vx_hsh_nid_del) > 0) iter_del := sintersect(iter_hsh.leftjoin(vx_hsh_nid_del), iter_cand);
    if (bit(count(iter_ins) + count(iter_del))) {
        # avoid doing this when ins/del are empty: res maybe a view on idx (readonly case)
        iter_cand.access(BAT_WRITE).insert(iter_ins).deleteBuns(iter_del).access(BAT_READ);
    }
    return iter_cand; # [iter,pre]
}

PROC vx_lookup(bat[void,bat] ws,
               bat[void,oid] id_iter,
               bat[void,oid] id_item,
               bat[void,int] id_kind,
               bat[void,str] iter_val,
               str elt_uri, str elt_loc, str attr_uri, str attr_loc, bit getattr) : BAT[oid,oid]
{
    # hash the strings, loop over the containers, and perform the lookups.
    var id_cont := get_container(id_kind);
    var qns     := bat(oid,int).insert(0@0,0);
    var id_pre  := bat(oid,oid,count(iter_val));
       iter_val := [xquery_hash](iter_val);

    id_cont.tunique().sort()@batloop() {
        if (getattr) qns := vx_lookup_qns(ws, $h, elt_uri, elt_loc, attr_uri, attr_loc);
        var id_val  := id_cont.ord_uselect($h).mirror().leftfetchjoin(id_iter).leftfetchjoin(iter_val.tmark(1@0));
        var key_id  := id_val.hmark(0@0);
        var key_val := id_val.tmark(0@0);
        var key_nid := vx_lookup(ws, $h, key_val, qns, getattr);
        var key_pre := ws_docfilter(ws, key_nid, $h, true).ssort(); # sort on [iter,pre]
        id_pre.insert(reverse(reverse(key_pre).leftfetchjoin(key_id)));
    }
    return id_pre;
}

# routine that converts NIDs to PREs and filters out pre-s that belong to deleted collection documents
# also makes sure that returned documents get an OPEN_DOCID ws entry
PROC ws_docfilter(bat[void,bat] ws,
                  bat[any::1,oid] any_pre,
                  oid cont, bit swizzle) : bat[any::1,oid]
{
    var map_pid   := ws.fetch(MAP_PID).fetch(cont);
    var nid_rid   := ws.fetch(NID_RID).fetch(cont);
    var pre_size  := ws.fetch(PRE_SIZE).fetch(cont);
    var frag_root := ws.fetch(FRAG_ROOT).fetch(cont);
    var open_cont := ws.fetch(OPEN_CONT);
    var open_docid:= ws.fetch(OPEN_DOCID);
    var open_name := ws.fetch(OPEN_NAME);

    # first, ensure sorted PREs
    if (swizzle) 
        swizzle := (ttype(map_pid) = oid);
    if (cont = WS) 
        swizzle := false;
    if (not(swizzle)) {
        any_pre := any_pre.tsort(); # no-op for read-only nsloc element index
        if (frag_root.exist(TEMP_DOC)) return any_pre; # temporary document (collections) can never have been deleted
        if (cont = WS) return any_pre; 
    } else if (any_pre.htype() = void) {
        any_pre := any_pre.leftfetchjoin(nid_rid).[swizzle](map_pid).tsort();
    } else {
        # headache: can only swizzle with a void head column..
        var any_nid := any_pre.tmark(0@0).leftfetchjoin(nid_rid).[swizzle](map_pid).tsort();
        any_pre := reverse(reverse(any_nid).leftfetchjoin(any_pre.hmark(0@0)));
    }

    # get the list of new documents and their root nodes of which we have received nodes
    var docpre_new := any_pre.docsused(frag_root, nid_rid, map_pid, pre_size).tdiff(open_docid);

    if (count(docpre_new) > 0) {
        # lock to access the meta tables for checking whether the new docs still exists
        lock_set(pf_short);
        var err := CATCH(docpre_new := docpre_new.outerjoin(doc_name));
        lock_unset(pf_short);
        if (not(isnil(err))) ERROR(err);

        # remove results that correspond to already deleted documents (if any)
        var docpre_nme := docpre_new.ord_select(str_nil,str_nil);
        if (count(docpre_nme) < count(docpre_new)) {
            var del := bat(void,oid,count(any_pre));
            var pre := any_pre.tmark(oid_nil);

            docpre_new.kdiff(docpre_nme)@batloop() {
                del.append(pre.ord_select($h,oid(lng($h) + pre_size.fetch($h))));
            }
            any_pre := any_pre.tdiff(del);
        }
        # register the newly opened documents
        open_name.append(docpre_nme.tmark(0@0));
        open_cont.append(docpre_nme.tmark(0@0).project(cont));
        open_docid.append(docpre_nme.hmark(0@0).leftjoin(reverse(frag_root.leftfetchjoin(nid_rid).[swizzle](map_pid))));
    }
    return assert_order(any_pre);
}

# find node pre numbers by ID (/NID) or IDREF. 
# ID/IDREF lookup uses the value index (vx), using the same method as attribute lookup
#
# INPUT TABLE oid iter |oid  item|int kind|int cont|str tokens
#             a single-shaped table, but each of them (except iter) may be constant 
#
# OUTPUT [key,NID], where key is the key of the input table     
PROC ws_findnodes(BAT[void,bat] ws, 
                  BAT[void,oid] id_iter, any id_item, any id_kind, any id_cont, any id_tokens, bit isid) : BAT[oid,oid] 
{
    var loc := str_nil, uri := "*"; # nil loc/uri tells vx_lookup to look for *all* ID/IDREF attributes

    if (count(id_iter) = 0) return bat(oid,oid);

    # get root nids, which identify the XML fragment in which we must look
    var id_root := get_root(ws, id_item, id_kind, id_cont).mposjoin(id_cont, ws.fetch(PRE_NID));

    # expand the token list by splitting on spaces, and expand split_root and split_cont with it
    var split_tokens      := id_tokens.[normSpace]().materialize(id_iter).ll_tokenize(id_iter.project(" "));
    var split_id          := split_tokens.hmark(0@0);
        split_tokens      := split_tokens.tmark(0@0);
    var split_root        := split_id.leftfetchjoin(id_root); # may stay constant
    var split_cont        := split_id.leftfetchjoin(id_cont).materialize(split_id);

    # separate numbers (i.e. nids) from string-tokens (only for ID lookups)
    var split_nids;
    if (isid) {
            split_nids    := [oid](split_tokens);
        var split_sel     := split_nids.ord_uselect(oid_nil).mirror();
            split_tokens  := split_sel.leftfetchjoin(split_tokens);
            split_nids    := split_nids.ord_select(oid_nil,oid_nil);
        loc := "*"; uri := str_nil;
    }

    # process all containers one-by-one, putting [split,pre]-s into the result 
    var result            := bat(oid,oid,count(split_tokens));
    var cont_unq          := id_cont.tunique().sort();
    cont_unq@batloop() {
        var cont          := $h;
        var nid_rid       := ws.fetch(NID_RID).fetch(cont);
        var map_pid       := ws.fetch(MAP_PID).fetch(cont);
        var pre_kind      := ws.fetch(PRE_KIND).fetch(cont);
        var pre_size      := ws.fetch(PRE_SIZE).fetch(cont);
        var pre_prop      := ws.fetch(PRE_PROP).fetch(cont);
        var pre_nid       := ws.fetch(PRE_NID).fetch(cont);
        var attr_cont     := ws.fetch(ATTR_CONT).fetch(cont);
        var attr_prop     := ws.fetch(ATTR_PROP).fetch(cont);
        var attr_qn       := ws.fetch(ATTR_QN).fetch(cont);
        var vx_hsh_nid    := ws.fetch(VX_HSH_NID).fetch(cont);
        var split         := split_cont.ord_uselect(cont).mirror();
        var id_split      := reverse(split.leftfetchjoin(split_id));
        var split_rootpre := split.leftfetchjoin(split_root).leftfetchjoin(nid_rid).[swizzle](map_pid);

        # use vx_lookup() to get the candidate nodes holding any ID/IDREF attribute
        var id_pre;
      { var qns     := vx_lookup_qns(ws, cont, "*", "*", uri, loc);
        var id__val := id_split.leftjoin(split_tokens);
        var key_val := id__val.tmark(0@0);
        var key_id  := id__val.hmark(0@0);
        var key_pre := vx_lookup(ws, cont, [xquery_hash](key_val), qns, true);
        if (key_pre.texist(1@0)) # if the index gave up, every elt is a candidate
            key_pre := cross(key_val, reverse(pre_kind.uselect(ELEMENT)).leftfetchjoin(pre_nid));

        # confirm matches. use get_attr_own to get the attr-ids of the nids in key_pre
        var my_cand   := key_pre.tmark(0@0);
        var my_key    := key_pre.hmark(0@0);
        var tmp       := get_attr_own(ws, my_cand, cont); 
        var chk_attr  := tmp.tmark(0@0);
        var chk_my    := tmp.hmark(0@0);

        # we have all attributes now, but only select the one we want
            tmp       := chk_attr.leftfetchjoin(attr_qn).leftjoin(mirror(qns)).hmark(0@0);
        if (count(tmp) < count(chk_attr)) {
            chk_attr  := tmp.leftfetchjoin(chk_attr);
            chk_my    := tmp.leftfetchjoin(chk_my);
        }
        { # check whether string value actually matches something in 'key_val'
        var chk_text  := chk_attr.leftfetchjoin(attr_prop).mposjoin(chk_attr.leftfetchjoin(attr_cont), ws.fetch(PROP_VAL));
        var chk_val   := chk_my.leftfetchjoin(my_key).leftjoin(key_val).tmark(0@0);
            tmp       := [=](chk_val, chk_text).ord_uselect(true).hmark(0@0).leftfetchjoin(chk_my); }
            key_pre   := key_pre.fetch(reverse(tmp)).leftfetchjoin(nid_rid).[swizzle](map_pid).tsort();

        if (isid) { # for ID() we also perform NID access; merging with IDs, conserves pre-order
            id_pre  := id_split.leftjoin(split_nids).leftjoin(nid_rid).ord_select(oid_nil,oid_nil).[swizzle](map_pid).tsort();
            id_pre  := merged_union(id_pre.tmark(0@0), key_pre.tmark(0@0), 
                                    id_pre.hmark(0@0), key_pre.hmark(0@0).leftfetchjoin(key_id));
            key_pre := id_pre.fetch(0);
            key_id  := id_pre.fetch(1);
        }
        id_pre  := reverse(key_id).leftfetchjoin(key_pre); }

        # for all specified document roots in this container (ONLY!), insert the found results
        split_rootpre.tsort().tunique()@batloop() {
            var pre   := $h;
            var limit := oid(lng(pre) + lng(pre_size.find(pre)));
            var ins   := id_pre.ord_select(pre,limit);
            # make ins [iter,pre] unique; exploiting pre_order
            var unq   := kunique(reverse(CTrefine(ins.tmark(0@0), ins.hmark(0@0))));
            result.insert(ins.fetch(reverse(unq)));
        }
    }
    return ssort(result); # result was sorted on cont|pre, now becomes iter|cont|pre
}

PROC _ws_coll_isolate(lng wsid,
                      BAT[void,bat] ws,
                      str colname,
                      BAT[void,oid] ancestor_nid,
                      BAT[void,oid] modified_nid,
                      BAT[void,oid] modified_page,
                      BAT[void,oid] modified_attr) : void 
{
    var ws_logtime := usec(); 
    if (ws_log_active) 
        ws_log(wsid, "ws_coll_isolate-" + str(ws_id(ws)));

    # it may be that the other thread has done _ws_open_coll (adding itself to 
    # ws_overlaps_ws, so we noticed it) but not yet __ws_open_coll(), which
    # requires the coll_shortlock (possessed by ws_isolate())
    if (ws.fetch(CONT_NAME).texist(colname) and
        not(isnil(ws.seqbase()))) # transactions set ws.seqbase nil when isolation is no longer required
    {
        if (ws_log_active) { 
            ancestor_nid@batloop() 
                ws_log(wsid, "ws_coll_isolate: ancestor_nid-" + str($t));
            modified_attr@batloop() 
                ws_log(wsid, "ws_coll_isolate: modified_attr-" + str($t));
            modified_page@batloop() 
                ws_log(wsid, "ws_coll_isolate: modified_page-" + str($t));
            ws_log(wsid, "ws_coll_isolate: modified_nid-" + str(count(modified_nid)));
            modified_nid.slice(0,20)@batloop() 
                ws_log(wsid, "ws_coll_isolate: modified_nid-" + str($t));
        }

        # get columns of a concurrent query (ws)
        var cont      := ws.fetch(CONT_NAME).reverse().find(colname);
        var attr_own  := ws.fetch(ATTR_OWN).fetch(cont);
        var attr_qn   := ws.fetch(ATTR_QN).fetch(cont);
        var attr_prop := ws.fetch(ATTR_PROP).fetch(cont);
        var pre_size  := ws.fetch(PRE_SIZE).fetch(cont);
        var pre_level := ws.fetch(PRE_LEVEL).fetch(cont);
        var pre_prop  := ws.fetch(PRE_PROP).fetch(cont);
        var pre_kind  := ws.fetch(PRE_KIND).fetch(cont);
        var pre_nid   := ws.fetch(PRE_NID).fetch(cont);
        var nid_rid   := ws.fetch(NID_RID).fetch(cont);
        var map_pid   := ws.fetch(MAP_PID).fetch(cont);

        # translate nid-s to that query's pre-s
        var ancestor_pre := ancestor_nid.leftjoin(nid_rid).tmark(0@0).[swizzle](map_pid);
        var modified_pid := modified_page.leftjoin(map_pid).tmark(0@0);

        # isolate the affected positions
        nid_rid.isolate(modified_nid, false);
        attr_own.isolate(modified_attr, false);
        attr_qn.isolate(modified_attr, false);
        attr_prop.isolate(modified_attr, false);
        pre_size.isolate(ancestor_pre, false);

        # isolate full pages
        pre_size.isolate(modified_pid, true);
        pre_level.isolate(modified_pid,true);
        pre_prop.isolate(modified_pid, true);
        pre_kind.isolate(modified_pid, true);
        pre_nid.isolate(modified_pid, true);
    }
    if (ws_log_active) 
        ws_log(wsid, "ws_coll_isolate: exec-" + str(ws_logtime - usec()));
}

PROC _ws_precommit(BAT[void,BAT] ws,
                   oid cont,
                   BAT[void,oid] modified_page,
                   BAT[void,oid] modified_attr,
                   bit map_pid_changed) : void
{
    var ws_logtime := usec(); 
    var wsid := ws_id(ws);
    var runtime := ws.fetch(CONT_RUNTIME).fetch(cont);
    var page_ws := runtime.fetch(RT_PAGE_WS);
    var attr_ws := runtime.fetch(RT_ATTR_WS);
    var overlap := reverse(ws_overlaps_ws).uselect(wsid).kunique().mirror();
    
    if (ws_log_active) {
        ws_log(wsid, "_ws_precommit");
        overlap@batloop() 
            ws_log(wsid, "_ws_precommit: overlaps-" + str($t));
        modified_attr@batloop() 
            ws_log(wsid, "_ws_precommit: modified_attr-" + str($t));
        attr_ws@batloop() 
            ws_log(wsid, sprintf("_ws_precommit: attr_ws-%s,%s", str($h), str($t)));
        modified_page@batloop() 
            ws_log(wsid, "_ws_precommit: modified_page-" + str($t));
        page_ws@batloop() 
            ws_log(wsid, sprintf("_ws_precommit: page_ws-%s,%s", str($h), str($t)));
    }

    # do not allow concurrent page inserts/deletes (simplictic and can be improved!!)
    if (map_pid_changed) {
        if (page_ws.exist(DOCID_MAX)) {
            var err := sprintf("conflicting update in map_pid from ws %s", str(page_ws.find(DOCID_MAX)));
            if (ws_log_active) ws_log(wsid, err);
            ERROR(err);
        }
        page_ws.insert(DOCID_MAX,wsid);
    }

    # concurrency control: abort conflicting queries
    var conflict1 := join(modified_page,page_ws).join(overlap);
    if (bit(count(conflict1))) { 
        var err := sprintf("conflicting update at page %s from ws %s (%d such errors in total)", 
                           str(modified_page.find(reverse(conflict1).fetch(0))), 
                           str(conflict1.fetch(0)), count(conflict1));
        if (ws_log_active) ws_log(wsid, err);
        ERROR(err);
    }

    var conflict2 := join(modified_attr,attr_ws).join(overlap);
    if (bit(count(conflict2))) { 
        var err := sprintf("conflicting update at page %s from ws %s (%d such errors in total)", 
                           str(modified_attr.find(reverse(conflict2).fetch(0))), 
                           str(conflict2.fetch(0)), count(conflict2));
        if (ws_log_active) ws_log(wsid, err);
        ERROR(err);
    }

    # insert pages in modified page list (similar for attr ids)
    page_ws.insert(reverse(modified_page).project(wsid));
    attr_ws.insert(reverse(modified_attr).project(wsid));

    if (ws_log_active)
        ws_log(wsid, "_ws_precommit: exec=" + str(usec() - ws_logtime));
}

PROC __ws_isolate(BAT[void,BAT] ws,
                  oid cont,
                  BAT[void,oid] modified_page,
                  BAT[void,oid] ancestor_nid,
                  BAT[void,oid] modified_nid,
                  BAT[void,oid] modified_attr) : void
{
    var ws_logtime := usec(); 

    # thanks to coll_shortlock, no new cont users will appear for the moment
    # also get pf_free, to block processing of ending queries, to achieve total stability
    var wsid := ws_id(ws);
    lock_set(pf_free);
    pf_free_held := wsid;

    var colname := ws.fetch(CONT_NAME).fetch(cont);

    if (ws_log_active)
        ws_log(ws, "ws_isolate-start pf_free_lock" + str(ws_logtime - (ws_logtime := usec()))); 

    # establish the current set of queries (ws-s) that were already using this collection
    lock_set(pf_short);
    var other_ws, err := CATCH(other_ws := [ws_bat](_ws_pinnedcoll(wsid, colname)).tmark(0@0));
    lock_unset(pf_short);

    if (ws_log_active)
        ws_log(ws, "ws_isolate-mid() exec" + str(ws_logtime - (ws_logtime := usec()))); 

    # isolate these working sets against our intended updates. 
    # copying pages can take time, thus we introduced pf_free: it allows to do the isolation without claiming pf_short
    if (isnil(err) and bit(count(other_ws)))
        err := CATCH([_ws_coll_isolate](wsid, other_ws, colname, const ancestor_nid,  const modified_nid,
                                                                 const modified_page, const modified_attr));
    pf_free_held := 0LL;
    lock_unset(pf_free);

    if (ws_log_active)
        ws_log(ws, "ws_isolate-end() exec" + str(ws_logtime - usec()));

    if (not(isnil(err))) ERROR(err);
}

# helper to compute [hsh,nid] combinations for the VX index
PROC vx_maintain(BAT[oid,oid] nid,
                 BAT[oid,oid] elemqn, 
                 BAT[oid,oid] attrqn,
                 BAT[oid,str] val) : BAT[int,oid]
{
    return reverse([:rotate_xor_hash=]([xquery_hash](val).access(BAT_WRITE), 13, 
                   [:rotate_xor_hash=]([int](elemqn).access(BAT_WRITE), 13, attrqn))).leftfetchjoin(nid);
}

PROC vx_maintain(BAT[oid,oid] nid,
                 BAT[oid,str] val) : BAT[int,oid]
{
    return reverse([xquery_hash](val)).leftfetchjoin(nid);
}


# proc to call from the update code: add new nodes and remove deleted ones
PROC __ws_postcommit(BAT[void,BAT] ws,
                     oid cont,  
                     BAT[int,oid] attr_ins, 
                     BAT[int,oid] attr_del,
                     BAT[int,oid] text_ins, 
                     BAT[int,oid] text_del,
                     BAT[oid,oid] nsloc_ins, 
                     BAT[oid,oid] nsloc_del,
                     BAT[void,oid] newqns,
                     BAT[void,oid] delpages) : void 
{
    # maintain the index while holding the collection lock
    var colname := ws.fetch(CONT_NAME).fetch(cont);
    var runtime := ws.fetch(CONT_RUNTIME).fetch(cont);
    __runtime_maintain(ws_id(ws),  
                       colname, 
                       runtime, 
                       attr_ins.access(BAT_WRITE).insert(text_ins), 
                       attr_del.access(BAT_WRITE).insert(text_del), 
                       nsloc_ins, 
                       nsloc_del, 
                       newqns, 
                       delpages);
}

# extend the master rid_* bats with new pages
PROC __ws_newpage(BAT[void,int] rid_size,
                  BAT[void,chr] rid_level,
                  BAT[void,oid] rid_prop,
                  BAT[void,chr] rid_kind,
                  BAT[void,oid] rid_nid,
                  BAT[void,oid] map_pid,
                  int new_pid) : int
{
    var empty_page_chr := empty_page.project(chr_nil);
    var empty_page_oid := empty_page.project(oid_nil);

    # take care to fill out the last page (should happen only for small documents)
    var lastpage_size := and(lng(count(rid_size)),REMAP_PAGE_MASK);
    if (lastpage_size > 0LL) {
        rid_size.append(empty_page.slice(lastpage_size,REMAP_PAGE_SIZE), true);
        rid_level.append(empty_page_chr.slice(lastpage_size,REMAP_PAGE_SIZE), true);
        rid_kind.append(empty_page_chr.slice(lastpage_size,REMAP_PAGE_SIZE), true);
        rid_prop.append(empty_page_oid.slice(lastpage_size,REMAP_PAGE_SIZE), true);
        rid_nid.append(empty_page_oid.slice(lastpage_size,REMAP_PAGE_SIZE), true);
    }

    # append (possibly multiple) new pages. We directly allocate 6% (1/16) free space
    var last_pid := new_pid + (new_pid / 16);
    while(new_pid <= last_pid) {
        rid_size.append(empty_page, true);
        rid_level.append(empty_page_chr, true);
        rid_kind.append(empty_page_chr, true);
        rid_prop.append(empty_page_oid, true);
        rid_nid.append(empty_page_oid, true);
        map_pid.append(oid_nil,true); # add page as unused to map_pid
        new_pid :+= 1;
    }
    return last_pid;
}

# append a new page. when the page is at the end; we 
PROC ws_newpage(BAT[void,BAT] ws,
                oid cont) : oid 
{
    var wsid := ws_id(ws);
    var runtime := ws.fetch(CONT_RUNTIME).fetch(cont);

    # if there is a free page, then this is easy
    var coll_shortlock := reverse(runtime).fetch(RT_LOCK_FREELIST);
    lock_set(coll_shortlock);
    var page_id := 0@0, err := CATCH(page_id := __runtime_newpage(runtime));
    lock_unset(coll_shortlock);

    if (isnil(err) and isnil(page_id)) {
        # no free page: we must physically extend the rid_* table 
        var colname := ws.fetch(CONT_NAME).find(cont);

        # get the shredlock, as we need append-exclusive access 
        lock_set(pf_short);
        err := CATCH(_shredlock_set(colname, wsid));
        lock_unset(pf_short);

        page_id := 0@0; 

        # inside the shredlock. try again. maybe another query added the desired pages
        lock_set(coll_shortlock);
        err := CATCH(page_id := __runtime_newpage(runtime));
        lock_unset(coll_shortlock);

        if (isnil(err) and isnil(page_id)) {
            var rid_size  := ws.fetch(_RID_SIZE).find(cont);
            var rid_level := ws.fetch(_RID_LEVEL).find(cont);
            var rid_prop  := ws.fetch(_RID_PROP).find(cont);
            var rid_kind  := ws.fetch(_RID_KIND).find(cont);
            var rid_nid   := ws.fetch(_RID_NID).find(cont);
            var map_pid   := ws.fetch(_MAP_PID).find(cont);

            # fully protect/lock the collection during bat extend
            coll_lock_set(ws, cont, COLL_SHORTLOCK, "ws_newpage"); 
            var ws_logtime := usec(); 
            sema_down(pf_extend_barrier);
 
            var last_pid, new_pid := 1 + ((count(rid_size) - 1) >> REMAP_PAGE_BITS);

            pf_assert(CATCH(last_pid := __ws_newpage(rid_size, rid_level, rid_prop, 
                                                     rid_kind, rid_nid, map_pid, new_pid)), 
                     "master updated failed (newpages)");

            # unprotect/lock the collection after bat extend
            sema_up(pf_extend_barrier);
            coll_lock_unset(ws, cont, COLL_SHORTLOCK, "ws_newpage", ws_logtime); 

            var pages_free := reverse(runtime.fetch(RT_LOCK_FREELIST));
            page_id := oid(new_pid);
            # create a new [nil,oid] bat with densely descending tail values, ending at new_pid
            var newpids := densebat(wrd(last_pid - new_pid)).seqbase(oid(new_pid + 1)).reverse().seqbase(oid(nil)).access(BAT_WRITE).revert(); # put in reverse order to give the pages out in order 

            # add extra created pages to the freelist
            if (isnil(err)) {
                lock_set(coll_shortlock);
                err := CATCH(pages_free.insert(newpids));
                lock_unset(coll_shortlock);
            }
        }
        if (isnil(err)) {
            lock_set(pf_short);
            err := CATCH(_shredlock_unset(colname, wsid));
            lock_unset(pf_short);
        }
    }
    if (not(isnil(err))) ERROR("ws_newpage: " + err);
    return page_id;
} 

# modify master nid_rid bat (extend it), return a 
PROC __ws_newnids(BAT[void,oid] nid_rid, int cnt) : int
{
    var nid_cnt := count(nid_rid);
    var nid_tgt := nid_cnt + cnt + (nid_cnt/16);
    var empty_page_oid := empty_page.project(oid_nil);
    var pgsz := int(REMAP_PAGE_SIZE);

    if (nid_tgt < pgsz) {
        # extend nid column of small documents by the amount needed, not more
        nid_rid.append(empty_page_oid.slice(1, nid_tgt - nid_cnt), true);
        nid_cnt := nid_tgt;
    } else while(nid_cnt < nid_tgt) {
        # append a whole number of pages
        nid_rid.append(empty_page_oid, true);
        nid_cnt :+= pgsz;
    }
    return nid_cnt;
}

# return free NID values in the head
PROC ws_newnids(BAT[void, BAT] ws,
                oid cont,
                int cnt) : BAT[oid,void]
{
    var wsid := ws_id(ws);
    var runtime := ws.fetch(CONT_RUNTIME).fetch(cont);
    var coll_shortlock := reverse(runtime).fetch(RT_LOCK_FREELIST);

    lock_set(coll_shortlock);
    var newnids := empty_bat, err := CATCH(newnids := __runtime_newnids(runtime, cnt));
    lock_unset(coll_shortlock);

    if (isnil(err) and bit(cnt :-= count(newnids))) {
        # not enough free nids page: we must physically extend the nid_rid table 
        var colname := ws.fetch(CONT_NAME).find(cont);

        # get the shredlock, as we need append-exclusive access  
        lock_set(pf_short);
        err := CATCH(_shredlock_set(colname, wsid));
        lock_unset(pf_short);

        var delta := empty_bat;
 
        # inside the shredlock. try again. maybe another query added the desired nids
        lock_set(coll_shortlock);
        err := CATCH(delta := __runtime_newnids(runtime, cnt));
        lock_unset(coll_shortlock);

        if (isnil(err)) {
            newnids.access(BAT_WRITE).append(delta);
            if (bit(cnt :-= count(delta))) {
                var nid_rid := ws.fetch(_NID_RID).find(cont);
                var nid_off := count(nid_rid);
                var nid_cnt;

                coll_lock_set(ws, cont, COLL_SHORTLOCK, "ws_newnids"); 
                var ws_logtime := usec(); 
                pf_assert(CATCH(nid_cnt := __ws_newnids(nid_rid, cnt)), "master update failed (newnids)");
                coll_lock_unset(ws, cont, COLL_SHORTLOCK, "ws_newnids", ws_logtime); 

                # put nids in reverse order in the freelist, so they are given out in order
                newnids.append(reverse(nid_rid.slice(nid_off,(nid_off :+= cnt) - 1)));
                delta := reverse(nid_rid.slice(nid_off,nid_cnt - 1)).copy().access(BAT_WRITE).revert(); 

                if (nid_cnt > nid_off) {
                    # add extra created NIDs to the freelist
                    lock_set(coll_shortlock);
                    err := CATCH(reverse(runtime.fetch(RT_NID_FREELIST)).append(delta));
                    lock_unset(coll_shortlock);
                }
            }
        }
        if (isnil(err)) {
            lock_set(pf_short);
            err := CATCH(_shredlock_unset(colname, wsid));
            lock_unset(pf_short);
        }
    }
    if (not(isnil(err))) ERROR("ws_newpage: " + err);
    return reverse(newnids);
} 




@- adding documents

The XQuery call fn:doc() is translated into ws_doc() MIL proc. It is optimized
to be able to handle a large collection of document names that all reside
in the same collection very quickly, i.e. in loop-lifted, bulk fashion.
When multiple collections are involved, things go one-at-a-time, thus more slowly. 

The decision process which documents are new and must be shredded (and x-locked)
and which are available (and need only pinning) is done inside the locked phase of
ws_doc(). Care was taken to allow large sets of documents that reside in a single
collection to be opened quickly (i.e. using the proper bulk primitives).
@mil
PROC _ws_open(lng wsid,
              BAT[void,str] idx_names, 
              BAT[void,str] idx_filenames, 
              BAT[void,timestamp] idx_timestamps, # inout param
              BAT[void,str] idx_colname,    # out param  
              BAT[void,oid] idx_coll,       # out param
              BAT[void,oid] idx_doc) : void 
{
    # get all the doc-id, coll-id's and collection names of known documents. The ones with nil-tail are unknown.
    var idx_doc_ := idx_names.outerjoin(reverse(doc_name)).access(BAT_WRITE);
    idx_doc_.replace(idx_doc_.ord_uselect(oid_nil).mirror().leftfetchjoin(idx_names).join(reverse(doc_location)));
    idx_coll.append(idx_doc_.outerjoin(doc_collection));
    idx_colname.append(idx_coll.outerjoin(collection_name));

    # of the non-nil doc-ids, try to get non-nil timestamps
    var idx_lim := idx_doc_.ord_select(oid_nil,oid_nil).leftjoin(doc_timestamp).ord_select(timestamp_nil,timestamp_nil);
    var sel_lim := idx_lim.tmark(0@0);
    var sel_idx := idx_lim.hmark(0@0);

    # check the timestamps
    var sel_del := [>](sel_idx.leftfetchjoin(idx_timestamps), sel_lim).ord_uselect(true);
    if (count(sel_del) > 0) { # delete stale documents
        var sel_name := mirror(sel_del).leftfetchjoin(sel_idx).leftjoin(idx_doc_).leftjoin(doc_name);
        del_doc_base(bit_nil,sel_name,false);
        idx_doc.append(idx_names.outerjoin(reverse(doc_name)));
        idx_doc.replace(idx_doc.ord_uselect(oid_nil).mirror().leftfetchjoin(idx_names).join(reverse(doc_location)));
    } else {
        idx_doc.append(idx_doc_);
    }

    # for not-yet-cached URIs: get lifetime from uri_lifetime BAT
    var idx_idx := idx_doc.ord_uselect(oid_nil).mirror();
    var idx_ts := idx_idx.leftfetchjoin(idx_filenames).ord_uselect(str_nil).mirror().leftfetchjoin(idx_timestamps).copy();
    idx_ts@batloop() { # URI (filename = nil)
        var b := [startsWith](idx_names.find($h), mirror(uri_lifetime)).uselect(true);
        var lifetime := lng_nil;
        if (b.count() > 0) {
            var matchlen := [length](mirror(b));
            lifetime :=  *(1000LL, uri_lifetime.find(matchlen.reverse().find(matchlen.max())));
        }
        # this may set ts to nil; such URIs are never cached (and thus do not need to be persistent)
        idx_timestamps.replace($h, add($t,lifetime));
    }

    # pin all collections of already available documents (once)
    colname_pins.insert(idx_doc.join(doc_collection).join(collection_name).tunique().project(wsid)); 
}

# this function processes only URIs still unknown to this ws; returns OPEN ids (point into OPEN_* ws entries) 
PROC ws_open(BAT[void,BAT] ws,
             BAT[void,str] idx_names,
             BAT[void,str] idx_colname,
             BAT[void,oid] idx_coll,
             BAT[void,oid] idx_doc,
             bit check_cache) : BAT[void,oid]
{
    var idx_timestamps, idx_filenames := idx_names;
    var open_docid    := ws.fetch(OPEN_DOCID);
    var open_name     := ws.fetch(OPEN_NAME);
    var open_cont     := ws.fetch(OPEN_CONT);
    var wsid := ws_id(ws);

    if (check_cache) {
        # create filenames from URIs (str_nil if not a filename-URI)
        var selidx_isurl  := [search](idx_names, "://").ord_select(0,int_nil);
        var selidx_match  := selidx_isurl.ord_uselect(4).mirror().leftfetchjoin(idx_names);
        var selidx_isfile := [startsWith](selidx_match, "file").ord_uselect(true).mirror().leftfetchjoin(idx_names).[string](7);
        var selidx_isFILE := [startsWith](selidx_match, "FILE").ord_uselect(true).mirror().leftfetchjoin(idx_names).[string](7);
        idx_filenames := copy(idx_filenames).access(BAT_WRITE);
        idx_filenames.replace(selidx_isurl.project(str_nil)).replace(selidx_isfile).replace(selidx_isFILE).access(BAT_READ);

        # get the actual timestamp of the document cached in the database. 
        idx_timestamps := idx_filenames.project(current_timestamp()).access(BAT_WRITE);
        idx_timestamps.replace([lastmod_time](idx_filenames.select(str_nil,str_nil)).select(timestamp_nil,timestamp_nil));

        # get the lock and pin all existing documents; return info from meta tables in (idx_colname, idx_coll, idx_doc)
        lock_set(pf_short);
        var err := CATCH(_ws_open(wsid, idx_names, idx_filenames, idx_timestamps, idx_colname, idx_coll, idx_doc));
        lock_unset(pf_short);
        if (not(isnil(err))) ERROR(err);
    }

    # add all available but not-yet-present collections to the ws
    var coll_colname := reverse(reverse(reverse(idx_coll).kunique()).ord_select(oid_nil,oid_nil).tdiff(ws.fetch(CONT_COLL))).leftfetchjoin(idx_colname);
    coll_colname@batloop() {
        var docBAT := [bat]([+](str(int($h)), ws_dsk).reverse().mirror()); # get master bats
        ws_opencoll(ws, docBAT, $t, $h); # collections are loaded one-by-one
    }
    var selidx_names, idx_cont := idx_coll.outerjoin(reverse(ws.fetch(CONT_COLL))).access(BAT_WRITE);

    # all still unknown documents (if any) are shredded one-by-one into one temporary collection
    if (check_cache) { 
        selidx_names := idx_doc.ord_uselect(oid_nil);
        check_cache := (bit(count(selidx_names)));
    }
    if (check_cache) {
        selidx_names  := selidx_names.mirror().leftfetchjoin(idx_names);
        var selidx_unq:= selidx_names.reverse().kunique().reverse();
        var commitBAT := bat(void,str);
        var docBAT    := bat(str,bat,count(selidx_unq));
        var locations := selidx_unq.tmark(0@0);
        var names     := [+]("::" + str(wsid) + "::",  locations);
        var colname   := names.fetch(0);
        var timestamps:= selidx_unq.hmark(0@0).leftfetchjoin(idx_timestamps);
        var docid_base:= oid(count(open_docid));
        var doCommit  := not(isnil(min(timestamps)));
        var coll_oid  := shred_into_docBAT(docBAT, locations, names, timestamps, colname, 
                                           oid_nil,                    # create new collection
                                           runtime(lock_nil,lock_nil), # no index to maintain 
                                           docid_base,                 # transient docid
                                           lng(0),                     # read-only (freespace=0)
                                           doCommit,                   # cache it persistently? 
                                           stream_nil, wsid); 
        var cont      := ws_opencoll(ws, docBAT, colname, coll_oid);
        if (doCommit) # persistent? (probably -- could have backed off -- should not matter)
            commitBAT.append([bbpname](docBAT).tmark(0@0));
        
        # commit the new collections
        pf_checkpoint(commitBAT, true);
        lock_set(pf_short);
        doc_undo.delete(doc_undo.select(wsid)); # if these remain, ws_destroy() would remove the new documents
        lock_unset(pf_short);

        # set new colname, cont, and doc_oid
        idx_cont.replace(selidx_names.project(cont));
        idx_doc.replace(selidx_names.leftjoin(locations.seqbase(coll_oid).reverse()));
    }

    # add in bulk all documents to the ws (could be thousands of them!!)
    var selidx_doc   := idx_doc.tdiff(open_docid).reverse().kunique().reverse();
    var selidx_cont  := selidx_doc.mirror().leftfetchjoin(idx_cont);
    var selidx_names := selidx_doc.mirror().leftfetchjoin(idx_names);
    open_docid.append(selidx_doc);
    open_cont.append(selidx_cont);
    open_name.append(selidx_names);
    open_name.reverse().accbuild("hash");

    return idx_doc.leftjoin(reverse(open_docid)).tmark(0@0);
}

# fn:doc() support. 
# - it filters out all documents already in the working set, and calls ws_open on the unknown ones. 
#   this filtering step is important, as open collections may have been deleted in the meantime.
#   while the running query lasts, such collections remain available (collection-wise snapshot isolation)
# - it looks up the document-root PRE (i.e. document node) for each opened document. With multi-document 
#   collections, the value of the PRE is no longer trivial (1@0).
PROC ws_opendoc(BAT[void,BAT] ws,
                BAT[void,str] idx_names,
                BAT[void,str] idx_colname,
                BAT[void,oid] idx_coll,
                BAT[void,oid] idx_doc,
                bit check_cache) : BAT[oid,oid]
{
    var idx_open  := idx_names.outerjoin(reverse(ws.fetch(OPEN_NAME))).tmark(0@0);
    var new_names := idx_open.uselect(oid_nil).mirror().leftfetchjoin(idx_names.tmark(0@0));
    var new_open  := new_names.mark(0@0).leftfetchjoin(ws_open(ws, new_names.tmark(0@0), idx_colname, idx_coll, idx_doc, check_cache));
    idx_open := idx_open.copy().access(BAT_WRITE).replace(new_open).access(BAT_READ);

    # return the resulting [rootpre,cont] combinations
    var idx_cont := idx_open.leftfetchjoin(ws.fetch(OPEN_CONT));
    var idx_doc  := idx_open.leftfetchjoin(ws.fetch(OPEN_DOCID));
    var idx_root := idx_cont.project(oid_nil).access(BAT_WRITE);
    kunique(reverse(idx_cont))@batloop() {
        var frag_root  := ws.fetch(FRAG_ROOT).fetch($h);
        var nid_rid    := ws.fetch(NID_RID).fetch($h);
        var map_pid    := ws.fetch(MAP_PID).fetch($h);
        var selidx_rid := mirror(idx_cont.ord_uselect($h)).leftfetchjoin(idx_doc).leftjoin(frag_root).outerjoin(nid_rid);

        # we may find a doc in an already open coll, that was not there when we first opened it (concurrent incremental shred)
        if (selidx_rid.texist(oid_nil)) # unknown root nid will lead to rid=nil
            ERROR("ws_opendoc(collection %s was added to the repository concurrently.", idx_names.find($t));
       
        idx_root.replace(selidx_rid.[swizzle](map_pid));
    }
    return reverse(idx_root).leftfetchjoin(idx_cont);
}
PROC ws_opendoc(BAT[void,BAT] ws, BAT[void,str] idx_names) : BAT[oid,oid] 
{   
    return ws_opendoc(ws,idx_names.seqbase(0@0), bat(void,str).seqbase(0@0),
                                                 bat(void,oid).seqbase(0@0),
                                                 bat(void,oid).seqbase(0@0), true); 
}

PROC ws_docavailable(BAT[void,BAT] ws, BAT[void,str] idx_names) : BAT[void,bit]
{
    var ret := bat(void,bit).seqbase(0@0);
    idx_names@batloop() { # try to open the documents now so there are no atomicity issues
       ret.append(isnil(CATCH(ws_opendoc(ws,bat(void,str).append($t))))); # failure means: unavailable
    }
    return ret;
}

PROC ws_collection_check(BAT[any::1,str] nms) : BAT[any::1,oid] {
    var coll := nms.leftjoin(reverse(collection_name)); 
    var miss := nms.kdiff(coll);
    if (count(miss) > 0) 
        ERROR("err:FODC0004: collection '%s' unknown (%d missing in total)", miss.fetch(0), count(tunique(miss)));
    return coll;
}

# pf:collection(), get 'collection root' as a single startpoint to query an entire collection
PROC ws_collection_root(BAT[void,BAT] ws, BAT[void,str] colnames) : BAT[oid,oid]
{
    var colnames_unq := colnames.tdiff(ws.fetch(CONT_NAME)).tunique().mirror(); # [str,str]
    lock_set(pf_short);
    var colnames_coll, err := CATCH({ colnames_coll := ws_collection_check(colnames_unq);
                                      colname_pins.insert(colnames_coll.project(ws_id(ws))); });
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
    colnames_coll@batloop() {
        var docBAT := [bat]([+](str(int($t)), ws_dsk).reverse().mirror()); # get master bats
        ws_opencoll(ws, docBAT, $h, $t); # collections are loaded one-by-one
    }
    var ret := reverse(colnames.leftjoin(reverse(ws.fetch(CONT_NAME)))).project(0@0); # [CONT,PRE]
    return ret;
}

# fn:collection(), get document nodes of all documents in a collection
PROC ws_collection(BAT[void,BAT] ws, BAT[oid,str] nms, BAT[void,oid] map) : BAT[oid,oid]
{
    var err, idx_coll, idx_doc, idx_colname, idx_names;
    nms := nms.fetch(nms.tmark(0@0).[startsWith]("::").ord_uselect(false));

    lock_set(pf_short);
    err := CATCH({ var doc := ws_collection_check(nms).leftjoin(reverse(doc_collection)).tsort();
                   var doch := doc.mark(0@0), doct := doc.tmark(0@0); # doc == doch.leftjoin(doct);
                   var docs := doct.leftjoin(doc_name).select(str_nil,str_nil);  # selected existing docs
                   doc := doch.leftjoin(docs.mirror().leftjoin(doct));  # reduced doc to existing docs
                   idx_names := doch.leftjoin(docs);
                   idx_doc := doc.tmark(0@0);
                   idx_coll := idx_doc.leftjoin(doc_collection).tmark(0@0);
                   idx_colname := idx_coll.leftjoin(collection_name).tmark(0@0); 
                   colname_pins.insert(reverse(nms).kunique().project(ws_id(ws))); });
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
    map.append(idx_names.hmark(0@0)); # map = SECOND RESULT VALUE
    return ws_opendoc(ws, idx_names.tmark(0@0), idx_colname, idx_coll, idx_doc, false); 
}

# create a sequence of elements with 2 attributes, out of a four-column table
PROC xmltab2(BAT[void,bat] ws, BAT[any,any] b) : BAT[void,oid]
{
  var eltqn := add_qname ("", "", "bun", ws);
  var at1qn := add_qname ("", "", "head", ws);
  var at2qn := add_qname ("", "", "tail", ws);
  var attr1 := [str](b.hmark(0@0));
  var attr2 := [str](b.tmark(0@0));
  var attrid := new(void,oid,2*count(attr1)).seqbase(0@0).append(attr1.project(oid_nil)).append(attr1.project(oid_nil));
  attrid.replace(reverse([oid]([lng](reverse(attr_constr(attr1.project(at1qn),attr1,ws).fetch(1))).[*](2LL))));
  attrid.replace(reverse([oid]([lng](reverse(attr_constr(attr1.project(at2qn),attr2,ws).fetch(1))).[*](2LL).[+](1LL))));
  return elem_constr(attr1.mark(0@0),attr1.project(eltqn), attrid.mirror().[lng]().[/](2LL).[oid](),
                                                           attrid.project(oid_nil), attrid.project(oid_nil),
                                                           attrid, attrid.project(WS), ws).fetch(1);
}

# execute any mil command..
PROC ws_mil(BAT[void,BAT] ws, str mil) : any
{
   var res := eval(mil);
   if (type(res) != bat) return str(res);
   return xmltab2(ws,res);
}

# create a sequence of elements with 3 attributes, out of a four-column table
PROC xmltab4(BAT[void,bat] ws,
             str eltname, BAT[void,str] item,
             str at1name, BAT[void,str] attr1,
             str at2name, BAT[void,str] attr2,
             str at3name, BAT[void,str] attr3) : BAT[void,oid]
{
  var eltqn := add_qname ("", "", eltname, ws);
  var at1qn := add_qname ("", "", at1name, ws);
  var at2qn := add_qname ("", "", at2name, ws);
  var at3qn := add_qname ("", "", at3name, ws);
  var elemid := new(void,oid,4*count(item)).seqbase(0@0)
                                           .append(item.project(oid_nil)).append(item.project(oid_nil))
                                           .append(item.project(oid_nil)).append(item.project(oid_nil));
  var attrid := elemid.copy().access(BAT_WRITE);
  elemid.replace(reverse([oid]([lng](reverse(text_constr(item, ws).fetch(1))).[*](4LL))));
  attrid.replace(reverse([oid]([lng](reverse(attr_constr(attr1.project(at1qn),attr1,ws).fetch(1))).[*](4LL).[+](1LL))));
  attrid.replace(reverse([oid]([lng](reverse(attr_constr(attr2.project(at2qn),attr2,ws).fetch(1))).[*](4LL).[+](2LL))));
  attrid.replace(reverse([oid]([lng](reverse(attr_constr(attr2.project(at3qn),attr3,ws).fetch(1))).[*](4LL).[+](3LL))));
  return elem_constr(item.mark(1@0),item.project(eltqn), elemid.mirror().[lng]().[/](4LL).[+](1LL).[oid](),
                                                         elemid, [isnil](elemid).[ifthenelse](oid_nil,WS),
                                                         attrid, [isnil](attrid).[ifthenelse](oid_nil,WS), ws).fetch(1);
}

# fn:collections(), get all collection *names*
PROC ws_collections(BAT[void,BAT] ws, bit consistent) : BAT[void,oid]
{
    var coll_nme, coll_sze, coll_mbs, coll_upd; 
    if (consistent) pflock_meta(ws_id(ws)); # stop all concurrent shred_doc/del_doc activities
    lock_set(pf_short);
    var err := CATCH( { coll_nme := mirror(collection_name.[startsWith]("::").uselect(false));
                        coll_sze := doc_name.uselect(str_nil,str_nil).mirror().leftjoin(doc_collection).histogram();
                        coll_sze := coll_nme.leftjoin(coll_sze).tmark(0@0).[str]();
                        var sz := coll_nme.leftjoin(collection_size).tmark(0@0);
                        coll_mbs := [ifthenelse]([>=](sz, 1024LL * 1024LL),
                                                 [/](sz, 1024LL * 1024LL).[str]().[+](" MiB"),
                                                 [/](sz, 1024LL).[str]().[+](" KiB"));
                        coll_upd := coll_nme.[lng]().[str]().[+]("_map_pid").[bat]().[int]().[ttype]().[=](oid).tmark(0@0).[str]();
                        coll_nme := coll_nme.leftjoin(collection_name).tmark(0@0); 
                        if (not(consistent)) { # here we include info about shredding in progress
                            var shred := mirror(colname_shredlock).[startsWith]("::").uselect(false).hmark(0@0);
                            var tmp := shred.tdiff(coll_nme); # new collection names being shredded
                            coll_nme := coll_nme.copy().access(BAT_APPEND).append(tmp);
                            coll_upd := coll_upd.copy().access(BAT_APPEND).append(tmp := tmp.project("unknown"));
                            coll_mbs := coll_mbs.copy().access(BAT_APPEND).append(tmp);
                            coll_sze := coll_sze.copy().access(BAT_APPEND).append(tmp);
                            coll_upd := [+](coll_upd, [isnil](coll_nme.outerjoin(reverse(shred))).[ifthenelse](""," (shredding..)"));
                        }});
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
    return xmltab4(ws, "collection", coll_nme, "numDocs", coll_sze, "size", coll_mbs, "updatable", coll_upd);
}

# pf:documents(), get all document *names* in a collection
PROC ws_documents(BAT[void,BAT] ws, BAT[any,str] coll_nme, bit consistent) : BAT[oid,oid]
{
    var doc_nme, doc_col, doc_url, doc_upd, doc_itr; 
    if (consistent) pflock_meta(ws_id(ws)); # stop all concurrent shred_doc/del_doc activities
    lock_set(pf_short);
    var err := CATCH( { var col_upd := collection_name.mirror().[lng]().[str]().[+]("_map_pid").[bat]().[int]().[ttype]().[=](oid).[str]();
                        doc_nme := doc_name.uselect(str_nil,str_nil).mirror().leftjoin(doc_collection);
                        doc_nme := coll_nme.leftjoin(reverse(collection_name)).leftjoin(reverse(doc_nme));
                        doc_itr := doc_nme.hmark(0@0);
                        doc_url := doc_nme.leftjoin(doc_location).tmark(0@0);
                        doc_col := doc_nme.leftjoin(doc_collection).tmark(0@0);
                        doc_upd := doc_col.leftjoin(col_upd).tmark(0@0); 
                        doc_col := doc_col.leftjoin(collection_name).tmark(0@0); 
                        doc_nme := doc_nme.leftjoin(doc_name).tmark(0@0); });
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
    return reverse(doc_itr).leftfetchjoin(xmltab4(ws, "document", doc_nme.tmark(0@0), "collection", doc_col, "url", doc_url, "updatable", doc_upd));
}

# pf:documents(), get all document *names* 
PROC ws_documents(BAT[void,BAT] ws, bit consistent) : BAT[void,oid]
{
    var doc_nme, doc_col, doc_url, doc_upd; 
    if (consistent) pflock_meta(ws_id(ws)); # stop all concurrent shred_doc/del_doc activities
    lock_set(pf_short);
    var err := CATCH( { var col_upd := collection_name.mirror().[lng]().[str]().[+]("_map_pid").[bat]().[int]().[ttype]().[=](oid).[str]();
                        doc_nme := mirror(doc_name.[startsWith]("::").uselect(false));
                        doc_url := doc_nme.leftjoin(doc_location).tmark(0@0);
                        doc_col := doc_nme.leftjoin(doc_collection).tmark(0@0);
                        doc_upd := doc_col.leftjoin(col_upd).tmark(0@0); 
                        doc_nme := doc_nme.leftjoin(doc_name).tmark(0@0); 
                        doc_col := doc_col.leftjoin(collection_name).tmark(0@0); });
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
    return xmltab4(ws, "document", doc_nme, "collection", doc_col, "url", doc_url, "updatable", doc_upd);
}

PROC ws_docname(BAT[void,BAT] ws, BAT[void,oid] id_iter, BAT[void,oid] id_item, BAT[void,int] id_kind) : BAT[oid,str]
{
    var id_conts := get_container(id_kind);
    var id_roots := get_root(ws, id_item, id_kind, id_conts).mposjoin(id_conts, ws.fetch(PRE_NID));
    var res_cont := id_roots.hmark(0@0).leftfetchjoin(id_conts);
    var res_iter := id_roots.hmark(0@0).leftfetchjoin(id_iter);
    var res_docid:= id_roots.tmark(0@0).mvaljoin(res_cont, ws.fetch(FRAG_ROOT)).tmark(0@0);
    var res := reverse(res_iter).leftfetchjoin(res_docid);
    lock_set(pf_short);
    var err := CATCH(res := res.leftjoin(doc_name));
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
    return res;
}

@- shredding documents

Documents are shredded one-at-a-time, even if we must shred a sequence of documents into the same
collection. It is implemented by shred_url(). Note that shred_url() adds the resulting
new document container to the BAT-of-BATs that is passed in as first parameter. If this
BAT-of-BATs already contains a document container, the XML document is added to this collection.

Some care is taken to give out unique document oids. The collection-oid is that of the first document
that belongs to it. The number is stored persistently (HACK) in a special bun in doc_collection.

The shred_doc_base() operator it depends on is made atomic by adding all new documents to a "doc_undo"
bat. Only if everything succeeds, the entries are removed from there. Otherwise, in ws_destroy(), 
all documents mentioned in "doc_undo" are deleted again. 
@mil
# update the doc_* and collection_* tables while holding the short lock
PROC _shred_into_docBAT(BAT[str,bat] docBAT, 
                        BAT[void,str] idx_location, 
                        BAT[void,str] idx_name, 
                        BAT[void,timestamp] idx_timestamps, 
                        str colname, 
                        oid coll_oid, 
                        lng wsid) : oid
{
    # get a new persistent doc id
    var docid_base := doc_collection.find(DOCID_CURID_HACK);
    var docid      := lng(docid_base);
    var docid_new  := docid + lng(count(idx_location));
    if (docid_new > lng(DOCID_MAX)) 
        ERROR("_shred_doc: out of document id's. Export all documents, and shred them into a new database\n");
    doc_collection.replace(DOCID_CURID_HACK, oid(docid_new));

    if (bit(count(doc_name.tintersect(idx_name)))) 
        ERROR("_shred_doc: document names should be globally unique\n");

    # determine the collection id for this doc
    if (isnil(coll_oid)) {
        coll_oid := docid_base;
        if (collection_name.texist(colname)) ERROR("_shred_doc: collection name should be globally unique\n");
        collection_name.insert(coll_oid, colname);
        collection_size.insert(coll_oid, old_sum([batdsksize](docBAT)));
        [persists]([rename](docBAT, [+](str(int(coll_oid)), mirror(docBAT))), true);
    } else {
        collection_size.replace(coll_oid, old_sum([batdsksize](docBAT)));
    }

    # all documents to the doc_* table
    var docid_location   := idx_location.tmark(docid_base);
    var docid_name       := idx_name.tmark(docid_base);
    var docid_timestamps := idx_timestamps.tmark(docid_base);
    doc_name.insert(docid_name);
    doc_location.insert(docid_location);
    doc_timestamp.insert(docid_timestamps);
    doc_collection.insert(docid_name.project(coll_oid));
    if (not(colname.startsWith("::"))) doc_undo.insert(docid_name.project(wsid));

    return docid_base;
}

# finish the master bats while holding the collection lock
PROC __shred_into_docBAT(BAT[str,bat] docBAT, 
                         str colname, 
                         oid coll_oid, 
                         oid docid_base, 
                         BAT[lock,bat] runtime,
                         oid pre,  
                         oid att,
                         bit updatable, 
                         lng wsid) : void
{
    # finish shred by setting the new (densely ascending doc_oids in FRAG_ROOT (in order)
    var root_frag := reverse(docBAT.fetch(FRAG_ROOT));
    root_frag.replace(root_frag.ord_uselect(TEMP_DOC).mark(docid_base), true).assert_order();

    # index the new document(s)
    if (coll_oid >= DOCID_MIN) {
        var commitbat := __runtime_addchunk(wsid, colname, coll_oid, runtime, 
                                                  docBAT.fetch(PRE_SIZE), 
                                                  docBAT.fetch(PRE_NID), 
                                                  docBAT.fetch(PRE_KIND), 
                                                  docBAT.fetch(PRE_PROP), 
                                                  docBAT.fetch(NID_RID), 
                                                  docBAT.fetch(ATTR_OWN), 
                                                  docBAT.fetch(ATTR_QN), 
                                                  docBAT.fetch(ATTR_PROP),
                                                  docBAT.fetch(PROP_VAL),
                                                  docBAT.fetch(PROP_TEXT),
                                                  docBAT.fetch(MAP_PID), pre, att, updatable);
        if (not(isnil(commitbat))) {
		docBAT.insert(commitbat + "_qn_nid", 
                              bat(commitbat + "_qn_nid")).insert(commitbat + "_vx_hsh_nid", bat(commitbat + "_vx_hsh_nid")); 
        }
    }
}


# main shredder may shred many documents into a single collection
PROC shred_into_docBAT(BAT[str,bat] docBAT, 
                       BAT[void,str] location, 
                       BAT[void,str] name, 
                       BAT[void,timestamp] timestamps, 
                       str colname, 
                       oid coll_oid, 
                       BAT[lock,bat] runtime,
                       oid docid_base, 
                       lng pageFree, 
                       bit doCommit, 
                       Stream s,
                       lng wsid) : oid
{
    var verbose := >=(wsid, WS_MAXID);
    var updatable := >(pageFree,0LL);
    var err := str_nil;
    var coll_shortlock := reverse(runtime).fetch(RT_LOCK_FREELIST);
    var pre := 0@0;
    var att := 0@0;
    if (count(docBAT) > 0) {
        pre := oid(count(docBAT.fetch(PRE_SIZE)));  
        att := oid(count(docBAT.fetch(ATTR_OWN)));  
    }

    if (isnil(s)) { 
        # shred multiple documents into a single collection (maybe empty at first)
        [shred_url](const docBAT, location, pageFree, coll_shortlock, verbose); 
    } else {
        # shred from a stream (a single document assumed here)
        shred_stream(docBAT, s, pageFree, coll_shortlock, verbose);
    }

    if (doCommit) {

      lock_set(pf_short);

      # possibly back off if auto-caching documents gets us into meta-data locking trouble
      if (colname.startsWith("::")) doCommit := pflock_free(doCommit); 

      # add doc to the database locked
      if (doCommit) err := CATCH(docid_base := _shred_into_docBAT(docBAT, location, name, timestamps, colname, coll_oid, wsid));

      lock_unset(pf_short);
      if (not(isnil(err))) ERROR(err);
    }
    if (isnil(coll_oid)) 
        coll_oid := docid_base; # new collection got oid of first doc in it 
    else 
        updatable := (ttype(docBAT.fetch(MAP_PID)) = oid);

    # finish the shred (set doc_oids in FRAG_ROOT, and maintain the nsloc index)
    var protect := not(isnil(coll_shortlock));
    if (protect) lock_set(coll_shortlock); # never lock a collection inside the short lock
    err := CATCH(__shred_into_docBAT(docBAT, colname, coll_oid, docid_base, runtime, pre, att, updatable, wsid));
    if (protect) lock_unset(coll_shortlock);
    if (not(isnil(err))) ERROR(err);

    return coll_oid;
}


PROC _shred_doc_base(BAT[oid,str] selidx_coll,
                     BAT[void,str] idx_names, 
                     BAT[void,str] idx_colnames, 
                     lng wsid) : BAT[oid,bat]
{
    # claim exclusive access to all collections
    var selidx_coll := reverse(reverse(idx_colnames).kunique().sort());
    [_shredlock_set](selidx_coll, wsid); 

    # now that we have access, check consistency
    var conflicts := mirror(reverse(idx_names)).join(reverse(doc_name));
    if (count(conflicts) > 0) {
        conflicts := conflicts.outerjoin(doc_collection).outerjoin(collection_name);
        ERROR("shred_doc: document %s already exists in collection %s (%d such errors)!\n", 
            reverse(conflicts).fetch(0), conflicts.fetch(0), count(conflicts));
    }

    # look up the locks and coll_oids while inside the short lock, and return these for later use
    [_runtime_get](selidx_coll);
    return reverse(mirror(reverse(idx_colnames)).outerjoin(reverse(collection_name))).outerjoin(colname_runtime);
}

PROC _shred_doc_cleanup(lng wsid, bit cleanup) : BAT[any,any]
{
    doc_undo.delete(doc_undo.select(wsid));
    if (cleanup) return _collection_cleanup(); # garbage collect empty collections
    return new(str,str);
}
var pf_commit_docmgt := true;

PROC shred_doc_base(bit cleanup,
                    BAT[void,str] idx_locations, 
                    BAT[void,str] idx_names, 
                    BAT[void,str] idx_colnames, 
                    BAT[void,lng] pageFrees, 
                    Stream s,
                    lng wsid) : void
{
    var commitBAT := new(void,str);
    var pivot, err, nr := 0;
    var selidx_colname := idx_colnames;
    if (count(selidx_colname) > 0)
        selidx_colname := reverse(reverse(idx_colnames).kunique().sort());

    var chk := select(idx_names.histogram(),2,int_nil);
    if (count(chk) > 0) 
        ERROR("A document named '%s' is added more than once (%d such errors).", reverse(chk).fetch(0), count(chk));

    lock_set(pf_short);
    err := CATCH(pivot := _shred_doc_base(selidx_colname, idx_names, idx_colnames, wsid));
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
    var idx_runtime := pivot.tmark(0@0);
    var idx_colloid := pivot.hmark(0@0);

    # collect IDs of new collections for the logger
    var newcoll := new(void, str, count(selidx_colname) * count(logger_bats)).seqbase(0@0);

    selidx_colname@batloop(){
        # process collections one-by one
        var colname  := $t;
        var coll_oid := idx_colloid.find($h);
        var runtime  := idx_runtime.find($h);
        var idx      := idx_colnames.ord_uselect(colname).hmark(0@0);  
        var percentage := pageFrees.find($h);
        var docBAT;

        if (isnil(coll_oid)) {
            docBAT := bat(str,bat,count(idx)); # new collection
        } else { # append to existing 
            docBAT := [bat]([+](str(int(coll_oid)), ws_dsk).reverse().mirror()).access(BAT_WRITE); 
        }

        # now shred all documents for one collection
        shred_into_docBAT(docBAT, 
                          idx.leftfetchjoin(idx_locations),
                          idx.leftfetchjoin(idx_names),
                          idx.project(timestamp_nil),
                          colname, coll_oid, runtime, oid_nil, percentage, true, s, wsid);

        # save bats outside the lock
        if (pf_commit_docmgt) [save](docBAT);

        if (isnil(coll_oid)) {
            # register new bats with the logger
            newcoll.append([bbpname](logger_bats.join(docBAT)));
        }
        commitBAT.insert([bbpname](docBAT.tmark(oid_nil)));
    }
    # checkpoint the new bats
    if (pf_commit_docmgt) {
        var start_trans := true;
        if (count(newcoll) > 0) {
            start_trans := false;
            lock_set(pf_wal);
            log_trans_start(pf_logger);
            [logger_add_bat](pf_logger, [bat](newcoll), newcoll);
            [log_bat_persists](pf_logger, [bat](newcoll), newcoll);
        } else if (cleanup) {
            # force checkpoint when documents were deleted (i.e. cleanup is set)
            lock_set(pf_wal);
            log_trans_start(pf_logger);
            start_trans := false;
        }
        var ok;
        var err := CATCH(ok := pf_checkpoint(tdiff(commitBAT, newcoll), start_trans));
        if (not(start_trans)) lock_unset(pf_wal);
        if (not(isnil(err))) ERROR(err);
        if (ok or cleanup) {
            # remove the in-memory undo log; and trim collection
            lock_set(pf_short);
            err := CATCH(commitBAT := _shred_doc_cleanup(wsid, cleanup));
            lock_unset(pf_short);
            if (isnil(err)) CATCH(collection_cleanup(commitBAT)); 
        }
    }
}


PROC shred_stream(Stream s,
                  str name,
                  str colname,
                  lng pageFree) : void
{
    var names := bat(void,str).append(name).access(BAT_READ).seqbase(0@0);
    var wsid := WS_MAXID + lng(clientid() + 1);
    var err := CATCH(shred_doc_base(false, names, names, names.project(colname), names.project(pageFree), s, wsid));
    ws_free(wsid); 
    if (not(isnil(err))) ERROR(err);
}


@- deleting documents

To make document deletes atomic, we first just set the document names to nil in the
administrative interface and commit that (using a partial checkpoint). 

Collections without any non-nil document names are garbage-collected periodically.
This happens at start-up and at times when the system detects that no queries are 
running (this is triggered by the last leaving query in ws_destroy()).
@mil
PROC _del_doc_replace(BAT[oid,str] bak_locations, 
                      BAT[oid,str] bak_names) : void 
{
    doc_location.replace(bak_locations);
    doc_name.replace(bak_names);
}

PROC _del_doc(bit cachedOnly,  
              BAT[any,str] names, # dummy param iff not(isnil(cachedOnly)) 
              BAT[oid,str] bak_locations, 
              BAT[oid,str] bak_names) : bit
{
    var del := doc_timestamp; # select all at first
    var ret := false;
    if (isnil(cachedOnly)) {
        del := reverse(names.outerjoin(reverse(doc_name)));
        if (del.exist(oid_nil)) {
            ERROR("_del_doc(%s): document not found in database (%d such errors)!\n", 
                                 names.find(del.find(oid_nil)), 
                                 count(reverse(del).uselect(oid_nil)));
        }
        del := kunique(del);
        ret := true;
    } else if (cachedOnly) {
        del := del.uselect(timestamp_nil,timestamp_nil);
    } 
    del := del.project(str_nil);
    bak_locations.insert(mirror(del).join(doc_location));
    bak_names.insert(mirror(del).join(doc_name));
    _del_doc_replace(del, del);
    return ret; # returns whether we should force a cleanup
}

PROC del_doc(bit cachedOnly, 
             BAT[any,str] names,
             bit pf_short_req) : bit
{
    var bak_locations := bat(oid,str);
    var bak_names := bat(oid,str);

    if (pf_short_req) lock_set(pf_short);
    var ret, err := CATCH(ret := _del_doc(cachedOnly, names, bak_locations, bak_names));
    if (not(isnil(err))) pf_assert(CATCH(_del_doc_replace(bak_locations, bak_names)), "in-memory recovery failed");
    if (pf_short_req) lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);

    return ret;
}

PROC del_doc_base(bit cachedOnly, 
                  BAT[any,str] names,
                  bit pf_short_req) : void
{
    # do the work in memory
    del_doc(cachedOnly, names, pf_short_req);

    # now here is our commit: checkpoint 2 bat descs and BBP.dir
    if (pf_commit_docmgt) {
        var ok := false;
        lock_set(pf_wal);
        var err := CATCH(ok := subcommit(bat(void,str).append("doc_name").append("doc_location")));
        lock_unset(pf_wal);
        if (not(ok)) {
            var msg := "XQDY0062: checkpoint failed (in del_doc_base), query aborted.\n";
            if (not(isnil(err))) msg := err + msg;
            ERROR(msg);
        }
    }
   
    if (pf_short_req) {
        # try to remove empty collections (non-critical)
        lock_set(pf_short);
        var commitBAT, err := CATCH(commitBAT := _collection_cleanup());
        lock_unset(pf_short);
        if (isnil(err)) CATCH(collection_cleanup(commitBAT));
    }
}


@+ Document Management Interfaces

The pf:add-doc, pf:del-doc XQuery builtin functions are implemented
in loop-lifted form by the following MIL PROCs.

Note that ws_doc (previous) is similarly called from XQuery for fn:doc()
and may also lead to on-the-fly document shredding.

@- XQuery Document Management Interface

The play_doc_tape() is a loop-lifted document management function. It may only occur in read-only
queries (i.e. not intermingled with updates). This is because we use checkpointing for
shredding (and not the WAL). We cannot use both mechanisms and still have an atomic operation.
@mil
PROC play_doc_tape(BAT[void,BAT] ws,
                   BAT[void,oid] item, 
                   BAT[void,int] kind, 
                   BAT[void,lng] int_values, 
                   BAT[void,str] str_values) : void
{
    var IDoid       := [and]([lng](item.mirror()), 3LL).ord_uselect(0LL).hmark(0@0);
    var IDlng       := IDoid.[lng]();
    var locations   := IDoid.leftfetchjoin(item).leftfetchjoin(str_values);
    var names       := [+](IDlng, 1).[oid]().leftfetchjoin(item).leftfetchjoin(str_values);
    var colnames    := [+](IDlng, 2).[oid]().leftfetchjoin(item).leftfetchjoin(str_values);
    var percentages := [+](IDlng, 3).[oid]().leftfetchjoin(item).leftfetchjoin(int_values);
    var del_doc     := percentages.ord_uselect(-1LL).hmark(0@0); 
    var add_doc     := percentages.ord_uselect(0LL,lng_nil).hmark(0@0); 
    shred_doc_base(del_doc(bit_nil, del_doc.leftfetchjoin(names), true),
                   add_doc.leftfetchjoin(locations), 
                   add_doc.leftfetchjoin(names), 
                   add_doc.leftfetchjoin(colnames), 
                   add_doc.leftfetchjoin(percentages),
                   stream_nil, ws_id(ws));
}

@- XQuery Document Management Interface For the Algebra backend

The DocmgmTape() is the same proc as play_doc_tape but adabted to the needs of the algebra
backend of pathfinder.
@mil
PROC DocmgmTape(BAT[void,BAT] ws,
                BAT[void,str] location, 
                BAT[void,str] docname, 
                BAT[void,str] colname, 
                BAT[void,lng] percentage) : void
{
    var del_doc := percentage.ord_uselect(-1LL).hmark(0@0);
    var add_doc := percentage.ord_uselect(0LL,lng_nil).hmark(0@0);

    shred_doc_base(del_doc(bit_nil, del_doc.leftfetchjoin(docname), true),
                   add_doc.leftfetchjoin(location), 
                   add_doc.leftfetchjoin(docname), 
                   add_doc.leftfetchjoin(colname), 
                   add_doc.leftfetchjoin(percentage),
                   stream_nil, ws_id(ws));
}

@- MIL Document Management Interface

This is the old MIL administrative interface, now superseded by the 
new {pf:add-doc(),pf:del-doc()} XQuery built-ins. It can continue to exist.

The shred_doc() operation is the MIL document management interface. It uses a "phony" working
set identifier derived from the MIL client-id.
@mil
PROC shred_doc(BAT[void,str] locations, 
               BAT[void,str] names, 
               BAT[void,str] colnames, 
               BAT[void,lng] pageFrees) : void
{
    var us := usec();
    var wsid := WS_MAXID + lng(clientid() + 1);
    var err := CATCH(shred_doc_base(false, 
                                    locations.tmark(0@0), 
                                    names.tmark(0@0), 
                                    colnames.tmark(0@0), 
                                    pageFrees.tmark(0@0), 
                                    stream_nil, wsid));
    ws_free(wsid); 
    if (not(isnil(err))) ERROR(err);
    var ms := (us := usec() - us) / 1000;
    var nr := count(locations);
    if (nr = 1)
        printf("%c Shredded 1 XML document (%s), total time after commit=%lld.%03llds\n",
               chr(35), names.fetch(0), /(ms,1000),%(ms,1000));
    else
        printf("%c Shredded %d XML documents, total time after commit=%lld.%03llds\n",
               chr(35), nr, /(ms,1000),%(ms,1000));
}
ADDHELP("shred_doc", "rittinge", "Oct 2006",
"PARAMETERS:\n\
- BAT[void,str] locations: URIs refering to the xml documents to be shredded)\n\
- BAT[void,str] names:     document names ('alias') in the database\n\
- BAT[void,str] colnames:  collection names ('alias') in the database\n\
- BAT[void,lng] pageFrees: percentage of pages left free in the database\n\
DESCRIPTION:\n\
Shred multiple xml documents to the internal Pathfinder format.",
"pathfinder");

PROC shred_doc(str location, 
               str name, 
               str colname, 
               lng percentage) : void
{
    shred_doc(bat(void,str).append(location).seqbase(0@0), 
              bat(void,str).append(name).seqbase(0@0), 
              bat(void,str).append(colname).seqbase(0@0), 
              bat(void,lng).append(percentage).seqbase(0@0)); 
}
ADDHELP("shred_doc", "rittinge", "Oct 2006",
"PARAMETERS:\n\
- str location: URI refering to the xml documents to be shredded)\n\
- str name:     document name ('alias') in the database\n\
- str colname:  collection name ('alias') in the database\n\
- lng pageFree: percentage of pages left free in the database\n\
DESCRIPTION:\n\
Shred single xml documents to the internal Pathfinder format.",
"pathfinder");

PROC shred_doc(str location, 
               str name) : void
{
    shred_doc(location, name, name, 0LL);
}
ADDHELP("shred_doc", "rittinge", "Oct 2006",
"PARAMETERS:\n\
- str location: URI containing the xml document to be shredded)\n\
- str name:     document name ('alias') in database\n\
DESCRIPTION:\n\
Shred single xml document to the internal Pathfinder format.\n\
(Leave no free pages and do not relate it to a collection.)",
"pathfinder");

PROC delete_doc(BAT[void,str] name) : void
{
    del_doc_base(bit_nil, name, true);
}
PROC delete_doc(str name) : void
{
    delete_doc(bat(void,str).append(name).seqbase(0@0));
}
ADDHELP("delete_doc", "tsheyar", "July 2004",
"PARAMETERS:\n\
str document name\n\
DESCRIPTION:\n\
delete the persistent BATS that store the document.",
"pathfinder");

PROC delete_all_docs(bit cachedOnly) : void
{
    del_doc_base(cachedOnly, doc_name, true);
}
ADDHELP("delete_all_docs", "tsheyar", "July 2004",
"DESCRIPTION:\n\
deletes all persistent document BATs that store xml documents;\n\
with parameter TRUE, only the (implicitely) cached documents are deleted,\n\
with parameter FALSE, also the (explicitely) shredded documents are deleted.",
"pathfinder");



@- MIL Interface for the Document Cache

When fn:doc() is used with a previously unseen URI, it is shredded on the fly and placed into 
the xml document cache (see text below). A number of procs are provided to monitor
and control the behavior of the cache.
@mil
const xmlcache_help := 
"The XML document cache keeps indexed copies of documents that where recently\n\
used in the fn:doc(URI) xquery function.\n\
\n\
The size of the cache is controlled using the 'xquery_cacheMB' setting in\n\
the 'MonetDB.conf' file.\n\
\n\
For file URIs, the cache looks at the last-modification-time of the file on disk\n\
to guarantee that the cached document is still up-to-date for answering queries from.\n\
\n\
For other URIs, *lifetime rules* determine how long documents can stay in the cache.\n\
Each lifetime rule consists of a URI prefix and the registered seconds of lifetime.\n\
\n\
The rule with longest prefix that matches an URI counts. Specifying a lifetime\n\
of 'int_nil' seconds means that the URI will *not* be cached at all.\n\
This is also the default if no prefix matches an URI.\n\
\n\
The name of a cached document is the same as its location (URI). For explicitly\n\
shredded documents (with 'shred_doc(location,name)'), the name is an 'alias' and\n\
may differ from the URI. Explicitly shredded documents fall outside the XML document\n\
cache; documents are only removed at explicit user request (with 'delete_doc(name)').";

PROC xmlcache_add_rule(str uri, 
                       any lifetime) : void 
{
    xmlcache_add_rule(uri, lng(lifetime)); 
}

PROC xmlcache_add_rule(str uri, 
                       lng lifetime) : void 
{
    lock_set(pf_short);
    var err := CATCH({ uri_lifetime.delete(uri); uri_lifetime.insert(uri, lifetime); });
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmlcache_add_rule", "boncz", "May 2005",
"DESCRIPTION:\nadd a new URI lifetime rule.\n\n" + xmlcache_help,  "pathfinder");

PROC xmlcache_del_rule(str uri) : void 
{
    lock_set(pf_short);
    var err := CATCH(uri_lifetime.delete(uri));
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmlcache_del_rule", "boncz", "May 2005",
"DESCRIPTION:\ndeletes an existing URI lifetime rule.\n\n" + xmlcache_help,  "pathfinder");

PROC xmlcache_print_rules() : void 
{
    lock_set(pf_short);
    var err := CATCH({  table(uri_lifetime.hmark(0@0).col_name("URI-prefix"), uri_lifetime.tmark(0@0).col_name("liftime-secs")); });
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmlcache_print_rules", "boncz", "May 2005",
"DESCRIPTION:\nshows all URI lifetime rules.\n\n" + xmlcache_help,  "pathfinder");

PROC xmlcache_print() : void 
{
    lock_set(pf_short);
    var err := CATCH({  table( doc_name.select(str_nil,str_nil).col_name("alias"), doc_location.col_name("URI"), doc_timestamp.col_name("valid-thru"), doc_collection.leftjoin(collection_size).col_name("size")); });
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmlcache_print", "boncz", "May 2005",
        "DESCRIPTION:\nshows the actual content of the XML document cache.\n\n" + xmlcache_help,  "pathfinder");

PROC xmldb_print() : void 
{
    lock_set(pf_short);
    var err := CATCH({  table( tsort(mirror(doc_timestamp.uselect(timestamp_nil)).join(doc_collection).join(collection_name)).col_name("collection"), doc_name.col_name("alias"), doc_location.col_name("URI"), doc_collection.leftjoin(collection_size).col_name("size")); });
    lock_unset(pf_short);
    if (not(isnil(err))) ERROR(err);
}
ADDHELP("xmldb_print", "boncz", "May 2005",
"DESCRIPTION:\nshows the actual content of the persistent XML document database (not the XML document cache).\n\nThis consists of all documents explicitly shredded with shred_doc(URI, alias).",  "pathfinder");

@- startup sequence
@mil
var pf_logger := xquery_logger_create(0, "xquery", "xquery_logs", 
                                      monet_environment.find("gdk_dbname"),
                                      LOGGER_VERSION);
# initialize persistent bats (use BBP as global mechanism to discover initialization)
if (isnil(CATCH(bat("doc_name").count()))) {
    collection_name  := bat("collection_name");
    collection_size  := bat("collection_size");
    doc_collection   := bat("doc_collection");
    doc_name         := bat("doc_name");
    doc_location     := bat("doc_location");
    doc_timestamp    := bat("doc_timestamp");
    uri_lifetime     := bat("uri_lifetime");

    # check that this database was created with a compatible pagesize as requested now
    var pagebits := 16; 
    if (doc_collection.exist(DOCID_PGBIT_HACK)) 
        pagebits := int(doc_collection.find(DOCID_PGBIT_HACK));
    if (pagebits != REMAP_PAGE_BITS) pf_assert("", "Set gdk_mem_pagebits=" + str(pagebits));
} else {
    # create doc_name table in case it does not exist
    collection_name := new(oid,str).persists(true).rename("collection_name");
    collection_size := new(oid,lng).persists(true).rename("collection_size");
    doc_collection  := new(oid,oid).persists(true).rename("doc_collection");
    doc_name        := new(oid,str).persists(true).rename("doc_name");
    doc_location    := new(oid,str).persists(true).rename("doc_location");
    doc_timestamp   := new(oid,timestamp).persists(true).rename("doc_timestamp");
    uri_lifetime    := new(str,lng).insert("tmp", 1LL).persists(true).rename("uri_lifetime");
    doc_collection.insert(DOCID_CURID_HACK, DOCID_MIN); # hack: store next oid as in invalid tuple
    doc_collection.insert(DOCID_PGBIT_HACK, oid(REMAP_PAGE_BITS)); # hack: store pagesize also
    pf_checkpoint(bat(str,void).key(true).reverse().append("uri_lifetime"), true);
}

# check whether this is an older version of the database that needs some conversion
{
    var xqprops;
    if (not(isnil(CATCH(count(xqprops := bat("xquery_props")))))) {
        xqprops := bat(str,str).rename("xquery_props").insert("version","Nov2009").persists(true);
        var b := view_bbp_name().reverse().mirror();
        var d := bat(void,str);
        d.append(reverse([endsWith](b, "vx_hsh_nid").uselect(true)));
        d.append(reverse([endsWith](b, "qn_nid").uselect(true)));
        if (bit(count(d))) {
            printf("# Databases created with a version older than Nov2009 detected:\n");
            printf("# destroying old value-indices..\n"); 
            printf("# (for optimal performance, re-shred all your documents)\n"); 
            [persists]([bat](d), false);
        }  
        subcommit(d.append("xquery_props"));
    }
}

# initialize uri_lifetime
{ var b := split(monet_environment.find("xquery_cacherules"),";").seqbase(0@0);
  var i := [r_search](b,"=").select(2,int_nil);
  uri_lifetime.insert(reverse(b.[string](0,[-](i,1))).leftfetchjoin([lng](b.[string]([+](i,1))))); }

# start the query cache 
const xquery_cache_lim := lng(monet_environment.find("xquery_procMB")) * 1024LL * 1024LL;
xquery_start_query_cache(xquery_cache_lim);

cleantmpdir(msec() / 1000LL);              # delete tmp files in DBFARM/DBNAME/tmp
collection_cleanup(_collection_cleanup()); # silently clean up the repository

mapi_register(xquery_frontend()); # open up mapi client access
if (monet_environment.find("monet_welcome") = "yes") {
    printf("%c MonetDB/XQuery module v11.1.2 loaded (default back-end is '%s')\n", int(35), monet_environment.find("xquery_backend"));
    #                                 ^^^^^^
    # Maintained via vertoo. Please don't modify by hand!
    # Contact MonetDB-developers@lists.sourceforge.net for details and/or assistance.
}
rpcd_start();                     # open up xrpc access
fork(pf_logmanager());            # start checkpointing thread


@= ws_c_decl
#define @1 @2
@h
#ifndef PATHFINDER_H
#define PATHFINDER_H

#include <monet.h>
#include <monettime.h>
#include <lock.h>
#include <monet_context.h>
#include <monet_interpreter.h>
#include <streams.h>
#include <mapi/mapi.h>
#include <sys.h>
#include <gdk_logger.h>
#ifdef LIBPF
#define LIBPF_SUPPORT LIBPF
#endif
#include <pathfinder.proto.h>
#include <pf_support.proto.h>

#define is_fake_project(b) (((b)->htype==TYPE_void) && (BATcount(b)==1) && ((b)->hseqbase==oid_nil))

#ifdef TEXT			/* Windows defines a symbol TEXT */
#undef TEXT
#endif
@:ws(c_decl)@
@:ws_decl(c)@

#define XTRACT_KIND(X)     (X & 63)
#define XTRACT_CONT(X)     (X >> 6)
#define SET_CONT_KIND(X,Y) (X << 6 | Y)

/* exports for XRPC */
pathfinder_export void  xquery_client_engine(mapi_client*);
pathfinder_export char* xquery_method(mapi_client*, int, lng*, char*, char*, char*, char*, char*, char*, lng, lng, lng, lng**, int*, str*, BAT*);
pathfinder_export void  xquery_client_end(mapi_client *, char *); 
pathfinder_export char* xquery_parse_val(int, char*, BAT*, BAT*, BAT* , BAT* , BAT* , char*, oid);
pathfinder_export lng   xquery_2pc_exec(mapi_client *mc, str qid, ptr hdl);

/* the xquery builtin type hierarchy */
typedef struct {
    int parent;
    int monet_tpe;
    int kind;
    char *prefix_loc;
    char *loc;
} xquery_type;

/**
 * simple type-system for fastpath into function cache
 * beware -- must always be kept in sync with pathfinder's type ystem
 */
#define XS_BOOLEAN       0
#define XS_INTEGER       1
#define XS_DECIMAL       2
#define XS_FLOAT         3
#define XS_DOUBLE        4
#define XS_ANYURI        5
#define XS_STRING        6
#define XS_UNTYPEDATOMIC 7 
#define XS_ANYATOMICTYPE 8
#define XS_ANYSIMPLETYPE 9
#define XS_ATTRIBUTE     10 
#define XS_COMMENT       11
#define XS_DOCUMENT      12
#define XS_ELEMENT       13
#define XS_PI            14
#define XS_TEXT          15
#define XS_NODE          16
#define XS_ANYNODE       17
#define XS_ANYTYPE       18
#define XS_ITEM          19
#define XS_ILLEGAL       20 

#define XQUERY_TYPES 20
#define XQUERY_ABSTRACT 8

pathfinder_export xquery_type xquery_types[XQUERY_TYPES+1];
pathfinder_export int xquery_typenr(char* tpe);
#endif
@c
#include "monetdb_config.h"
#include "pathfinder.h"
#include "shredder.h"

#define DOCBAT (ELEM | (1<<6))

/**
 * We define the following hierarchy for the XQuery types
 *
 *                     item
 *                      |
 *                  xs:anyType
 *                      |
 *    -------------------------------------
 *    |                                   |
 * anyNode                        xs:anySimpleType
 *    |                                   |
 *  node                          xs:anyAtomicType
 *    |                             | 
 *    |- attribute                  |- xs:untypedAtomic
 *    |- comment                    |- xs:double
 *    |- document                   |    |
 *    |- element                    |    |- xs:float
 *    |- processing-instruction     |    |- xs:decimal
 *    \_ text                       |         |
 *                                  |         \_ xs:integer
 *                                  |
 *                                  |- xs:string
 *                                  |- xs:boolean
 *                                  \_ xs:anyURI
 */
xquery_type xquery_types[XQUERY_TYPES+1] =
{/*  0 */ {  8, TYPE_bit, BOOL,   "xs:boolean",        "boolean"},
 /*  1 */ {  2, TYPE_lng, INT,    "xs:integer",        "integer"},
 /*  2 */ {  4, TYPE_dbl, DEC,    "xs:decimal",        "decimal"},
 /*  3 */ {  4, TYPE_dbl, DBL,    "xs:float",          "float"},
 /*  4 */ {  8, TYPE_dbl, DBL,    "xs:double",         "double"},
 /*  5 */ {  8, TYPE_str, STR,    "xs:anyURI",         "anyURI"},
 /*  6 */ {  8, TYPE_str, STR,    "xs:string",         "string"},
 /*  7 */ {  8, TYPE_str, STR,    "xs:untypedAtomic",  "untypedAtomic"},
 /*  8 */ {  9, TYPE_str, STR,    "xdt:anyAtomicType", "anyAtomicType"},
 /*  9 */ { 18, TYPE_str, STR,    "xs:anySimpleType",  "anySimpleType"},
 /* 10 */ { 16, TYPE_oid, 97,     "xs:attribute",      "attribute"},
 /* 11 */ { 16, TYPE_oid, DOCBAT, "xs:comment",        "comment"},
 /* 12 */ { 16, TYPE_oid, DOCBAT, "xs:document",       "document"},
 /* 13 */ { 16, TYPE_oid, DOCBAT, "xs:element",        "element"},
 /* 14 */ { 16, TYPE_oid, DOCBAT, "xs:processing-instruction", "processing-instruction"},
 /* 15 */ { 16, TYPE_oid, DOCBAT, "xs:text",           "text"},
 /* 16 */ { 17, TYPE_oid, DOCBAT, "xs:node",           "node"},
 /* 17 */ { 18, TYPE_oid, DOCBAT, "xs:anyNode",        "anyNode"},
 /* FIXME: maybe "anyType" should be put after "anySimpleType" */
 /* 18 */ { 19, TYPE_str, STR,    "xs:anyType",        "anyType"},
 /* 19 */ { 20, TYPE_oid, DOCBAT, "xs:item",           "item"},
 /* 20 */ {  0, TYPE_oid, DOCBAT, NULL,                "illegal type", } };

#define XQUERY_BUFSIZE 16364

/*
 * return xquery type number, given a type string
 */
int
xquery_typenr(char* tpe) 
{
    int i;
    for(i=0; i<XQUERY_TYPES; i++) 
        if (strcmp(tpe, xquery_types[i].prefix_loc) == 0) break;
    return i;
}

/*
 * find the common ancestor of two xquery types
 */
static int
xquery_type_common_ancestor(int t1, int t2) 
{
    if (t1 < 0 || t1 >= XQUERY_TYPES) return t2;
    if (t2 < 0 || t2 >= XQUERY_TYPES) return t1;
    while(t1 != t2) {
        if (t1 < t2) t1 = xquery_types[t1].parent;  
        else         t2 = xquery_types[t2].parent;  
    }
    return t1;
} 

/* representation of a xquery function signature */
typedef struct {
    int update;                   /* is an updating function? */
    int argc;                     /* number of params */
    unsigned int zero;            /* bit-mask that indicates whether param (bit) i can be the empty sequence */
    unsigned int multiple;        /* bit-mask that indicates whether param (bit) i can be a sequence */
    unsigned char tpe[MAXPARAMS]; /* xquery type number (point into above xquery_types table) */
    char name[1];                 /* method name */
} xquery_sig;


/* xquery functions */
typedef struct _xquery_function {
    struct _xquery_function *next;
    oid vid;                      /* vid of first param */
    lng size;                     /* byte size of prepared tree */
    xquery_sig *sig;              /* method signature */
    char* mil;                    /* generated MIL */
    char  proc[1];                /* MIL procname */
} xquery_function;


/* xquery modules */
typedef struct _xquery_module {
    struct _xquery_module *next;
    char* prologue;               /* MIL procs defined in this module */
    char* epilogue;               /* MIL procs undefs for this module */
    xquery_function *functions;   /* functions declared in this module */
    char *module;                 /* module logical url */
    char location[1];             /* module location (at-hint) */
} xquery_module;

/* global list of known modules and their function declarations */
xquery_module *xquery_compiled_modules = NULL;


/* cached functions */
typedef struct _xquery_prepared_function {
    struct _xquery_prepared_function *next;
    xquery_function *def;         /* function definition */
    YYSTREE lt;                   /* cached MIL tree */
} xquery_prepared_function;

/* loaded modules */
typedef struct _xquery_loaded_module {
    struct _xquery_loaded_module *next;
    xquery_module* def;           /* module definition */
    char* ns;                     /* namespace under which the module was loaded */
    int nslen;
} xquery_loaded_module;

/* cached MIL client + Xquery specific context */
typedef struct {
    bit initialized;

    Cntxt stk; /* stack to execute module prologues and non-cacheable queries in */  
    /* MIL child context of ctx->stk; it is marked as a reusable stack frame so variable records are kept */
    Cntxt repeat_stk; /* stk to execute the prepared function trees in  for fast reuse */ 
    Cntxt repeat_max; /* c->maxstk for repeats */ 

    /* query buffer reuse */
    size_t buflen;
    char *buf;
    int cacheid;        /* only reuse from the same cache */
        
    /* error stream (normally fderr=GDKerr=GDKout) */
    stream *fderr;
    char *errbuf;

    /* live BAT handles (note we gave them a memory refcount in BBP) */
    BAT *proc_vid, *var_usage;
    BAT *dbl_values, *int_values, *str_values;
    BAT *fun_vid000, *fun_iter000, *fun_item000, *fun_kind000, *loop000;

    /* pointers into MIL variable records (to set values) */
    lng *time_compile; 
    lng *time_shred; 
    lng *time_exec; 
    lng *time_print; 

    int   *xrpc_shredBAT; 
    char **xrpc_module;
    char **xrpc_method;
    char **xrpc_mode;
    lng   *xrpc_timeout;
    lng   *xrpc_seqnr;
    char **xrpc_qid;
    char **xrpc_caller;
    ptr   *xrpc_hdl;

    char **genType;

    /* counts used to remove query (non-module) procs and stacks after execution */
    size_t var_usage_size; 
    size_t proc_vid_size;

    int mode; /* XQ bitmasks */ 

    /* size of the cached procs */
    size_t cachesize;

    xquery_prepared_function* prepared_functions;
    xquery_loaded_module *loaded_modules; 
} xquery_client;

/* bitmasks for mode */
#define XQ_DEBUG 1
#define XQ_MAPI  2 
#define XQ_ALGEBRA 4
#define XQ_MILPRINT_SUMMER 8
#define XQ_XRPC 16

size_t xquery_client_bytes = 64<<20; /* 64MB of procs should be enough for anyone */
int xquery_cacheid = 0;

MT_Lock pf_compiler_lock;
MT_Lock pf_module_lock;
MT_Lock pf_cache_lock;

#include "compiler/include/compile_interface.h"

/*
 * =================== MIL execution ================================
 *
 * int     
 * xquery_tree_exec(xquery_client *ctx, YYSTREE t, int repeat) { 
 * - execute parsed MIL tree
 *
 * YYSTREE 
 * xquery_mil2tree(xquery_client *ctx, char* buf) { 
 * - parse MIL buffer into a tree
 *
 * int
 * xquery_mil_exec(xquery_client *ctx, char* buf) { 
 * - execute MIL buffer (parse & execute)
 *
 * int
 * xquery_compile_exec(xquery_client *ctx, int options, char* xquery, int is_url, 
 *                     char** prologue, char** query, char** epilogue, char* module) 
 * - translate xquery to MIL and execute
 *
 * all int-returning functions return error(0)/ok(1)
 */

/*
 * execute parsed MIL tree, return error(0)/ok(1)
 */
static int 
xquery_tree_exec(xquery_client *ctx, 
                 YYSTREE t, 
                 int repeat) 
{ 
    ValRecord res;
    int ret = interpret(repeat?ctx->repeat_stk:ctx->stk, t, &res);
    if (repeat) {
        /* record the maxstk. recall we do not ever free stacks for the repeated execs  */ 
        ctx->repeat_max = monet_clients[ctx->stk].maxstk;
    } else if (ctx->repeat_max) {
        /* restore MIL maxstk so we do not lose our repeat cfg */ 
        monet_clients[ctx->stk].maxstk = ctx->repeat_max; 
    }
    if (ret == -TOK_RETURN) {
        /* ignore return value here */
        VALclear(&res);
        ret = 1;
    } else if (ret >= 0) {
        ret = 1;
    } else {
        ret = 0;
    }
    CLEANUP(t);
    return ret;
}

/*
 * parse MIL buffer into a tree
 */
static YYSTREE 
xquery_mil2tree(xquery_client *ctx, 
                char* buf) 
{ 
    Client c = monet_clients + ctx->stk;
    YYSTREE ret = NULL, treebak = c->tree;
    char* bufbak = c->input;
    int listing_bak = c->listing;
    c->listing = 0;
    c->input = buf;
    c->tree = NULL;
    if (parseClient(c, FALSE))
        ret = c->tree;
    c->tree = treebak;
    c->input = bufbak;
    c->listing = listing_bak;
    return ret;
}

/*
 * execute MIL buffer (parse & execute), return error(0)/ok(1)
 */
static int 
xquery_mil_exec(xquery_client *ctx,  
                char* buf) 
{
    int ret = 0;
    YYSTREE t;

    if (ctx->mode&XQ_DEBUG)
        mnstr_write(ctx->fderr, buf, strlen(buf), 1);

    t = xquery_mil2tree(ctx, buf); 
    if (t) {
        ret = xquery_tree_exec(ctx, t, 0);
        Myyfree(t);
    }
    return ret;
}

/*
 * fire off a MIL runtime proc that executes transaction commit (2pc, the perpare request) 
 */
lng
xquery_2pc_exec(mapi_client *mc, str qid, ptr hdl)
{
    xquery_client *ctx = (xquery_client*) mc->fc;
    *(ctx->xrpc_hdl) = hdl;
    *(ctx->xrpc_qid) = GDKstrdup(qid);
    if (!xquery_mil_exec(ctx, "{ var ws; CATCH(execute_update_tape(ws := ws_create(0)); })"))
        return 0;
    return *ctx->xrpc_seqnr; /* return the seqnr we got assigned in MIL (used for message logging) */
}

static char* xquery_parse_ident(char* p); 
static char* xquery_parse_space(char* p); 
static char* xquery_parse_string(char* p, char *buf, int len); 

/*
 * translate xquery to MIL and execute, return error(0)/ok(1)
 * We collect the MIL scripts in three sections (prologue,query,epilogue).
 * The query may be NULL, in which case we mean that it should be ignored. 
 */
#define PFURLCACHE(fcn, query, cache) {\
    char *_url = query;\
    query = PFurlcache(_url, cache);\
    if (query == NULL) {\
        err = (char*) alloca(strlen(_url)+80);\
        sprintf(err, "%s(%s): could not retrieve query\n", fcn, _url);\
}    }
extern char* PFmaxstack;
static int
xquery_compile_exec(xquery_client *ctx, 
                    int options,
                    char* xquery, 
                    int is_url, 
                    char** prologue, 
                    char** query, 
                    char** epilogue,
                    char* module)
{
    int is_mil = 0, len=0, ret = 0;
    char *url = xquery;
    char *err = NULL;

    /* Setting the StandOff flag based on runtime settings */
    if ((GDKgetenv("standoff") != NULL) && (strcmp(GDKgetenv("standoff"),"enabled") == 0))
        options |= COMPILE_OPTION_STANDOFF;

    MT_lock_set(&pf_compiler_lock, "xquery_compile_exec");
    if (is_url) {
        int l = strlen(xquery);
        is_mil = (l > 4 && xquery[l-4] == '.' && xquery[l-3] == 'm' && xquery[l-2] == 'i' && xquery[l-1] == 'l');
        PFURLCACHE("xquery_compile_exec", xquery, !is_mil);
    }
    if (err == NULL) {
        char *del = NULL;
        if (is_mil && query == NULL) {
            *prologue = xquery; *epilogue = NULL;
        } else {
            /* determine how much stack space we have: add extra to
               local var to conservatively estimate bottom of stack
               and to have some slack after last test, then subtract
               stack size to find top */
            PFmaxstack = ((char*) &xquery) + sizeof(size_t)*8192 - THREAD_STACK_SIZE;
#if defined(_MSC_VER) && SIZEOF_SIZE_T == 8
            /* On Windows, the application stack (i.e. the stack of
               the main thread) is 1 MB.  On 64 bit Windows, our
               thread stack (THREAD_STACK_SIZE) is 2 MB, and the above
               calculation can overflow due to the location of the
               application stack.  This is why we do the calculation
               again for a 1 MB stack.
               Did somebody say HACK? */
            if (PFmaxstack > (char*) &xquery)
                    PFmaxstack = ((char*) &xquery) + sizeof(size_t)*8192 - 1024*1024;
#endif
            PFerrbuf = ctx->errbuf;
            PFerrbuf[0] = 0;
            err = PFcompile_MonetDB(xquery, url, prologue, &del, epilogue, options,
                                    *(ctx->genType) /* Using the content of the MIL variable genType
                                                       defined in the prologue is yet another hack
                                                       to make the serialization information visible
                                                       to the algebra backend that does not use prologue
                                                       and epilogue structures. */);
            ctx->errbuf = PFerrbuf; 
            if (err == NULL && module != NULL){
                /* get the module namespace URL Y from pattern: "module namespace X = Y" */
                char *p0 = xquery_parse_space(xquery); 
                err = "xquery_compile_exec: cannot parse module namespace.\n";
                if (strncmp(p0, "module", 6) == 0) {
                    char *p1 = xquery_parse_space(p0+=6); 
                    if (p1 > p0 && strncmp(p1, "namespace", 9) == 0) {
                        char *p2 = xquery_parse_space(p1+=9); 
                        if (p2 > p1) {
                            char* p3 = xquery_parse_ident(p2);
                            if (p3 > p2) {
                                char *p4 = xquery_parse_space(p3); 
                                if (p4 >= p3 && *p4++ == '=') {
                                    char *p5 = xquery_parse_space(p4); 
                                    if (p5 >= p4 && xquery_parse_string(p5, module, 1024) > p5) {
                                        err = NULL; /* success! */
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        if (err == NULL || (prologue && *prologue) || (del && *del)) {
            ret = 1; /* no errors, or some MIL script came out still */
        }
        if (query) {
            *query = del;
        } else if (del) {
            free(del); /* we are ignoring the query part apparently */
        }
    }
    if (err) {
        len = strlen(err);
        if (ctx->mode&XQ_MAPI) {
            /* put ! before error lines */
            char *p = err, *q = err;

            while (*p) {
                if (*p++ == '\n')
                    len++;
            }
            err = (char*) alloca(len+3);
            *err = strncmp(q, "warning", 7)?'!':'#'; 
            for (p = err + 1; *q; q++) {
                *p++ = *q;
                if (*q == '\n')
                    *p++ = strncmp(q+1, "warning", 7)?'!':'#'; 
            }
            /* guard against errors that do not terminate in a newline */
            if (q > err && q[-1] != '\n')
                *p++ = '\n';
            else if (p[-1] == '!')
                p--;
            *p = 0;
            len = p - err;
        }
    }
    MT_lock_unset(&pf_compiler_lock, "xquery_compile_exec");
    
    /* write errors and debug info on client stream, however, in case of
     * XRPC, we leave the error message to the XRPC receiver, so that it
     * can create correct SOAP Fault message. */
    if ( !(ctx->mode&XQ_XRPC) && err && *err) mnstr_write(ctx->fderr, err, len, 1);

    /* execute the three MIL sections */
    if (ret && *prologue && **prologue)
        ret = xquery_mil_exec(ctx, *prologue);

    if (ret && query) {
        if (*query && **query)
            ret = xquery_mil_exec(ctx, *query);

        if (ret && *epilogue && **epilogue)
            ret = xquery_mil_exec(ctx, *epilogue);
    }
    return ret?ret+is_mil:0;
}

/*
 * =================== function admin ================================
 *
 * xquery_sig*
 * xquery_sig_init(char *proc)
 * - infer the xquery function signature from the mangled MIL procname (return NULL on error)
 *
 * int
 * xquery_sig_match(xquery_sig *sig, int argc, int* mincnt, int* maxcnt, int *argtpe)
 * - check whether sig can match the actual parameters (return true/false)
 *
 * xquery_function* 
 * xquery_resolve(xquery_client *ctx, char *ns, char *method, int argc, int *mincnt, int* maxcnt, int* argtpe)
 * - resolve a method call in the current xquery context (return NULL if not resolved)
 *
 * char*
 * xquery_function_call(xquery_client *ctx, lng usec, lng *seqnr, char *ns, char *module, char *method, 
 *                      lng seqnr, char* qid, char* caller, lng timeout, char* mode, 
 *                      int argc, int itercnt, int** argcnt, int* argtpe, char** argval, BAT *shredBAT)
 * - call a function ns:method(). try to use the function cache (ie re-use a cached MIL tree).
 *   otherwise generate MIL yourself, interpret it (and cache it). Returns error string (NULL if ok).
 */

/*
 * infer the xquery function signature from the mangled MIl procname (return NULL on error)
 */
static xquery_sig* 
xquery_sig_init(char *proc) 
{
    char *cur = (char*) alloca(strlen(proc)); 
    xquery_sig* sig;
    int update = 0, len = 0;

    if (proc[0] == 'u' && proc[1] == 'p') {
        update = 1; /* proc names starting with 'up' are updating functions */
    } else if (proc[0] == 'd' && proc[1] == 'm') {
        update = 2; /* proc names starting with 'dm' are document management functions */
    } else if (proc[0] != 'f' || proc[1] != 'n') {
        return NULL;
    }

    proc = strchr(proc, '_'); /* skip fnXXXXXXXX_' */
    if (proc == NULL) return NULL;
    strcpy(cur, ++proc);
    sig = (xquery_sig*) GDKmalloc(sizeof(xquery_sig)+strlen(proc)); 
    if (sig == NULL) return NULL;
    sig->update = update;
    sig->argc = sig->zero = sig->multiple = 0;

    /* get method name */
    while(cur[0]) {
        if (cur[0] == '_' && cur[1] == '_') {
            cur++; /* unescape '_' */
        } else if (cur[0] == '_' && (cur[1] == '4' || cur[1] == '5') && cur[2] == '_') {
            sig->name[len++] = (cur[1] == '4')?'-':'.'; cur += 3; continue;
        } else if (cur[0] == '_') {
            break; /* unescaped '_' => end of function name */
        }
        sig->name[len++] = *cur++;
    }
    sig->name[len] = 0;
    while(*cur++ == '_') {
        char *tpe = cur;
                
        /* parse namespace part */
        for(len=0; cur[0]; cur++) {
            if (cur[0] == '_' && cur[1] == '_') {
                tpe[len++] = '_';
                cur++; /* unescape '_' */
            } else if (cur[0] != '_') {
                tpe[len++] = cur[0];
            } else {
                tpe[len++] = ':';
                cur++; break;
            }
        }
        /* parse type part */
        for(; cur[0]; cur++) {
            if (cur[0] == '_' && cur[1] == '_') {
                tpe[len++] = '_';
                cur++; /* unescape '_' */
            } else if (cur[0] != '_') {
                tpe[len++] = cur[0];
            } else {
                break;
            }
        }
        if (cur[-1] == '0') {
            sig->zero |= 1<<sig->argc; 
        } else if (cur[-1]  == '2') {
            sig->zero |= 1<<sig->argc; 
            sig->multiple |= 1<<sig->argc; 
        } else if (cur[-1]  == '3') {
            sig->multiple |= 1<<sig->argc; 
        }
        cur[-1] = 0;
        if ((sig->tpe[sig->argc++] = xquery_typenr(tpe)) >= XQUERY_TYPES) {
            /* unknown type: don't cache this function */
            GDKfree(sig);
            return NULL;
        }
    }
    return sig;
}


/*
 * check whether sig can match the actual parameters (return true/false)
 */
static int
xquery_sig_match(xquery_sig *sig, int argc, int* mincnt, int* maxcnt, int *argtpe) 
{
    int i, tpe[MAXPARAMS];
    for(i=0; i < argc; i++) {
        if (mincnt[i] == 0 && !(sig->zero & (1<<i))) return 0; 
        if (maxcnt[i] > 1 && !(sig->multiple & (1<<i))) return 0; 
        /* If (argtpe[i] == -1), then the actual value of this parameter
         * is an empty sequence.
         * If the actual value of this parameter is an empty sequence,
         * and this parameter can be an empty sequence, we _should not_
         * check the parameter type, instead, we copy the type
         * information from the function signature into 'tpe' */
        if (argtpe[i] == -1 && (sig->zero & (1<<i))) {
            tpe[i] = sig->tpe[i];
        } else {
            tpe[i] = argtpe[i];
            while(sig->tpe[i] != tpe[i]) {
                if (tpe[i] >= XQUERY_TYPES) return 0;
                tpe[i] = xquery_types[tpe[i]].parent;
            }
        }
    }
    for(i=0; i< argc; i++) {
        argtpe[i] = tpe[i];
    }
    return 1;
}

/*
 * resolve a method call in the current xquery context (return NULL if nonresolved)
 */
static xquery_function*
xquery_resolve(xquery_client *ctx, char *ns, char *method, int argc, int *mincnt, int* maxcnt, int* argtpe) 
{
    xquery_loaded_module *mod = ctx->loaded_modules;
    int nslen = strlen(ns);

    /* look up ns and method */
    while(mod) {
        if (mod->nslen == nslen && strncmp(mod->ns, ns, nslen) == 0) {
            xquery_function *fun = mod->def->functions;
            while(fun) {
                if (argc == fun->sig->argc && strcmp(method, fun->sig->name) == 0) {
                    if (xquery_sig_match(fun->sig, argc, mincnt, maxcnt, argtpe)) return fun;
                }
                fun = fun->next;
            }
        }
        mod = mod->next;
    }
    return NULL;
}


@= seqbase
    BATseqbase(@4@1, @2);
    if (mil) {
        snprintf(mil+m, XQUERY_BUFSIZE-m, "@1.seqbase(" @3 ");\n");
        mil[XQUERY_BUFSIZE - 1] = 0;
        m += strlen(mil+m);
    }
@= bunappend
    if (BUNappend(@6@1, @2, FALSE) == NULL) return "xquery_method: allocation error while inserting in @1";
    if (mil) {
        snprintf(mil+m, XQUERY_BUFSIZE-m, "@1.append(" @4 @5 ");\n", @3 @2);
        mil[XQUERY_BUFSIZE - 1] = 0;
        m += strlen(mil+m);
    }
@c
char*
xquery_parse_val(int t, char* v, BAT* item, BAT* kind, BAT* int_values, BAT* dbl_values, BAT* str_values, char* mil, oid cont) {
    int mt = xquery_types[t].monet_tpe;
    int k = xquery_types[t].kind;
    int vallen = sizeof(oid)+sizeof(oid);
    size_t m = mil?strlen(mil):0;
    oid buf[2], id;
    void *val = (void*) buf;

    if (mt == TYPE_str) {
        val = v;
    } else if (mt == TYPE_oid) {
        buf[0] = (oid) ((size_t) v);
    } else if (ATOMfromstr(mt, &val, &vallen, v) <= 0) {
       return "xquery_parse_val: illegal value.\n";
    }
    if (mt == TYPE_bit) {
        id = (*(bit*) val == TRUE);
        @:bunappend(item, &id, *(oid*), OIDFMT, "@0")@
    } else if (mt == TYPE_lng) {
        @:bunappend(int_values, val, *(lng*), LLFMT, "LL")@
        BATiter ivi = bat_iterator(int_values);
        id = *(oid*) BUNhead(ivi, BUNfnd(BATmirror(int_values), val));
        @:bunappend(item, &id, *(oid*), OIDFMT, "@0")@
    } else if (mt == TYPE_dbl) {
        @:bunappend(dbl_values, val, *(dbl*), "%g")@
        BATiter idi = bat_iterator(dbl_values);
        id = *(oid*) BUNhead(idi, BUNfnd(BATmirror(dbl_values), val));
        @:bunappend(item, &id , *(oid*), OIDFMT, "@0")@
    } else if (mt == TYPE_str) {
        @:bunappend(str_values, val, (str), "\"%s\"")@
        BATiter isi = bat_iterator(str_values);
        id = *(oid*) BUNhead(isi, BUNfnd(BATmirror(str_values), val));
        @:bunappend(item, &id, *(oid*), OIDFMT, "@0")@
    } else {
        @:bunappend(item, val, *(oid*), OIDFMT, "@0")@
        k = (cont << 6) | ((t == XS_ATTRIBUTE)?ATTR:ELEM);
    }
    @:bunappend(kind, &k, *(int*), "%d")@
    return NULL;
}

/*
 * call a function ns:method(). try to use the function cache (ie re-use a cached MIL tree).
 * otherwise generate MIL yourself, interpret it (and cache it). Returns error string (NULL if ok).
 */
static char xquery_function_error[80] = "xquery_method: error during execution.\n";
static char* 
xquery_function_call(xquery_client *ctx, 
                     lng usec, 
                     char *ns, 
                     char *module, 
                     char *method, 
                     lng *seqnr, 
                     char *qid,
                     char *caller,
                     lng timeout,
                     char* mode,
                     lng argc, 
                     lng itercnt, 
                     lng** argcnt, 
                     int* argtpe, 
                     char** argval, 
                     BAT *shredBAT) 
{
    xquery_prepared_function *prepfun = ctx->prepared_functions;
    xquery_function *fun;
    int i, j, k, l, m=0, tpe[MAXPARAMS], mincnt[MAXPARAMS], maxcnt[MAXPARAMS];
    char debug[XQUERY_BUFSIZE], *src, *mil=debug, *cur = mil, *end = mil + XQUERY_BUFSIZE-1;

    /* determine minimum and maximum sequence of the parameters, and the common ancestor type of each sequence  */
    for(j=0; j<argc; j++) {
        mincnt[j] = maxcnt[j] = argcnt[0][j];
        tpe[j] = -1;
    }
    for(l=i=0; i<itercnt; i++) {
        for(j=0; j<argc; j++) {
            if (argcnt[i][j] > maxcnt[j]) maxcnt[j] = argcnt[i][j];
            if (argcnt[i][j] < mincnt[j]) mincnt[j] = argcnt[i][j];
            for(k=0; k<argcnt[i][j]; k++,l++) {
                tpe[j] = xquery_type_common_ancestor(tpe[j],argtpe[l]);
            }
        }
    }

    /* try to resolve the parameters */
    fun = xquery_resolve(ctx, ns, method, argc, mincnt, maxcnt, tpe);
    if (fun == NULL)
        return (char*) -1; /* no such udf. but it may be a built-in, actually */

    /* create a prepared function record for this MIL client */
    while(prepfun && prepfun->def != fun) prepfun = prepfun->next;
    if (prepfun == NULL) {
        prepfun = (xquery_prepared_function*) GDKmalloc(sizeof(xquery_prepared_function));
        if (prepfun == NULL)
            return "xquery_function_call: allocation failed.\n";
        prepfun->def = fun;
        prepfun->lt = NULL;
        prepfun->next = ctx->prepared_functions;
        ctx->prepared_functions = prepfun;
    }
        
    /* generate small MIL query that calls the function PROC */
    if (fun->mil == NULL) {
        MT_lock_set(&pf_cache_lock, "xquery_function_call");
        if (fun->mil == NULL) {
            int ret;

            /* create working set */
            src = (char*) PFstartMIL(fun->sig->update);
            while(*src && cur < end) *cur++ = *src++;

            if (shredBAT) {
                /* add shredded RPC request message to the working set */
                src = " var cont := ws_opencoll(ws, bat(xrpc_shredBAT), \"\", TEMP_DOC);\n"
                      " fun_kind000.inplace([+](fun_kind000.ord_select(64,int_nil),<<(int(cont),6) - 64));\n";
                /* the above weirdness is to handle nested requests made from a cached ws in 2PC.
                 * in that case, the XRC message is *not* cont=1, but something else, therefore
                 * we have to adjust the kind numbers of all node values.
                 */
                while(*src && cur < end) *cur++ = *src++;
            }

            /* call UDF */
            ret = snprintf(cur, XQUERY_BUFSIZE-(cur-mil), 
                           PFudfMIL(), 
                           fun->proc, 0, 0, 0,0, 0, 0, 0, 0, fun->sig->name, fun->sig->name, 0, 0, 0, 0);
            mil[XQUERY_BUFSIZE - 1] = 0;
            if (ret > 0) cur += ret;

            /* destroy working set */
            ret = snprintf(cur, XQUERY_BUFSIZE-(cur-mil), "%s", PFstopMIL(fun->sig->update));
            mil[XQUERY_BUFSIZE - 1] = 0;
            if (ret > 0) cur += ret;

            /* done! execute the script */
            if (cur >= end) {
                return "xquery_function_call: generated MIL query exceeds buffer size.\n";
            }
            *cur = 0;
            fun->mil = GDKstrdup(mil);
        }
        MT_lock_unset(&pf_cache_lock, "xquery_function_call");
    }
    if (fun->mil == NULL)
       return "xquery_function_call: malloc failed.\n";

    /* if no MIL tree is available, create it now */
    if (prepfun->lt == NULL) {
        prepfun->lt = xquery_mil2tree(ctx, fun->mil);
        if (prepfun->lt == NULL) {
            GDKfree(prepfun);
            return "xquery_function_call: error during parsing .\n";
        }
        if (fun->size == 0) {
            size_t dummy;
            fun->size = Myysize(prepfun->lt, &dummy);
        }
        ctx->cachesize += fun->size; /* the prepared function cache is kept within a certain size */
    }

    /* put the actual parameters into the fun_* bats (and *_values containers) */
    if (!(ctx->mode&XQ_DEBUG)) mil = NULL;
    for(l=j=0; j<itercnt; j++) {
        for(i=0; i<argc; i++) {
            oid vid = i + fun->vid, iter = j+1;
            if (i == 0 && j > 0) {
                @:bunappend(loop000, &iter, *(oid*), OIDFMT, "@0", ctx->)@
            }
            for(k=0; k<argcnt[j][i]; k++,l++) {
                /* 'hack': perform simple atomic casts using Monet's ATOMfromstr */
                /* it is doubtful how well pathfinder supports other
                 * casts and what we should do here in those cases.
                 */
                int t = argtpe[l];
                if (fun->sig->tpe[i] < XQUERY_ABSTRACT)
                    t = fun->sig->tpe[i]; /* just parse the value string as if it was from the desired type */ 

                /* parse value from tpe/str into a MonetDB BAT representation. */ 
                cur = xquery_parse_val(t, argval[l], ctx->fun_item000, ctx->fun_kind000, 
                                       ctx->int_values, ctx->dbl_values, ctx->str_values, mil, 1); 
                if (cur) return cur;
                @:bunappend(fun_vid000, &vid, *(oid*), OIDFMT, "@0", ctx->)@
                @:bunappend(fun_iter000, &iter, *(oid*), OIDFMT, "@0", ctx->)@
            }
        }
    }
    @:seqbase(loop000, 0, "0@0", ctx->)@
    @:seqbase(fun_vid000, 0, "0@0", ctx->)@
    @:seqbase(fun_iter000, 0, "0@0", ctx->)@
    @:seqbase(fun_kind000, 0, "0@0", ctx->)@
    @:seqbase(fun_item000, 0, "0@0", ctx->)@

    /* for debugging purposes, we simulate a full MIL on the log; even if parts are cached */
    if (mil) { 
        /* MIL corresponding to BUNins calls we made from C */
        mnstr_write(ctx->fderr, mil, strlen(mil), 1);
        /* MIL corresponding to UDF call sequence (even if we already have it cached) */
        mnstr_write(ctx->fderr, fun->mil, strlen(fun->mil), 1);
    }

    /* override some query-specific MIL runtime variables from here */
    if(ctx->mode & XQ_XRPC) {
        @:setvar(xrpc_module,module)@
        @:setvar(xrpc_method,method)@
        @:setvar(xrpc_mode,mode)@
        @:setvar(xrpc_qid,qid)@
        @:setvar(xrpc_caller,caller)@
        (*ctx->xrpc_timeout) = timeout;
        ctx->xrpc_shredBAT[0] = shredBAT?shredBAT->batCacheid:0;
    }
    *(ctx->time_compile) = GDKusec() - usec;

    /* Done preparing the query. Time to (re-)execute the MIL tree */
    if (xquery_tree_exec(ctx, prepfun->lt, 1)) {
        if(seqnr)
            *seqnr = *ctx->xrpc_seqnr;
        return NULL;
    }
    return xquery_function_error;
}


/*
 * =================== module admin ================================
 *
 * void
 * xquery_module_free(xquery_module *mod)
 * - free module structure 
 *
 * xquery_module* 
 * xquery_module_compile(xquery_client *ctx, char *url)
 * - get an xquery module, compile it and cache it (return NULL on error)
 *
 * char*
 * xquery_module_load(xquery_client *ctx, char *ns, char *module, char *location)
 * - check whether we already loaded the module, or if we already 
 *   have it cached. If not, fetch&compile. Returns error string or NULL if ok.
 */

/* 
 * free module structure 
 */
static void 
xquery_module_free(xquery_module *mod) 
{
    xquery_function *fun = mod->functions;
    while(fun) {
        xquery_function *del = fun;
        fun = fun->next;
        if (del->mil) GDKfree(del->mil);
        GDKfree(del);
    }
    if (mod->epilogue) free(mod->epilogue);
    if (mod->prologue) free(mod->prologue);
    GDKfree(mod);
}

/*
 * get an xquery module, compile it and cache it (return NULL on error)
 */
xquery_module*
xquery_module_compile(xquery_client *ctx, 
                      char *url) 
{
    xquery_module *mod = NULL;
    BAT *b = ctx->proc_vid;
    int cnt = BUNlast(b);
    int ret, url_len = strlen(url)+1;
    BUN p, q;

    mod = GDKmalloc(sizeof(xquery_module)+url_len+1024);
    if (mod == NULL) return NULL;
    memset(mod, 0, sizeof(xquery_module)+url_len+1024);
    strcpy(mod->location, url);
    mod->module = mod->location + url_len;

    ret = xquery_compile_exec(ctx, 0, url, 1, &mod->prologue, NULL, &mod->epilogue, mod->module);
    if (!ret) {
        xquery_module_free(mod);
        return NULL;
    } 
    if (ret == 2) mod->module = NULL; /* for MIL modules (ret == 2), mod->module == NULL (we don't know it :-( )*/

    BATiter bi = bat_iterator(b);
    for(p = cnt, q = BUNlast(b); p < q; p++) {
        char *proc = (char*) BUNhead(bi,p);
        xquery_function *fun = (xquery_function*) GDKmalloc(sizeof(xquery_function)+strlen(proc));
        if (fun == NULL) {
            xquery_module_free(mod);
            return NULL;
        }
        strcpy(fun->proc, proc);
        fun->vid = *(lng*) BUNtail(bi,p);
        fun->sig = xquery_sig_init(fun->proc);
        fun->mil = NULL;
        fun->size = 0;
        if (fun->sig == NULL) {
            GDKfree(fun);
        } else {
            fun->next = mod->functions;
            mod->functions = fun;
        }
    }
    mod->next = xquery_compiled_modules;
    xquery_compiled_modules = mod;
    return mod;
}

/*
 * check whether we already loaded the module, or if we already have it cached. 
 * If not, fetch&compile. Returns error string or NULL if ok.
 */
static char* 
xquery_module_load(xquery_client *ctx, 
                   char *ns, 
                   char *module,
                   char *location) 
{
    xquery_loaded_module *mod, *prev = NULL;
    int nslen = strlen(ns);

    /* check whether it was already loaded in this query */
    for(mod = ctx->loaded_modules; mod; prev = mod, mod = mod->next) {
        if (strcmp(mod->def->location, location) == 0) {
            if (mod->nslen == 0) {
                if (prev) prev->next = mod->next;
                else ctx->loaded_modules = NULL;
                break; /* put module at front of list */
            }
            if (mod->def->module && module && strcmp(mod->def->module, module)) 
                return "xquery_module_load: import module statement does not match module namespace declaration.\n";
        }
    }
    if (mod == NULL) {
        xquery_module *def;
        mod = (xquery_loaded_module*) GDKmalloc(sizeof(xquery_loaded_module));
        if (mod == NULL) 
            return "xquery_module_load: could not allocate.\n";
        mod->def = NULL;

        MT_lock_set(&pf_module_lock, "xquery_module_load");
        for(def = xquery_compiled_modules; def; def = def->next) 
            if (strcmp(def->location, location) == 0) break;

        if (def == NULL){
            mod->def = xquery_module_compile(ctx, location);
        }
        MT_lock_unset(&pf_module_lock, "xquery_module_load");

        /* if a compiled module was found, we still need to execute it */
        if (def && xquery_mil_exec(ctx, def->prologue))
            mod->def = def; /* TODO: find a way to share PROC defs between MIL sessions */

        if (mod->def == NULL) {
            GDKfree(mod);
            return "xquery_module_load: could not load module.\n";
        }
        ctx->var_usage_size = BATcount(ctx->var_usage);
        ctx->proc_vid_size = BATcount(ctx->proc_vid);
    } else if (ctx->mode&XQ_DEBUG) {
        mnstr_write(ctx->fderr, mod->def->prologue, strlen(mod->def->prologue), 1);
    }
    mod->ns = ns;
    mod->nslen = nslen;
    mod->next = ctx->loaded_modules;
    ctx->loaded_modules = mod;

    if (mod->def->module && module && strcmp(mod->def->module, module)) 
        return "xquery_module_load: import module statement does not match module namespace declaration.\n";
    return NULL;
}

/*
 * wait for all queries to finish; free all clients; clear PF url cache
 * freeing the cached YYTREEs in the xquery clients is done lazily 
 * (postponed to the next initialization of the record)
 */
static void
xquery_client_flushall(void)
{
    int i, wait = 100;
    while(wait) {
        MT_lock_set(&pf_cache_lock, "xquery_client_flushall");
        i = active_clients("xquery");
        if (i == 0) {
            /* ok, no xquery clients are active */
            xquery_module *mod = xquery_compiled_modules;
            while(mod) {
                xquery_module *del = mod;
                mod = mod->next;
                xquery_module_free(del);
            }
            xquery_compiled_modules = NULL;
            PFurlcache_flush();
            wait = 0;
        }
        MT_lock_unset(&pf_cache_lock, "xquery_client_flushall");
        MT_sleep_ms(wait);
    }
}

/*
 * flush the cache. 
 */
int
CMDxquery_start_query_cache(lng *maxsize)
{
    xquery_client_flushall();
    if (*maxsize > 0) xquery_client_bytes = *maxsize;
    xquery_cacheid++; /* old clients need to clean up their resources */
    return GDK_SUCCEED;
}


/*
 * =================== client session management ================================
 *
 * xquery_client *
 * xquery_client_alloc(mapi_client *mc);
 * - allocate a new xquery client, returns error message (NULL on success)
 *
 * char*
 * xquery_client_init(mapi_client *mc);
 * - initialize a new xquery cache context, returns error string (NULL if ok)
 *
 * void
 * xquery_client_free(mapi_client *mc);
 * - free a xquery cache context (terminate)
 *
 * void
 * xquery_client_reset(xquery_client *ctx, char *err);
 * - reset xquery execution for next statement
 *
 * void
 * xquery_client_end(xquery_client *ctx, char *err);
 * - end of xquery execution (struct stays alive for reuse). 
 */

@= find_var
    v = VARfind(&ctx->stk, "@1");
    if (v == NULL) 
        return "xquery_client_alloc_: failed to lookup @1 variable.\n"; 
    if (v->binding.vtype != TYPE_@2) 
        return "xquery_client_alloc_: @1 variable has wrong type != int.\n"; 
    ctx->@1 = &v->binding.val.@3; 

@= find_bat
{   Variable v = VARfind(&ctx->stk, "@1");
    char buf[256];
    snprintf(buf, sizeof(buf), "mapi_%d_@1", ctx->stk);
    buf[sizeof(buf) - 1] = 0;
    ctx->@1 = NULL;
    if (v && v->binding.vtype == TYPE_bat) {
        ctx->@1 = BATdescriptor(v->binding.val.bval);
        BBPrename(v->binding.val.bval, buf); 
    }
    if (ctx->@1 == NULL) return "xquery_client_alloc: failed to lookup @1 variable.\n"; 
}
@c

static xquery_client*
xquery_client_new(Cntxt stk) {
    xquery_client *ctx = (xquery_client*)GDKmalloc(sizeof(xquery_client));
    if (ctx) {
        memset(ctx, 0, sizeof(xquery_client));
        ctx->stk = stk; 
        ctx->fderr = GDKerr;
        ctx->errbuf = GDKmalloc(GDKMAXERRLEN);
    }
    return ctx; 
}

static char *
xquery_client_alloc_(xquery_client *ctx)
{
    Variable v;

    if (!xquery_mil_exec(ctx, (char*) PFinitMIL()))
        return "xquery_client_alloc: failed to execute init script.\n"; 

    @:find_var(genType,str,sval)@

    @:find_var(xrpc_shredBAT,int,ival)@
    @:find_var(xrpc_module,str,sval)@
    @:find_var(xrpc_method,str,sval)@
    @:find_var(xrpc_caller,str,sval)@
    @:find_var(xrpc_hdl,ptr,pval)@
    @:find_var(xrpc_qid,str,sval)@
    @:find_var(xrpc_mode,str,sval)@
    @:find_var(xrpc_seqnr,lng,lval)@
    @:find_var(xrpc_timeout,lng,lval)@

    @:find_var(time_exec,lng,lval)@
    @:find_var(time_print,lng,lval)@
    @:find_var(time_shred,lng,lval)@
    @:find_var(time_compile,lng,lval)@

    @:find_bat(proc_vid)@
    @:find_bat(var_usage)@
    @:find_bat(dbl_values)@
    @:find_bat(int_values)@
    @:find_bat(str_values)@
    @:find_bat(fun_vid000)@
    @:find_bat(fun_iter000)@
    @:find_bat(fun_item000)@
    @:find_bat(fun_kind000)@
    @:find_bat(loop000)@

    ctx->var_usage_size = BATcount(ctx->var_usage);
    ctx->proc_vid_size = BATcount(ctx->proc_vid);
    ctx->cachesize = 0;
    ctx->loaded_modules = NULL;
    ctx->prepared_functions = NULL;
    ctx->cacheid = xquery_cacheid;
    ctx->buflen = 0;
    ctx->buf = GDKmalloc(XQUERY_BUFSIZE);
    if (ctx->buf == NULL) 
        return "xquery_client_alloc: failed to allocate.\n";
    ctx->buflen = XQUERY_BUFSIZE-1;
    ctx->repeat_stk = CNTXTnew(ctx->stk);
    ctx->initialized = 0;
    CNTXTuse(ctx->repeat_stk);
    monet_cntxt[ctx->repeat_stk].reuse = TRUE;

    return NULL;
}

/* 
 * allocate a new xquery client, returns client cntxt (0 on error)
 */
static char* 
xquery_client_alloc(mapi_client *mc)
{
    xquery_client *ctx = xquery_client_new(mc->stk);

    mc->fc = ctx;
    if (!ctx) {
        return "!ERROR: no space to allocate xquery client\n";
    } else {
        return xquery_client_alloc_(ctx);
    }
}

@= unfix
    if (ctx->@1) BBPunfix(ctx->@1->batCacheid);
    ctx->@1 = NULL;
@c
/* 
 * free a xquery cache context (terminate).
 */
static void
xquery_client_free_(xquery_client *ctx)
{
    /* free all prepared functions and loaded modules */
    xquery_prepared_function *fun = ctx->prepared_functions; 
    xquery_loaded_module *mod= ctx->loaded_modules; 

    while(fun) {
        xquery_prepared_function *del = fun;
        fun = fun->next;
        if (del->lt) Myyfree(del->lt);
        GDKfree(del);
    }
    while(mod) {
        xquery_loaded_module *del = mod;
        mod = mod->next;
        /* free all modules */
        if (del->def->epilogue && !xquery_mil_exec(ctx, del->def->epilogue))
            fprintf(stderr, "xquery_client_free: client %d error dropping %s\n", ctx->stk, del->def->location);
        GDKfree(mod);
    }
    ctx->loaded_modules = NULL;
    ctx->prepared_functions = NULL;

    /* free the query buffer */
    if (ctx->buf) GDKfree(ctx->buf);
    ctx->buf = NULL;
    ctx->buflen = 0;
    ctx->xrpc_shredBAT = NULL;

    /* unfix the BAT handles */
    @:unfix(int_values)@
    @:unfix(dbl_values)@
    @:unfix(str_values)@
    @:unfix(fun_vid000)@
    @:unfix(fun_iter000)@
    @:unfix(fun_item000)@
    @:unfix(fun_kind000)@
    @:unfix(loop000)@
    @:unfix(proc_vid)@
    @:unfix(var_usage)@

    /* close the MIL client session */
    monet_cntxt[ctx->repeat_stk].reuse = FALSE;
    CNTXTclear(ctx->repeat_stk);
    CNTXTfree1(ctx->repeat_stk);
    CNTXTdelete(ctx->repeat_stk);
}

static void 
xquery_client_free(mapi_client *mc) 
{
    xquery_client_free_(mc->fc);
}


/*
 * initialize a new xquery cache context, returns error string (NULL if ok)
 */ 
static char *
xquery_client_init_(xquery_client *ctx )
{
    ctx->mode = 0;
    if (!ctx->initialized) {
        ctx->initialized = 1;
        if (!xquery_mil_exec(ctx, (char*) PFvarMIL()))
            return "xquery_client_init: failed to execute variable declarations.\n"; 
    }
    return NULL;
}


static char* 
xquery_client_init(mapi_client *mc) 
{ 
    xquery_client *ctx = mc->fc;
    char* err = xquery_client_init_(ctx);

    ctx->fderr = mc->c->fdout;

    if (err == 0 && (ctx->cacheid != xquery_cacheid || ctx->cachesize > xquery_client_bytes)) {
        /* re-initialize everything */
        xquery_client_free_(ctx);
        err = xquery_client_alloc_(ctx);
    }
    if (err)
        fprintf(stderr, "xquery_client_init: client %d %s\n", mc->stk, err);
    return err;
}



/* 
 * end of xquery execution (struct stays alive for reuse).
 */
static void 
xquery_client_reset(xquery_client *ctx, char *err) 
{
    oid zero = 0, one = 1;

    /* cleanup all variables generated by PFvarMIL(); 
     * these persist accross sessions as the generated PROCs refer to them.
     * This mostly for leakage monitoring purposes, such that dir(); does
     * not show any tmp_X bats. Not doing this would not cause a leak; next 
     * session the old values are overwritten (and cleaned up in MIL).
     */
    Variable v = monet_cntxt[ctx->stk].var;
    while(v && strcmp(v->name, "v_vid000")) {
        if (v->binding.vtype == TYPE_bat) {
          BBPdecref(v->binding.val.bval, TRUE);
          v->binding.vtype = TYPE_int;
        }
        v = v->next;
    }

    /* undo any inserts by the query into the var_usage bats */
    BATsetcount(ctx->var_usage, ctx->var_usage_size);
    BATsetcount(ctx->proc_vid, ctx->proc_vid_size);

    /* empty all bats (static variables) */
    BATclear(ctx->loop000);
    BUNins(ctx->loop000, &zero, &one, FALSE);

    BATclear(ctx->fun_vid000);
    BATclear(ctx->fun_iter000);
    BATclear(ctx->fun_kind000);
    BATclear(ctx->fun_item000);
    BATclear(ctx->dbl_values);
    BATclear(ctx->int_values);
    BATclear(ctx->str_values);
    BUNappend(ctx->str_values, "", FALSE);

    if(ctx->mode & XQ_XRPC) {
        *ctx->xrpc_shredBAT = int_nil; 
        (*ctx->xrpc_qid)[0] = 0;
        (*ctx->xrpc_caller)[0] = 0;
        (*ctx->xrpc_module)[0] = 0;
        (*ctx->xrpc_method)[0] = 0;
        (*ctx->xrpc_mode)[0] = 0;
        *ctx->xrpc_timeout = 30000LL;
    }

    MT_lock_set(&pf_cache_lock, "xquery_client_reset");
    /* only deactivate the loaded modules */
    xquery_loaded_module *mod= ctx->loaded_modules; 
    while(mod) {
            mod->nslen = 0;
            mod->ns = NULL; 
            mod = mod->next;
    }
    MT_lock_unset(&pf_cache_lock, "xquery_client_reset");

    if (err) 
        fprintf(stderr, "xquery_server: client %d %s\n", ctx->stk, err);
}


/*
 * ========== parse xquery to identify 'import module's and function calls ==========
 *
 * char*
 * xquery_parse_ident(char* p) 
 * - parse an identifier; accept any UTF-8 characters in it (is that correct?)
 *
 * char*
 * xquery_parse_comment(char* p)
 * - parse an xquery (: .. :)  comment. Note it may be nested.
 *
 * char*
 * xquery_parse_space(char* p)
 * - parse xquery space, which may include comments 
 *
 * char*
 * xquery_parse_string(char* p, char *buf, int len)
 * - parse an XML datamodel string. Deliver an unescaped version as a result 
 *
 * char*
 * xquery_parse_numeric(char* p, char **tpe)
 * - parse an XML datamodel numeric, and determine its minimal type (xs:integer, xs:decimal or xs:double) 
 *
 * all above functions return the new pointer in the xquery after parsing.
 *
 * char*
 * xquery_prepare(xquery_client *ctx, lng usec, char* xquery)
 * - parse xquery; chop off module imports, and recognize single method calls. These are re-executed
 *   from a cached MIL tree. Otherwise use pathfinder to compile. Returns error string (NULL if ok).
 */

/* 
 * parse an identifier; accept any UTF-8 characters in it (is that correct?)
 */
static char* 
xquery_parse_ident(char* p) 
{
    if ((*p >= 'a' && *p <= 'z') || (*p >= 'A' && *p <= 'Z') || *(unsigned char*) p >= 128) {
        p++;
        while((*p >= 'a' && *p <= 'z') || (*p >= 'A' && *p <= 'Z') || (*(unsigned char*) p >= 128) ||
              (*p == '_') || (*p == '.') || (*p == '-') || (*p >= '0' && *p <= '9')) p++;
    }
    return p;
}

/* 
 * parse an xquery (: .. :)  comment. Note it may be nested.
 */
static char* 
xquery_parse_comment(char* p) 
{
    int nesting;
    for(nesting=1; *p; p++) {
        if (p[0] == ':' && p[1] == ')' && --nesting == 0) return p+2;
        if (p[0] == '(' && *(++p) == ':') nesting++;
    }
    return p;
}

/* 
 * parse xquery space, which may include comments 
 */
#define ISSPACE(c) ((c) == ' ' || (c) == '\t' || (c) == 10 || (c) == 13) 
static char* 
xquery_parse_space(char* p) 
{
    while(*p) {
        while(ISSPACE(*p)) p++;
        if (p[0] != '(' || p[1] != ':') break; 
        p = xquery_parse_comment(p+2);
    }
    return p;
}

/* 
 * parse an XML datamodel string. Deliver an unescaped version as a result 
 */
static char* 
xquery_parse_string(char *src, 
                    char *buf, 
                    int len) 
{
    char *p = src, *q = buf, *r = buf+len-1;
    int sep1 = *p++;
    int sep2 = (sep1=='"')?'\'':'"';
    int escape = 0;
    if (sep1 != '\'' && sep1 != '"') return src;
    while(*p) {
        if (escape) {
            if (p[0] == sep2) escape = 0;
            if (q+1 > r) return src; /* no room */
            *q++ = *p; p++;
        } else {
            if (p[0] == sep1) break;
            if (q+1 > r) return src; /* no room */
            if (p[0] == sep2) {
                if (p[1] == sep1 && p[2] == sep1 && p[3] == sep2) {
                    *q++ = sep1; p += 4;
                } else {
                    *q++ = *p; p++;
                    escape = 1;
                }
            } else if (p[0] == '&') {
                if (p[1] == '#' && p[2] == 'x') {
                    unsigned long v = 0; 
                    for(p+=3; *p; p++) {
                        if (*p >= '0' && *p <= '9') {
                            v = (v << 4) + (*p - '0');
                        } else if (*p >= 'a' && *p <= 'f') {
                            v = (v << 4) + (*p - 'a');
                        } else if (*p >= 'A' && *p <= 'F') {
                            v = (v << 4) + (*p - 'A');
                        } else {
                            break;
                        }
                    }
                    *q++ = (char) v;
                } else if (p[1] == '#') {
                    unsigned long v = 0; 
                    for(p+=2; *p; p++) {
                        if (*p >= '0' && *p <= '9') {
                            v = (v * 10) + (*p - '0');
                        } else {
                            break;
                        }
                    }
                    /* UTF8 generation */
                    if (v < 0x80) {
                        *q++ = v; /* ahh! ascii */
                    } else if (v < 0x800) {
                        if (q+2 > r) return src; 
                        *q++ = 0xC0 | (v >> 6);
                        *q++ = 0x80 | (v & 0x3F);
                    } else if (v < 0x10000) {
                        if (q+3 > r) return src; 
                        *q++ = 0xE0 | (v >> 12);
                        *q++ = 0x80 | ((v >> 6) & 0x3F);
                        *q++ = 0x80 | (v & 0x3F);
                    } else if (v < 0x200000) {
                        if (q+4 > r) return src; 
                        *q++ = 0xF0 | (v >> 18);
                        *q++ = 0x80 | ((v >> 12) & 0x3F);
                        *q++ = 0x80 | ((v >> 6) & 0x3F);
                        *q++ = 0x80 | (v & 0x3F);
                    } else if (v < 0x4000000) {
                        if (q+5 > r) return src; 
                        *q++ = 0xF8 | (v >> 24);
                        *q++ = 0x80 | ((v >> 18) & 0x3F);
                        *q++ = 0x80 | ((v >> 12) & 0x3F);
                        *q++ = 0x80 | ((v >> 6) & 0x3F);
                        *q++ = 0x80 | (v & 0x3F);
                    } else /* if (v < 0x80000000) */ {
                        if (q+6 > r) return src; 
                        *q++ = 0xFC | (v >> 30);
                        *q++ = 0x80 | ((v >> 24) & 0x3F);
                        *q++ = 0x80 | ((v >> 18) & 0x3F);
                        *q++ = 0x80 | ((v >> 12) & 0x3F);
                        *q++ = 0x80 | ((v >> 6) & 0x3F);
                        *q++ = 0x80 | (v & 0x3F);
                    }
                } else if (p[1] == 'l' && p[2] == 't' && p[3] == ';') {
                    *q++ = '<'; p += 4;
                } else if (p[1] == 'g' && p[2] == 't' && p[3] == ';') {
                    *q++ = '<'; p += 4;
                } else if (p[1] == 'a' && p[2] == 'm' && p[3] == 'p' && p[4] == ';') {
                    *q++ = '&'; p += 5;
                } else if (p[1] == 'q' && p[2] == 'u' && p[3] == 'o' && p[4] == 't' && p[5] == ';') {
                    *q++ = '"'; p += 6;
                } else if (p[1] == 'a' && p[2] == 'p' && p[3] == 'o' && p[4] == 's' && p[5] == ';') {
                    *q++ = '\''; p += 6;
                }
            } else {
                *q++ = *p++; /* just copy a byte */
            }
        }
    }
    if (*p != sep1) return src;
    *q = 0;
    return p+1;
}

/* 
 * parse an XML datamodel numeric, and determine its minimal type (xs:integer, xs:decimal or xs:double) 
 */
static char* 
xquery_parse_numeric(char* p, 
                     int *tpe) 
{
    *tpe = XS_INTEGER;
    if (p[0] == '-' || p[0] == '+') p++;
    while(p[0] >= '0' && p[0] <= '9') p++;
    if (p[0] == '.' && (p[1] >= '0' && p[1] <= '9')) {
        p++;
        while(p[0] >= '0' && p[0] <= '9') p++;
        *tpe = XS_DECIMAL;
    }
    if (p[0] == 'e' || p[0] == 'E') {
        char *q = p+1;
        if (q[0] == '-' || q[0] == '+') q++;
        if (q[0] >= '0' && q[0] <= '9') {
           while(q[0] >= '0' && q[0] <= '9') q++;
           p = q;
           *tpe = XS_DOUBLE;
        }
    }
    return p;
}

/* 
 * parse xquery; chop off module imports, and recognize single method calls. These are re-executed
 * from a cached MIL tree. Otherwise use pathfinder to compile. Returns error string (NULL if ok).
 */
static char xquery_too_complex[80] = "xquery_prepare: xquery is too complex for cached execution.\n";
#define xquery_nondescriptive_error ((char*) -1)

static char*
xquery_prepare(xquery_client *ctx, 
               lng usec, 
               char* query) 
{
    char *err = NULL;
    int nsbuf = 0, loaded_modules = 0, len;
    char *ns = (char*)&nsbuf, *nsend = ns, *locend, *loc = NULL, *q, *p = query;
    char val[1024], url1[1024], url2[1024];

    /* Setting the Algebra flag; server setting overules compile-time default; client choice overrules server setting */
    int options = 0;
#if MILPRINT_SUMMER_IS_DEFAULT
         if (((ctx->mode & XQ_ALGEBRA) != 0) || \
            (((ctx->mode & XQ_MILPRINT_SUMMER) == 0) && \
              (GDKgetenv("xquery_backend") != NULL) && (strcmp(GDKgetenv("xquery_backend"),"algebra") == 0)))
            options = COMPILE_OPTION_ALGEBRA;
#else /* ALGEBRA_IS_DEFAULT */
        if (((ctx->mode & XQ_MILPRINT_SUMMER) == 0) && \
            (((ctx->mode & XQ_ALGEBRA) != 0) || \
             (GDKgetenv("xquery_backend") == NULL) || (strcmp(GDKgetenv("xquery_backend"),"milprint_summer") != 0)))
            options = COMPILE_OPTION_ALGEBRA;
#endif
if (options != COMPILE_OPTION_ALGEBRA) {
    if (ctx->mode&XQ_DEBUG) {
        /* for debugging purposes, we simulate a full MIL on the log; even if parts are cached */
        char *prologue = (char*) PFinitMIL();
        mnstr_write(ctx->fderr, prologue, strlen(prologue), 1);
        prologue = (char*) PFvarMIL();
        mnstr_write(ctx->fderr, prologue, strlen(prologue), 1);
    }

    /* handle optional xquery version string */
    p = xquery_parse_space(p);
    if (strncmp(p, "xquery", 6) == 0 && ISSPACE(p[6])) {
        err = "xquery_prepare: illegal xquery version or encoding";
        p = xquery_parse_space(p+6);
        if (strncmp(p, "version", 7) == 0 && ISSPACE(p[7])) {
            q = xquery_parse_space(p+7);
            p = xquery_parse_string(q, val, 256);
            if (p > q && strcmp(val,"1.0") == 0) {
                p = xquery_parse_space(p);
                if (strncmp(p, "encoding", 8) == 0 && ISSPACE(p[8])) {
                    q = xquery_parse_string(p+8, val, 256);
                    if (q > p+8 && strcmp(val, "UTF-8") == 0) 
                        p = xquery_parse_space(q);
                }
                if (*p++ == ';') err = NULL; /* ok! */
            }
        }
    }
   
    /* parse one ore more import module statements */  
    while(*p && err == NULL) {
        p = xquery_parse_space(p);
        if (strncmp(p, "import", 6) || !ISSPACE(p[6])) {
            break;
        } else {
            /* e.g. import module namespace xmark = "/cygwin/tmp/" at "/cygwin/tmp/mod1.xq" */
            p = xquery_parse_space(p+6);
            if ( (strncmp(p, "module", 6) == 0 && ISSPACE(p[6])) ){
                len = p[0] == 'm' ? 6 : 10;
                p = xquery_parse_space(p+len);
                if (strncmp(p, "namespace", 9) == 0 && ISSPACE(p[9])) {
                    ns = xquery_parse_space(p+9);
                    nsend = xquery_parse_ident(ns);
                    p = xquery_parse_space(nsend);
                    if (*p == '=') {
                        p = xquery_parse_space(p+1);
                    } else {
                        err = "xquery_prepare: expecting '=' after namespace declaration";
                        break;
                    }
                }
                q = xquery_parse_string(p, url1, 1024);
                if (q == p) {
                    err = "xquery_prepare: expecting URI after namespace declaration";
                    break;
                }
                p = xquery_parse_space(q);
                if (*p == 'a' && *(++p) == 't' && ISSPACE(p[1])) do {
                    char bak = *nsend;
                    loc = xquery_parse_space(p+1);
                    locend = xquery_parse_string(loc, url2, 1024);
                    if (locend == loc) {
                        err = "xquery_prepare: expecting URI after at-hint in module import";
                        break; 
                    }
                    p = xquery_parse_space(locend);

                    /* cut off module import from the query, and load it here (to have it cached later) */
                    *nsend = 0;

                    err = xquery_module_load(ctx, ns, url1, url2);
                    *nsend = bak; 
                    if (err) break;
                    loaded_modules++;
                } while (*p == ',');

                if (*p == ';') {
                    p++;
                } else if (err == NULL) {
                    err = "xquery_prepare: missing ';' after module import.\n";
                }
            } else {
                err = xquery_too_complex; /* we understand no other imports than module */
                break;
            }
        }
    }

    if (loaded_modules == 0) {
        err = xquery_too_complex; /* must at least load one module to be able to just execute a function */
    } else if (err == NULL) {
        /* detect queries that consist of a single method call only */
        nsend = ns = (char*) &nsbuf; 
        loc = p; 
        locend = xquery_parse_ident(loc);
        p = xquery_parse_space(locend);
        if (*p == ':') {
            ns = loc; nsend = locend; 
            loc = ++p; 
            locend = xquery_parse_ident(p);
            p = xquery_parse_space(locend);
        }
        if (*p != '(' || ((locend-loc) == 2 && loc[0] == 'i' && loc[1] == 'f')) {
            err = xquery_too_complex; /* this was not a function call, but an if-statement */
        } else {
            lng cnt[MAXPARAMS];
            int tpe[MAXPARAMS];
            char* param[MAXPARAMS];
            lng argc = 0;
    
            /* e.g. ns:function(1,2,3) */
            p = xquery_parse_space(p+1);
            if (*p == ')') {
               p++;
            } else do {
                int strsep = *p;
                param[argc] = p;
                tpe[argc] = XS_STRING;
                cnt[argc] = 1;
                if (strsep == '"' || strsep == '\'') {
                    p = xquery_parse_string(param[argc], val, 1024);
                    if (p <= param[argc]) {
                        err = "xquery_prepare: could not parse string literal.\n";
                        break;
                    }
                    /* get the properly escaped string and use it as parameter later */
                    len = strlen(val);
                    param[argc] = (char*) alloca(len+1);
                    memcpy(param[argc], val, len);
		    param[argc][len] = 0;
                } else if (*p == '-' || *p == '+' || *p == '.' || (*p >= '0' && *p <= '9')) {
                    p = xquery_parse_numeric(param[argc], tpe+argc);
                    if (p <= param[argc]) {
                        err = "xquery_prepare: could not parse numeric literal.\n";
                        break;
                    }
                } else {
                    /* complex parameter */
                    err = xquery_too_complex; 
                    break;
                }
                if (++argc >= MAXPARAMS) {
                    err = "xquery_prepare: too many parameters.\n";
                    break;
                }
                param[argc] = NULL;
                p = xquery_parse_space(p);
                if (*p == ')') {
                    p++; 
                    break;
                } else if (*p != ',') {
                    err = "xquery_prepare: expecting a ',' between function parameters.\n";
                    break;
                }
                p = xquery_parse_space(p+1);
            } while(*p);
    
            if (err == NULL) {    
                p = xquery_parse_space(p);
                if (*p == '/') {
                    err = xquery_too_complex;  /* what was this case?? */
                } else if (*p) {
                    err = "xquery_prepare: unexpected characters after ';'.\n";
                } else {
                    /* all set: make the call */
                    lng* cnt_ptr = cnt;
                    char nsbak = *nsend, locbak = *locend;
                    *nsend = 0; *locend = 0;
                    err = xquery_function_call(ctx, usec, ns, url1, loc, 0LL, "", "", 0LL, "", argc, 1, &cnt_ptr, tpe, param, NULL);
                    *nsend = nsbak; *locend = locbak;
                }
            }
        }
    }
    /* if (err != NULL) we could return that error here, but it is more consistent 
     * to let the PF compiler give out all error messages ("why make error queries fast?").
     *
     * we can re-enable immediate return if we (ever) decide to create a low-resource XQ
     * implementation that only supports prepared queries and lacks a linked PF compiler.  
     */
    if (err == xquery_function_error) /* MIL execution failed, do not generate error in PF compiler */
       return xquery_nondescriptive_error;
} else {
    err = xquery_nondescriptive_error;
}
    if (err) {
        char *sec1 = NULL;
        char *sec2 = NULL;
        char *sec3 = NULL;
        /* compile and execute the query (minus module imports) */
        err = xquery_nondescriptive_error;
        if (xquery_compile_exec(ctx, options, query, 0, &sec1, &sec2, &sec3, NULL)) 
                err = NULL;
        if (sec1) free(sec1);
        if (sec2) free(sec2);
        if (sec3) free(sec3);
    } 
    return err; 
}

/*
 * =================== exported functions ================================
 *
 * see MIL module definition (top of file)
 */

@= builtin_operand
{
    int _k = interpret(stk, arg(lt, @1), res);
    if (_k < 0) {
        return _k;
    }
    @3 = VALconvert(@2, res);
    if ((@3 == ILLEGALVALUE)  || (@2 == TYPE_bat && !@3)) {
        return handle_paramerror(res,@1,res->vtype,@2);
    }
}
@= setvar
    if ((@2) != NULL && strcmp(*(ctx->@1), @2)) { 
        char* newval = GDKstrdup(@2);
        if (newval == NULL) return "setvar(@1): malloc failure";
        GDKfree(*(ctx->@1));
        (*ctx->@1) = newval;
    }
@c
static char* 
xquery_change_genType(xquery_client *ctx, char* val) {
    @:setvar(genType,val)@
    return NULL;
}

void
xquery_client_engine(mapi_client *mc)
{
    ssize_t n = 0;
    size_t curlen;
    xquery_client *ctx = mc->fc;
    stream *in = mc->c->fdin, *out = mc->c->fdout;
    char *p, *xquery, *err = (char*) -1, *msg = NULL;
    lng usec;

    /* buf is the mode string that is available during execution in the MIL genType var */
    char buf[113], *genType, *output; 
    strcpy(buf, "timing-mapi-dm");
    genType = buf+7; /* initially skips over  over "timing-" */
    output = buf+12; /* place to overwrite with alternatives to "dm" */
    output[100] = 0; /* max 100 characters in output string, ensure zero at end */

    ctx->mode = XQ_MAPI;
    while (1) {
        usec = GDKusec();

        /* use the MAPI protocol to read as much xquery buffer as possible */
        if (mnstr_flush(out)) {
            msg = "could not flush prompt"; 
            break;
        }

        for (p = ctx->buf, curlen = 0; p; ) {
            n = mnstr_read(in, p + curlen, 1, ctx->buflen - curlen);
            if (n <= 0 || p[curlen] == 0)
                break;

            curlen += n;
            if (curlen == ctx->buflen) {
                p = GDKrealloc(ctx->buf, ctx->buflen + 1025);
                if (p) {
                    ctx->buflen += 1024;
                    ctx->buf = p;
                }
            }
        }
        if (curlen == 0 || !p || n < 0)
            break;
        p[curlen] = 0;        /* terminate (we know there is space) */
        if (*p == 'X') {
                /* remove newline */
                if (p[curlen - 1] == '\n') p[curlen - 1] = 0;  
                p++;
                switch(*p) {
                case 'p': /* profile */
#ifdef MT_LOCK_TRACE
                        if (genType != buf) MT_locktrace_start();
#endif
                        genType = buf; /* now includes "timing-" prefix */
                        break;
                case 'a': /* enable algebra? */
                        p += sizeof("algebra ") - 1;
                        /* clear previous state */
                        ctx->mode &= ~(XQ_ALGEBRA|XQ_MILPRINT_SUMMER);
                        /* set new state */
                        if (*p == '1')
                            ctx->mode |= XQ_ALGEBRA;
                        if (*p == '0')
                            ctx->mode |= XQ_MILPRINT_SUMMER;
                        break;
                case 'o': /* output format */
                        p += sizeof("output ") - 1;
                        strncpy(output, p, 100); /* 'output' has room for 100 chars */
                        if (strncmp(output, "debug", 5) == 0)
                            ctx->mode |= XQ_DEBUG;
                        break;
                case 'c': { /* shred from stream */
                        int i, perc=0;
                        char *dname, *cname;
                        p += sizeof("copy ")-1;

                        /* start shredder (first character should be skipped) */
                        if (mnstr_flush(out)) {
                                msg = "could not flush prompt"; 
                                break;
                        }
                        dname = GDKstrdup(p);
                        if (dname == NULL) {
                                msg = "malloc failure"; 
                                break;
                        }
                        cname = strchr(dname, ',');
                        if (cname == NULL)  { 
                                /* default collection name is the document name */
                                cname = dname; 
                        } else {
                                char *percStr = strchr(cname+1, ',');
                                *cname++ = 0;
                                if (percStr) perc=atoi(percStr+1);
                        }

                        /* insert into document collection (p is name)*/
                        snprintf(ctx->buf, ctx->buflen, "shred_stream(Stream(" SZFMT "LL), \"%s\", \"%s\", lng(%d));\n", (size_t) in, dname, cname, perc);
                        ctx->buf[ctx->buflen - 1] = 0;
                        i = xquery_mil_exec(ctx, ctx->buf); /* use MIL to add it to the repository */
                        GDKfree(dname);
                        if (i < 0) msg = "shred_stream failed"; 
                        break;
                    }
                }
                continue; 
        } else if (*p == 'S' || *p == 's') {
    
            /* execute query */
            xquery = p+1;
if (p[1] == '4' && p[2] == '2' && p[3] == '\n' && p[4] == 0 && genType == buf && output[2] == 0) { /* use '42' as latency query test */ 
    mnstr_printf(GDKout, "42\n\nTrans       0.000 msec\nShred       0.000 msec\nQuery       0.000 msec\nPrint       0.000 msec\n"); 
} else {
            err = xquery_change_genType(ctx, genType);
            if (err == NULL) 
                err = xquery_prepare(ctx, usec, xquery);
            if (err && err != xquery_nondescriptive_error) {
                /* report errors back to client */
                mnstr_write(ctx->fderr, err, strlen(err), 1);
            }
            if (ctx->mode&XQ_DEBUG) {
                /* memory debugging */
                xquery_prepared_function *fun = ctx->prepared_functions;
                BAT *b = NULL;
                view_client_size(&b, &mc->stk);
                if (b) {
                   while(fun) {
                      BUNins(b, fun->def->proc, &fun->def->size, FALSE);
                      fun = fun->next;
                   } 
                   BATprintf(GDKstdout, b);
                   BBPreclaim(b);
                } 
            }
}
        }
        /* second and on queries also need a cleared context */
        xquery_client_reset(ctx, NULL); 
    }
    xquery_client_end(mc, msg); 
#ifdef MT_LOCK_TRACE
    if (genType == buf) MT_locktrace_end();
#endif
}

void 
xquery_client_end(mapi_client *mc, char *err) 
{ 
    xquery_client *ctx = (xquery_client*) mc->fc;

    xquery_client_reset(ctx, err); 
    monetSetChannel(THRget(THRgettid()), NULL, NULL);
}

int
xquery_frontend( ptr *F)
{
    mapi_frontend *f = (mapi_frontend*)GDKmalloc(sizeof(mapi_frontend));
    if (f) {
        f->name = GDKstrdup("xquery");
        if (f->name) {
            char *p = GDKgetenv("mapi_clients");
            int nr = p?atoi(p):0;
            *F = f;
            f->cache_limit = (nr>0)?nr:1;
	    f->f_alloc = xquery_client_alloc;
	    f->f_init  = xquery_client_init;
	    f->f_free  = xquery_client_free;
	    f->f_engine  = xquery_client_engine; /* read/parse/execute loop */
	    return GDK_SUCCEED;
        }
        GDKfree(f);
    }
    return GDK_FAIL;
}

/*
 * execute xquery and return the result in a string.
 */
int
CMDxquery(Cntxt stk, 
          YYSTREE lt, 
          ValPtr res)
{
    lng usec = GDKusec();
    char *mode = "xml", *xquery = NULL, *err = NULL;
    bit no=0, *is_url=&no;
    Client c = NULL;
    xquery_client *ctx;

    /* this is a BUILTIN because we must obtain the client context (and
     * thus the streams) */
    if (lt->cnt > 4)
        return handle_argerror(res, lt->cnt, 3);
    if (lt->cnt == 1) {
        @:builtin_operand(0,TYPE_str,xquery)@
    } else {
        @:builtin_operand(0,TYPE_str,mode)@
        @:builtin_operand(1,TYPE_str,xquery)@
    }
    if (lt->cnt == 3) {
        @:builtin_operand(2,TYPE_bit,is_url)@
    }
    CNTXTclient(stk, &c);
    ctx = xquery_client_new(c->stk);
    ctx->fderr = GDKerr;
    MT_lock_set(&pf_cache_lock, "CMDxquery");
    err = xquery_client_alloc_(ctx);
    if (!err)
        err = xquery_client_init_(ctx);
    MT_lock_unset(&pf_cache_lock, "CMDxquery");

    if (err == NULL && *is_url == TRUE) { 
        MT_lock_set(&pf_compiler_lock, "CMDxquery");
        PFURLCACHE("CMDxquery", xquery, 1);
        MT_lock_unset(&pf_compiler_lock, "CMDxquery");
    }
    err = xquery_change_genType(ctx, mode);
    if (err == NULL) {
        buffer *b = buffer_create(XQUERY_BUFSIZE);
        if (b) {
            stream *s = buffer_wastream(b, "CMDxquery");
            if (s) {
                /* run the query, but collect all output in a buffer */
                stream *bak = GDKout;
                THRsetdata(0,s);
                err = xquery_prepare(ctx, usec, xquery);
                if (err == NULL || err == xquery_nondescriptive_error) {
                    res->val.sval = GDKmalloc(b->pos+1);
                    if (res->val.sval) {
                        memcpy(res->val.sval, b->buf, b->pos);
                        res->val.sval[b->pos] = 0;
                        res->vtype = TYPE_str;
                    } else {
                        err = "CMDxquery: failed to allocate buffer";
                    }
                }
                THRsetdata(0,bak);
                mnstr_close(s);
            } else {
                err = "CMDxquery: failed to create stream";
            }
            buffer_destroy(b);
            mnstr_destroy(s);
        } else {
            err = "CMDxquery: failed to allocate streambuffer";
        }
    }
    MT_lock_set(&pf_cache_lock, "CMDxquery");
    xquery_client_free_(ctx);
    MT_lock_unset(&pf_cache_lock, "CMDxquery");
    if (err) {
        GDKerror((err==xquery_nondescriptive_error)?res->val.sval:err);
        VALclear(res);
        return GDK_FAIL;
    } 
    /* mnstr_printf(GDKout,"RES = %s\n",res->val.sval); */
    return GDK_SUCCEED;
}

int
CMDxquery_logger_create( logger *L, int *debug, str fn, str dirname, str dbname, int *version) 
{
    /* TODO: instead of passing NULL, pass the real implementation of
     *       the XRPC PRE_COMMIT handler */
	logger *l = logger_create(*debug, fn, dirname, dbname, *version, NULL, NULL, NULL);
	
	if (l) {
		*(logger**)L = l;
		return GDK_SUCCEED;
	}
	return GDK_FAIL;
}

/* xquery_method : execute a loop-lifted xquery function
 *
 * flags         = 1: timing, 2: serialization mode, 4: debug
 * argc          = #params
 * itercnt       = #iterations
 * argcnt[iter]  = #items per param
 * argtpe[]      = xquery type of each parameter (as a XS_* integer code)
 * argval[]      = str representation of item (e.g. '42')
 *
 * shredBAT   = optional shredded document table, that is added to working set
 *              params of type xs:anyNode are represented as int pre-numbers.
 *
 * we return an error string, or NULL iff everything went A-OK
 */
char*
xquery_method(mapi_client *mc,
              int flags,
              lng *seqnr,
              char* mode,
              char* module,
              char* location,
              char* method,
              char* qid,
              char* caller,
              lng timeout,
              lng argc,
              lng itercnt,
              lng** argcnt,
              int* argtpe,
              str* argval,
              BAT* shredBAT)
{
    lng time_xrpcServApp = GDKusec();
    lng usec = GDKusec();
    xquery_client *ctx = (xquery_client*) mc->fc;
    char *genType = (flags&2)?"xml-noheader-noroot":"xml-noheader-xrpc";
    char *err, *ns = "fn";
    stream *s = NULL;

    ctx->mode |= XQ_XRPC;
    ctx->errbuf = GDKerrbuf;
    if(shredBAT)
        *ctx->xrpc_shredBAT = BBPcacheid(shredBAT);
    err = xquery_change_genType(ctx, genType);
    if (err) return err;

    if (flags&4) { /* write debug output */
        s = open_wastream("/tmp/xrpc.mil");
        char *prologue = (char*) PFinitMIL();
        if (s) {
            ctx->fderr = s;
            ctx->mode |= XQ_DEBUG;
        }
        mnstr_write(ctx->fderr, prologue, strlen(prologue), 1);
    }
    if (err == NULL && module){
        err = xquery_module_load(ctx, ns="xrpc", module, location); 
    }

    if (err == NULL) { 
        err = xquery_function_call(ctx, usec, ns, module, method, seqnr, qid, caller, timeout, mode, argc, itercnt, argcnt, argtpe, argval, shredBAT);
        if (err == (char*) -1) err = "xquery_method: function could not be resolved.\n";
        else if (err == xquery_function_error) err = xquery_nondescriptive_error;
    }

    time_xrpcServApp = GDKusec() - time_xrpcServApp;
    if (flags&1){
        /* print timing ourselves */
        fprintf(stdout, "\n"
               "XRPC_Server_Application:       " LLFMT " microsec\n"
               "XRPC_Network_Server_2_Client:  " LLFMT " microsec\n",
               (*ctx->time_shred + *ctx->time_compile + *ctx->time_exec),
               *ctx->time_print);
    }

    if (s) {
        mnstr_close(ctx->fderr);
        mnstr_destroy(ctx->fderr);
        ctx->fderr = GDKerr;
    }
    return err;
}

/*
 * module initialization
 */
#include <libxml/parser.h>
#include <libxml/xmlmemory.h>

bat *
xquery_prelude(void)
{
    if (GDKgetenv("xquery_backend") == NULL) {
#if MILPRINT_SUMMER_IS_DEFAULT
        GDKsetenv("xquery_backend","milprint_summer");
#else /* ALGEBRA_IS_DEFAULT */
        GDKsetenv("xquery_backend","algebra");
#endif
    }

    if (GDKgetenv("xrpc_docroot") == NULL) {
        char *prefix = GDKgetenv("prefix");
        char xrpc_docroot[1024];
        snprintf(xrpc_docroot, 1024, "%s%cshare%cmonetdb%cxrpc",
                 prefix ? prefix : PREFIX, DIR_SEP, DIR_SEP, DIR_SEP);
        GDKsetenv("xrpc_docroot",xrpc_docroot);
    }

    MT_lock_init(&pf_compiler_lock, "pf_compiler_lock");
    MT_lock_init(&pf_module_lock, "pf_module_lock");
    MT_lock_init(&pf_cache_lock, "pf_cache_lock");

    xmlMemSetup(GDKfree, GDKmalloc, GDKrealloc, GDKstrdup);
    xmlInitParser();
   
    xquery_compiled_modules = NULL;
    return NULL;
}

/*
 * module cleanup
 */
void
xquery_epilogue(void)
{
    xquery_client_flushall();
    MT_lock_destroy(&pf_compiler_lock);
    MT_lock_destroy(&pf_cache_lock);
}
/* vim:set shiftwidth=4 expandtab: */
