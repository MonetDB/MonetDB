@' The contents of this file are subject to the MonetDB Public License
@' Version 1.1 (the "License"); you may not use this file except in
@' compliance with the License. You may obtain a copy of the License at
@' http://monetdb.cwi.nl/Legal/MonetDBLicense-1.1.html
@'
@' Software distributed under the License is distributed on an "AS IS"
@' basis, WITHOUT WARRANTY OF ANY KIND, either express or implied. See the
@' License for the specific language governing rights and limitations
@' under the License.
@'
@' The Original Code is the MonetDB Database System.
@'
@' The Initial Developer of the Original Code is CWI.
@' Portions created by CWI are Copyright (C) 1997-2007 CWI.
@' All Rights Reserved.

@f gdk_bbp
@a M. L. Kersten, P. Boncz, N. J. Nes
@* BAT Buffer Pool (BBP)
The BATs created and loaded are collected in a BAT buffer pool.  
The Bat Buffer Pool has a number of functions:
@table @code

@item administration and lookup
The BBP is a directory which contains status information about all known BATs. 
This interface may be used very heavily, by data-intensive applications.
To eliminate all overhead, read-only access to the BBP may be done by 
table-lookups. The integer index type for these lookups is @%bat@, as 
retrieved by @%BBPcacheid(b)@. The @%bat@ zero is reserved for the nil bat. 

@item persistence
The BBP is made persistent by saving it to the dictionary file 
called @emph{BBP.dir} in the database. The dictionary can always be 
reconstructed from the @emph{ .desc} files. Its main role is to 
speed-up system restart, because now it requires just a few IOs 
instead of a complete directory scan and reading all descriptors
to find all BATs.

When the number of BATs rises, having all files in one directory
becomes a bottleneck.  The BBP therefore implements a scheme that distributes
all BATs in a growing directory tree with at most 64 BATs stored in one node.

@item buffer management
The BBP is responsible for loading and saving of BATs to disk. It also 
contains routines to unload BATs from memory when memory resources 
get scarce. For this purpose, it administers BAT memory reference 
counts (to know which BATs can be unloaded) and BAT usage statistics 
(it unloads the least recently used BATs).

@item recovery
When the database is closed or during a run-time syncpoint, the system
tables must be written to disk in a safe way, that is immune for system 
failures (like disk full). To do so, the BBP implements an atomic commit and 
recovery protocol: first all files to be overwritten are moved to a BACKUP/
dir. If that succeeds, the writes are done. If that also fully succeeds
the BACKUP/ dir is renamed to DELETE_ME/ and subsequently deleted.
If not, all files in BACKUP/ are moved back to their original location.

@item unloading
Bats which have a logical reference (ie. a lrefs > 0) but no memory
reference (refcnt == 0) can be unloaded. Unloading dirty bats means,
moving the original (committed version) to the BACKUP/ dir and saving
the bat. This complicates the commit and recovery/abort issues.
The commit has to check if the bat is already moved. And The recovery
has to always move back the files from the BACKUP/ dir.

@item reference counting
Bats use have two kinds of references: logical and physical (pointer) ones.
Both are administered with the BBPincref/BBPdecref routines. For 
backward compatibility, we maintain BBPfix/BBPunfix as shorthands
for the adjusting the pointer references.

@item share counting
Views use the heaps of there parent bats. To save guard this, the
parent has a shared counter, which is incremented and decremented
using BBPshare and BBPunshare. These functions make sure the
parent is memory resident as required because of the 'pointer' sharing.
@end table
@{
@h
#ifndef _GDK_BBP_H_
#define _GDK_BBP_H_

#define BBPINIT		2048
#define BBPMAXSIZE	1024*1024

#define BBPLOADED	1	/* set if bat in memory */
#define BBPSWAPPED	2	/* set if dirty bat is not in memory */
#define BBPTMP          4	/* set if non-persistent bat has image on disk */
#define BBPDELETED	16	/* set if bat persistent at last commit is now transient */
#define BBPEXISTING	32	/* set if bat was already persistent at end of last commit */
#define BBPNEW 		64	/* set if bat has become persistent since last commit */
#define BBPPERSISTENT	96	/* mask for currently persistent bats */
#define BBPSTATUS	127

#define BBPDELETING	2048	/* set while we are deleting (special case in module unload) */
#define BBPUNSTABLE	2176	/* set while we are unloading */
#define BBPUNLOADING	128	/* set while we are unloading */
#define BBPLOADING	256	/* set while we are loading */
#define BBPSAVING       512	/* set while we are saving */
#define BBPWAITING      2944
#define BBPRENAMED	1024	/* set when bat is renamed in this transaction */

#define BBPTRIM_ALL	(((size_t)1) << (sizeof(size_t)*8 - 2))	/* very large positive size_t */
#define BBPLASTUSED(x)  ((x) & 0x7fffffff)	/* stamp is always a positive int */

gdk_export int BBPin;		/* BATs swapped into BBP  */
gdk_export int BBPout;		/* BATs swapped out of BBP */
gdk_export int BBPsize;		/* current occupied size of BBP array */

/* global calls */
gdk_export void BBPinit(void);
gdk_export void BBPexit(void);
gdk_export int BBPdir(int cnt, bat *subcommit);

/* update interface */
gdk_export void BBPclear(bat bid);
gdk_export bat BBPinsert(BAT *b);
gdk_export void BBPcacheit(BAT *b);
gdk_export void BBPuncacheit(bat bid);
gdk_export int BBPreclaim(BAT *b);
gdk_export int BBPsave(BAT *b);
gdk_export int BBPrename(bat bid, str nme);
gdk_export BAT *BBPrecycle(int ht, int tt, size_t cap);
gdk_export wrd BBPrecycle_minsize(wrd);

/* query interface */
gdk_export bat BBPindex(str nme);
gdk_export BAT *BBPdescriptor(bat b);

/* swapping interface */
gdk_export int BBPrecover(void);
gdk_export lng BBPdiskscan(void);
gdk_export int BBPsync(int cnt, bat *subcommit);
gdk_export int BBPincref(bat b, int logical);
gdk_export void BBPkeepref(bat i);
gdk_export void BBPreleaseref(bat i);
gdk_export void BBPreleaselref(bat i);
gdk_export int BBPdecref(bat b, int logical);
gdk_export void BBPshare(bat b);
gdk_export void BBPunshare(bat b);
gdk_export void BBPextend(dbl factor, int buildhash);

gdk_export void BBPatom_drop(int atom);
gdk_export void BBPatom_load(int atom);

gdk_export int BBPbackup(BAT *b, bit subcommit);

@
@c
#include "monetdb_config.h"
#include "gdk.h"
#include "gdk_storage.h"
@}
@-
The BBP has now a fixed address, so re-allocation due to a growing BBP 
caused by one thread does not disturb reads to the old entries by another.
This is implemented using anonymous virtual memory; extensions on the same 
address are guaranteed because a large non-committed VM area is requested 
initially. New slots in the BBP are found at O(1) by keeping a freelist
that uses the 'next' field in the BBPrec records.
@{
@c
BBPrec *BBP = NULL;		/* fixed base VM address of BBP array */
bat BBPmaxsize = BBPMAXSIZE;	/* size of non-commited VM BBP array */
bat BBPlimit = 0;		/* current committed VM BBP array */
bat BBPsize = 0;		/* current used size of BBP array */

@}
@-
The hash index uses a bucket index (int array) of size mask that is
tuned for perfect hashing (1 lookup). The bucket chain uses the 'next'
field in the BBPrec records.
@{
@h
#define BBPnamecheck(s) (((s)[0]=='t' && (s)[1]=='m' && (s)[2]=='p' &&\
			  (s)[3]=='_')?strtol(s+4,NULL,8):0)
#define BBPtmpcheck(s) (((s)[0]=='t' && (s)[1]=='m' && (s)[2]=='p' &&\
			  (s)[3]=='_')?1:0)

@c
bat *BBP_hash = NULL;		/* BBP logical name hash buckets */
bat BBP_mask = 0;		/* number of buckets = & mask */

static void BBPspin(bat bid, str debug, int event);
static int BBPfree(BAT *b, str calledFrom);
static int BBPdestroy(BAT *b);
static void BBPuncacheit_(bat bid, int unloaddesc);
static void BBPinitcache(void);
static int BBPprepare(bit subcommit);


static int stamp = 0;
static INLINE int
BBPstamp(void)
{
	return ++stamp;
}

static void
BBPsetstamp(int newstamp)
{
	stamp = newstamp;
}


static void
BBP_insert(bat i)
{
	bat idx = (bat) (strHash(BBP_logical(i)) & BBP_mask);

	BBP_next(i) = BBP_hash[idx];
	BBP_hash[idx] = i;
}

static void
BBP_delete(bat i)
{
	bat *h = BBP_hash;
	str s = BBP_logical(i);
	bat idx = (bat) (strHash(s) & BBP_mask);

	for (h += idx; (i = *h) != 0; h = &BBP_next(i)) {
		if (strcmp(BBP_logical(i), s) == 0) {
			*h = BBP_next(i);
			break;
		}
	}
}

@-
other globals
@c
int BBP_curstamp = 0;		/* unique stamp for creation of a bat */
MT_Id BBP_notrim = ~((MT_Id) 0);	/* avoids BBPtrim when we really do not want it */
int BBP_dirty = 0;		/* BBP structures modified? */
int BBPin = 0;			/* bats loaded statistic */
int BBPout = 0;			/* bats saved statistic */

@}

@+ BBP Consistency and Concurrency
While GDK provides the basic building blocks for an ACID system, in
itself it is not such a system, as we this would entail too much overhead
that is often not needed. Hence, some consistency control is left to the
user. The first important user constraint is that if a user updates a 
BAT, (s)he himself must assure that no-one else accesses this BAT. 

Concerning buffer management, the BBP carries out a swapping policy.
BATs are kept in memory till the memory is full. If the memory is full,
the malloc functions initiate BBP trim actions, that unload the coldest BATs 
that have a zero reference count. The second important user constraint 
is therefore that a user may only manipulate live BAT data in memory if it 
is sure that there is at least one reference count to that BAT.

The main BBP array is protected by two locks:
@table @code
@item GDKcacheLock]
this lock guards the free slot management in the BBP array.  The BBP 
operations that allocate a new slot for a new BAT (@%BBPinit@,@%BBPcacheit@), 
delete the slot of a destroyed BAT (@%BBPreclaim@), or rename a BAT 
(@%BBPrename@), hold this lock. It also protects all BAT (re)naming actions 
include (read and write) in the hash table with BAT names.
@item GDKswapLock
this lock guards the swap (loaded/unloaded) status of the BATs. Hence, all
BBP routines that influence the swapping policy, or actually carry out the
swapping policy itself, acquire this lock (e.g. @%BBPfix@,@%BBPunfix@).
Note that this also means that updates to the BBP\_status indicator array
must be protected by GDKswapLock. 

To reduce contention GDKswapLock was split into multiple locks; it is now 
an array of lock pointers which is accessed by GDKswapLock[ABS(bat)\&BBP_BATMASK]
@end table

Routines that need both locks should first acquire the locks in the GDKswapLock 
array (in ascending order) and then GDKcacheLock (and release them in reverse order).

To obtain maximum speed, read operations to existing elements in the BBP are 
unguarded. As said, it is the users responsibility that the BAT that is being 
read is not being modified. BBP update actions that modify the BBP data structure 
itself are locked by the BBP functions themselves. Hence, multiple concurrent BBP read 
operations may be ongoing while at the same time at most one BBP write 
operation @strong{on a different BAT} is executing.  
This holds for accesses to the public (quasi-)
arrays @%BBPcache@, @%BBPstatus@, @%BBPrefs@, @%BBPlogical@ and @%BBPphysical@. These 
arrays are called quasi as now they are actually stored together in one big BBPrec 
array called BBP, that is allocated in anonymous VM space, so we can reallocate 
this structure without changing the base address (a crucial feature if read 
actions are to go on unlocked while other entries in the BBP may be modified). 
@{
@h
#define BBP_status_set(bid, mode, nme) {				\
		BBP_status(bid) = mode;					\
}

#define BBP_status_on(bid, flags, nme) \
		BBP_status_set(bid, BBP_status(bid) | flags, nme);

#define BBP_status_off(bid, flags, nme) \
		BBP_status_set(bid, BBP_status(bid) & ~(flags), nme);

#define BBP_unload_inc(bid, nme) {						\
		gdk_set_lock(GDKunloadLock, nme);				\
		BBPunloadCnt++;							\
		gdk_unset_lock(GDKunloadLock, nme); 				}

#define BBP_unload_dec(bid, nme) {						\
		gdk_set_lock(GDKunloadLock, nme);				\
		if (--BBPunloadCnt == 0) gdk_signal_cond(GDKunloadCond, nme);	\
		assert(BBPunloadCnt >= 0);					\
		gdk_unset_lock(GDKunloadLock, nme);				}
@c
static MT_Id locked_by = 0;

static INLINE MT_Id
BBP_getpid(void)
{
	MT_Id x = MT_getpid();

	return x;
}

static int BBPunloadCnt = 0;

void
BBPlock(str nme)
{
	int i;

	/* wait for all pending unloads to finish */
	gdk_set_lock(GDKunloadLock, nme);
	if (BBPunloadCnt > 0)
		gdk_wait_cond(GDKunloadCond, GDKunloadLock, nme);

	for (i = 0; i <= BBP_THREADMASK; i++)
		gdk_set_lock(GDKtrimLock(i), nme);
	BBP_notrim = BBP_getpid();
	for (i = 0; i <= BBP_THREADMASK; i++)
		gdk_set_lock(GDKcacheLock(i), nme);
	for (i = 0; i <= BBP_BATMASK; i++)
		gdk_set_lock(GDKswapLock(i), nme);
	locked_by = BBP_notrim;

	gdk_unset_lock(GDKunloadLock, nme);
}

void
BBPunlock(str nme)
{
	int i;

	for (i = BBP_BATMASK; i >= 0; i--)
		gdk_unset_lock(GDKswapLock(i), nme);
	for (i = BBP_THREADMASK; i >= 0; i--)
		gdk_unset_lock(GDKcacheLock(i), nme);
	BBP_notrim = 0;
	locked_by = 0;
	for (i = BBP_THREADMASK; i >= 0; i--)
		gdk_unset_lock(GDKtrimLock(i), nme);
}


static void
BBPinithash(void)
{
	bat i = BBPsize;

	for (BBP_mask = 1; (BBP_mask << 1) <= BBPlimit; BBP_mask <<= 1)
		;
	BBP_hash = (bat *) GDKmalloc(BBP_mask * sizeof(bat));
	memset(BBP_hash, 0, BBP_mask * sizeof(bat));
	BBP_mask--;

	while (--i > 0) {
		str s = BBP_logical(i);

		if (s) {
			if (*s != '.' && BBPtmpcheck(BBP_logical(i)) == 0) {
				BBP_insert(i);
			}
			if (BBP_logical(-i)) {
				BBP_insert(-i);
			}
		} else {
			BBP_next(i) = BBP_free(i & BBP_THREADMASK);
			BBP_free(i & BBP_THREADMASK) = i;
		}
	}
}

@-
BBPextend must take the trimlock, as it is called when other BBP locks are
held and it will allocate memory. This could trigger a BBPtrim, causing deadlock.
@c
void
BBPextend(dbl factor, int buildhash)
{
	int newsize = (int) (BBPlimit * factor);
	size_t maxsize = MAX(newsize * 2, BBPmaxsize) * sizeof(BBPrec);

	BBP_notrim = BBP_getpid();
	BBP = (BBPrec *) GDKvmrealloc(BBP, BBPlimit * sizeof(BBPrec), newsize * sizeof(BBPrec), BBPmaxsize * sizeof(BBPrec), &maxsize, 1);
	if (BBP == NULL)
		GDKfatal("BBPextend: failed to extend BAT pool\n");

	memset(BBP + BBPlimit, 0, (newsize - BBPlimit) * sizeof(BBPrec));
	BBPlimit = newsize;
	BBPmaxsize = (int) (maxsize / sizeof(BBPrec));

	if (buildhash) {
		int i;

		GDKfree(BBP_hash);
		BBP_hash = NULL;
		for (i = 0; i <= BBP_THREADMASK; i++)
			BBP_free(i) = 0;
		BBPinithash();
	}
	BBP_notrim = 0;
}

static INLINE char *
BBPparse(str *cur)
{
	char *base, *c = *cur;

	for (c++; GDKisspace(*c); c++)
		;
	for (base = c; !(GDKisspace(*c) || *c == ','); c++)
		;
	*c = 0;
	*cur = c;
	return base;
}


static INLINE str
BBPtmpname(str s, int len, bat i)
{
	s[--len] = 0;
	while (i > 0) {
		s[--len] = '0' + (i & 7);
		i >>= 3;
	}
	s[--len] = '_';
	s[--len] = 'p';
	s[--len] = 'm';
	s[--len] = 't';
	return s + len;
}

static INLINE str
BBPphysicalname(str s, int len, bat i)
{
	s[--len] = 0;
	while (i > 0) {
		s[--len] = '0' + (i & 7);
		i >>= 3;
	}
	return s + len;
}

static int
recover_dir(int direxists)
{
	if (direxists) {
		/* just try; don't care about these nonvital files */
		GDKunlink(BATDIR, "BBP", "bak");
		GDKmove(BATDIR, "BBP", "dir", BATDIR, "BBP", "bak");
	}
	return GDKmove(BAKDIR, "BBP", "dir", BATDIR, "BBP", "dir");
}

#define CONTINUE_IF_NO_SIZES_IN_BBP 1	/* provide backward compatibility */

static int BBPrecover_subdir(void);

void
BBPinit(void)
{
	FILE *fp = GDKfilelocate("BBP", "rb", "dir");
	int i = 0, max_stamp = 0, min_stamp = 0x7fffffff;
	char *s, logical[1024], batname[1024], path[1024];
	char *c, buf[3000];
	struct stat st;
	oid BBPoid = 0;

	/* first move everything from SUBDIR to the BAKDIR (its parent) */
	if (BBPrecover_subdir() < 0) {
		GDKfatal("BBPinit: cannot properly process %s.\n", SUBDIR);
	}

	/* try to obtain a BBP.dir frokm bakdir */
	GDKfilepath(path, BAKDIR, "BBP", "dir");
	if (stat(path, &st) == 0) {
		/* backup exists: *must* use it */
		if (fp)
			fclose(fp);
		fp = recover_dir(fp != NULL) ? NULL : GDKfilelocate("BBP", "rb", "dir");
	} else if (fp == NULL) {
		/* there was no BBP.dir either. Panic! try to use a BBP.bak */
		GDKfilepath(path, BAKDIR, "BBP", "bak");
		if (stat(path, &st)) {
			/* no BBP.bak (nor BBP.dir or BACKUP/BBP.dir): create a new one */
			GDKwarning("BBPdir: initializing BBP.\n");
			if (BBPdir(0, NULL) == 0)
				fp = GDKfilelocate("BBP", "rb", "dir");
		} else if (GDKmove(BATDIR, "BBP", "bak", BATDIR, "BBP", "dir") == 0) {
			GDKwarning("BBPinit: reverting to dir saved in BBP.bak.\n");
			fp = GDKfilelocate("BBP", "rb", "dir");
		}
	}
	if (fp == NULL) {
		/* now it is time for real panic */
		GDKfatal("BBPinit: could not write %s%cBBP.dir\n", BATDIR, DIR_SEP);
	}

	/* scan the BBP.dir to obtain its current size */
	BBPlimit = BBPINIT;
	BBPsize = 1;
	if ((c = fgets(buf, sizeof(buf), fp)) != NULL) {
		int ptrsize, oidsize;

		if (sscanf(c, "%d %d", &ptrsize, &oidsize) != 2) {
#ifdef CONTINUE_IF_NO_SIZES_IN_BBP
			GDKwarning("old BBP without size indications: assuming compatible with current server");
			GDKwarning("if this assumption is not correct, quit() immediately!");
			GDKwarning("otherwise, first commit() to write a new BBP");
			ptrsize = SIZEOF_SIZE_T;
			oidsize = SIZEOF_OID;
#else
			GDKerror("old BBP without size indications");
			GDKerror("add a line to the top of the BBP file with two numbers:");
			GDKerror("the size (in bytes) of pointers and the size (in bytes) of OIDs\nfor the server the BBP was created with");
			exit(1);
#endif
		} else {
			c = fgets(buf, sizeof(buf), fp);
		}
		if (ptrsize != SIZEOF_SIZE_T || oidsize != SIZEOF_OID) {
			GDKerror("database created with incompatible Mserver");
			exit(1);
		}
		if (c != NULL) {
			BBPoid = OIDread(c);
			if ((c = strstr(c, "BBPsize")) != NULL) {
				sscanf(c, "BBPsize=%d", &i);
				i = (int) (i * BATMARGIN);
				if (i > BBPlimit)
					BBPlimit = i;
			}
		}
	}


	/* alloc structures; try to reserve as much space as possible */
	for (;;) {
		size_t size = (size_t) BBPlimit * sizeof(BBPrec);
		size_t maxsize = (size_t) BBPmaxsize * sizeof(BBPrec);

		BBP = (BBPrec *) GDKvmalloc(size, &maxsize, 1);
		MT_alloc_register(BBP, maxsize, 'P');
		if (BBP && maxsize >= BBPmaxsize * sizeof(BBPrec)) {
			BBPmaxsize = (int) (maxsize / sizeof(BBPrec));
			break;
		}
		MT_alloc_register(BBP, maxsize, 'p');
		if (BBP)
			GDKvmfree(BBP, size, maxsize);
		if ((BBPmaxsize /= 2) < BBPlimit) {
			GDKfatal("BBPinit: could not alloc arena\n");
		}
	}
	memset(BBP, 0, BBPlimit * sizeof(BBPrec));
	MT_mmap_pin(BBP, BBPlimit * sizeof(BBPrec));

	/* scan the BBP.dir, and insert the BATs into the BBP */
	while ((c = fgets(buf, sizeof(buf), fp)) != NULL) {
		int j = 0;

		while (*c != '[')
			c++;
		for (c++; GDKisspace(*c); c++)
			;
		i = atoi(c);
		if (GDKisdigit(*c) && i > 0) {
			for (c++; *c != ','; c++)
				;
			for (c++; GDKisspace(*c); c++)
				;
			j = atoi(c);
			if (!GDKisdigit(*c))
				c--;
			else
				for (c++; *c != ','; c++)
					;
		} else {
			GDKerror("BBPinit: ignore line %s\n", buf);
			continue;
		}
		if (i >= BBPsize) {
			BBPsize = i + 1;
			if (BBPsize >= BBPlimit)
				BBPextend(BATMARGIN, FALSE);
		}
		strcpy(batname, BBPparse(&c));
		BBP_status_set(i, BBPEXISTING | (j & ~(BBPLOADED | BBPNEW)), "BBPinit");
		BBP_physical(i) = GDKstrdup(BBPparse(&c));
		BBP_lastused(i) = BBPLASTUSED(atoi(BBPparse(&c)));
		if (BBP_lastused(i) > max_stamp)
			max_stamp = BBP_lastused(i);
		if (BBP_lastused(i) < min_stamp)
			min_stamp = BBP_lastused(i);
		BBP_refs(i) = 0;
		BBP_lrefs(i) = 1;	/* any BAT we encounter here is persistent, so has a logical reference */
		c = strchr(batname, '~');
		if (c && c == batname) {
			s = BBPtmpname(logical, 64, i);
		} else {
			if (c)
				*c = 0;
			strcpy(logical, batname);
			s = logical;
			j++;
		}
		/* the backup of the logical name is set too */
		BBP_logical(i) = GDKstrdup(s);
		BBP[ABS(i)].bak[0] = NULL;
		BBP[ABS(i)].bak[1] = NULL;
		if (BBPtmpcheck(s))	/* only backup temp names */
			BBP[ABS(i)].bak[(i) < 0] = BBP_logical(i);
		i = -i;
		if (c && c[1]) {
			BBP_logical(i) = GDKstrdup(c + 1);
			/* only backup temp names */
			if (BBPtmpcheck(BBP_logical(i)))
				BBP[ABS(i)].bak[(i) < 0] = BBP_logical(i);
			j++;
		} else {
			BBP_logical(i) = NULL;
		}
	}
	fclose(fp);

	/* normalize saved LRU stamps */
	if (min_stamp <= max_stamp) {
		for (i = 1; i < BBPsize; i++)
			if (BBPvalid(i))
				BBP_lastused(i) -= min_stamp;
		BBPsetstamp(max_stamp - min_stamp);
	}

	BBPinitcache();
	/* init hash table */
	BBPinithash();
	BBP_notrim = 0;

	if (BBPoid == 0) {
		OIDseed(OIDrand());	/* if not yet done, init oid */
	}
	OIDbase(BBPoid);

	/* will call BBPrecover if needed */
	if (BBPprepare(0)) {
		GDKfatal("BBPinit: cannot properly process %s.\n", BAKDIR);
	}

	/* cleanup any leftovers (must be done after BBPrecover) */
	BBPdiskscan();
}

@}
@- 
During the exit phase all non-persistent BATs are removed.
Upon exit the status of the BBP tables is saved on disk.
This function is called once and during the shutdown of the
server. Since shutdown may be issued from any thread (dangerous)
it may lead to interference in a parallel session.
@{
@c

void
BBPexit(void)
{
	bat i;

	if (!BBP)
		return;		/* AARGH */

	BBPlock("BBPexit");	/* stop all threads ever touching more descriptors */

	/* free all memory (just for leak-checking in Purify) */
	for (i = 0; i < BBPsize; i++) {
		if (BBPvalid(i)) {
			BAT *b = BBP_cache(i);

			if (b) {
				if (isVIEW(b))
					VIEWdestroy(b);
				else
					BATfree(b);
			}
			BBPuncacheit_(i, TRUE);
			if (BBP_logical(i) != BBP[ABS(i)].bak[(i) < 0])
				GDKfree(BBP[ABS(i)].bak[(i) < 0]);
			BBP[ABS(i)].bak[(i) < 0] = NULL;
			GDKfree(BBP_logical(i));
			BBP_logical(i) = NULL;
		}
		if (BBP_physical(i)) {
			GDKfree(BBP_physical(i));
			BBP_physical(i) = NULL;
		}
	}
	GDKfree(BBP_hash);
	BBP_hash = 0;
}

@}
@-
The routine @%BBPdir@ creates the BAT pool dictionary file. 
It includes some information about the current state of affair in the pool.
The location in the buffer pool is saved for later use as well.
This is merely done for ease of debugging and of no importance to front-ends.
The tail of non-used entries is reclaimed as well. 
@{
@c
static int
new_bbpentry(stream *s, bat i)
{
	int r = stream_printf(s, "[  %d, %d, %s",
			      (int) i, BBP_status(i) & BBPPERSISTENT, BBPname(i));

	if (r < 0)
		return r;
	if (BBPvalid(-i)) {
		r = stream_printf(s, "~%s", BBPname(-i));
		if (r < 0)
			return r;
	}
	return stream_printf(s, ", %s, %d ]\n", BBP_physical(i), BBP_lastused(i));
}

int
BBPdir(int cnt, bat *subcommit)
{
	FILE *fp = NULL;
	stream *s = NULL;
	bat i = 0, j = 1;

	if (GDKdebug & 17)
		THRprintf(GDKout, "#BBPdir: writing BBP.dir (%d bats).\n", (int) BBPsize);
	IODEBUG {
		THRprintf(GDKout, "#BBPdir start oid=");
		OIDwrite(GDKout);
		THRprintf(GDKout, "\n");
	}
	fp = (FILE *) GDKfilelocate("BBP", "wb", "dir");
	if (fp)
		s = file_wastream(fp, "BBP.dir");
	if (s &&
	    stream_printf(s, "%d %d\n", SIZEOF_SIZE_T, SIZEOF_OID) >= 0 &&
	    OIDwrite(s) == 0 &&
	    stream_printf(s, " BBPsize=%d\n", (int) BBPsize) >= 0) {
		for (i = 1; i < BBPsize; i++) {
			int mask = BBPPERSISTENT;	/* BBP.dir consists of all persisnet bats */

			/* but for subcommits, all except the bats in the list retain their existent mode */
			if (subcommit) {
				while (j < cnt && subcommit[j] < i)
					j++;
				if (j < cnt && subcommit[j] != i)
					mask = BBPEXISTING;
			}

			/* write the entry */
			if (BBP_status(i) & mask) {
				if (new_bbpentry(s, i) < 0)
					break;
				IODEBUG new_bbpentry(GDKerr, i);
			}
		}
	}
	if (s) {
		stream_close(s);
		stream_destroy(s);
	} else if (fp) {
		fclose(fp);
	}
	IODEBUG THRprintf(GDKout, "#BBPdir end\n");

	if (i < BBPsize) {
		GDKsyserror("BBPdir failed:\n");
		return -1;
	}
	return 0;
}

@}
@+ BBP Readonly Interface

These interface functions do not change the BBP tables. If they only
access one specific BAT, the called must have ensured that no other thread
is modifying that BAT, therefore such functions do not need locking.
@{
@- 
BBP index lookup by BAT name:
@c
static INLINE bat
BBP_find(str nme, int lock)
{
	bat i = BBPnamecheck(nme);

	if (i > 0) {
		/* for tmp_X BATs, we already know X */
		str s = BBP_logical(i);

		if (i >= BBPsize || s == NULL || strcmp(s, nme)) {
			i = 0;
		}
	} else if (*nme != '.') {
		/* must lock since hash-lookup traverses other BATs */
		if (lock)
			gdk_set_lock(GDKnameLock, "BBPindex");
		for (i = BBP_hash[strHash(nme) & BBP_mask]; i; i = BBP_next(i)) {
			if (strcmp(BBP_logical(i), nme) == 0)
				break;
		}
		if (lock)
			gdk_unset_lock(GDKnameLock, "BBPindex");
	}
	return i;
}

bat
BBPindex(str nme)
{
	return BBP_find(nme, TRUE);
}

BATstore *
BBPgetdesc(bat i)
{
	BAT *b = NULL;

	if (i < 0)
		i = -i;
	if (i != bat_nil && i < BBPsize && i && BBP_logical(i)) {
		str nme = BBP_physical(i);

		b = BBP_cache(i);
		if (b == NULL)
			b = (BAT *) BBP_desc(i);
		if (b == NULL && nme) {
			int lock = locked_by ? BBP_getpid() != locked_by : 1;

			b = BATloaddesc(nme);
			IODEBUG THRprintf(GDKout, "#BATloaddesc(%s) = %d\n", nme, (b == NULL) ? -1 : 0);

			if (b == NULL) {
				GDKerror("BBPgetdesc: deleting illegal bat(%d) %s\n", i, nme);
				BBPclear(i);
			} else if (BBP_desc(i) == NULL) {
				if (lock)
					gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPgetdesc");
				BBP_desc(i) = (BATstore *) b;
				if (lock)
					gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPgetdesc");
			}
		}
		if (b)
			BBP_lastused(i) = BBPLASTUSED(BBPstamp());
	}
	return (BATstore *) b;
}

str
BBPlogical(bat bid, str buf)
{
	if (buf == NULL) {
		return NULL;
	} else if (BBPcheck(bid, "BBPlogical")) {
		if (bid < 0 && BBP_logical(bid) == NULL)
			bid = -bid;
		strcpy(buf, BBP_logical(bid));
	} else {
		*buf = 0;
	}
	return buf;
}

str
BBPphysical(bat bid, str buf)
{
	if (buf == NULL) {
		return NULL;
	} else if (BBPcheck(bid, "BBPphysical")) {
		strcpy(buf, BBP_physical(ABS(bid)));
	} else {
		*buf = 0;
	}
	return buf;
}

@}
@+ BBP Update Interface
Operations to insert, delete, clear, and modify BBP entries.
Our policy for the BBP is to provide unlocked BBP access for 
speed, but still write operations have to be locked.
@{
#ifdef DEBUG_THREADLOCAL_BATS
Create the shadow version (reversed) of a bat.
@- 
An existing BAT is inserted into the BBP 
@c
static INLINE str
BBPsubdir_recursive(str s, bat i)
{
	i >>= 6;
	if (i >= 64) {
		s = BBPsubdir_recursive(s, i);
		*s++ = DIR_SEP;
	}
	i &= 63;
	*s++ = '0' + (i >> 3);
	*s++ = '0' + (i & 7);
	return s;
}

static INLINE void
BBPgetsubdir(str s, bat i)
{
	if (i >= 64) {
		s = BBPsubdir_recursive(s, i);
	}
	*s = 0;
}

bat
BBPinsert(BAT *b)
{
	MT_Id pid = BBP_getpid();
	int lock = locked_by ? pid != locked_by : 1;
	str s;
	long_str dirname;
	bat i;
	int idx = pid & BBP_THREADMASK;

	/* ciritical section: get a new BBP entry */
	if (lock) {
		gdk_set_lock(GDKtrimLock(idx), "BBPreplace");
		gdk_set_lock(GDKcacheLock(idx), "BBPinsert");
	}

	/* find an empty slot */
	if (BBP_free(idx) <= 0) {
		for (i = 0; i <= BBP_THREADMASK; i++)
			if (lock == 0 || i != idx)
				gdk_set_lock(GDKcacheLock(i), "BBPinsert");
		gdk_set_lock(GDKnameLock, "BBPinsert");
		if (BBPsize++ >= BBPlimit) {
			BBPextend(BATMARGIN, TRUE);
			/* it seems BBPextend could return and still leaving
			   BBP_free(idx) == 0 */
			if (BBP_free(idx) == 0)
				BBP_free(idx) = BBPsize - 1;
		} else {
			BBP_free(idx) = BBPsize - 1;
		}
		gdk_unset_lock(GDKnameLock, "BBPinsert");
		for (i = BBP_THREADMASK; i >= 0; i--)
			if (lock == 0 || i != idx)
				gdk_unset_lock(GDKcacheLock(i), "BBPinsert");
	}
	i = BBP_free(idx);
	BBP_free(idx) = BBP_next(BBP_free(idx));

	if (lock) {
		gdk_unset_lock(GDKcacheLock(idx), "BBPinsert");
		gdk_unset_lock(GDKtrimLock(idx), "BBPreplace");
	}
	/* rest of the work outside the lock , as GDKstrdup/GDKmalloc may trigger a BBPtrim */

	/* fill in basic BBP fields for the new bat */

	if (++BBP_curstamp < 0)
		BBP_curstamp = 0;
	b->batCacheid = i;
	b->batStamp = BBP_curstamp;
	b->creator_tid = BBP_getpid();

	BBP_status_set(i, BBPDELETING, "BBPentry");
	BBP_cache(i) = NULL;
	BBP_desc(i) = NULL;
	BBP_refs(i) = 1;	/* new bats have 1 pin */
	BBP_lrefs(i) = 0;	/* ie. no logical refs */

	if (BBP[ABS(i)].bak[(i) < 0] == NULL) {
		s = BBPtmpname(dirname, 64, i);
		BBP_logical(i) = GDKstrdup(s);
		BBP[ABS(i)].bak[(i) < 0] = BBP_logical(i);
	} else
		BBP_logical(i) = BBP[ABS(i)].bak[(i) < 0];
	BBP_logical(-i) = NULL;

	/* Keep the physical location around forever */
	if (BBP_physical(i) == NULL) {
		char name[64], *nme;

		BBPgetsubdir(dirname, i);
		nme = BBPphysicalname(name, 64, i);
		
		BBP_physical(i) = (str) GDKmalloc(strlen(dirname) + strlen(nme) + 1 + 1 /* EOS + DIR_SEP */);
		GDKfilepath(BBP_physical(i), dirname, nme, NULL);

		BATDEBUG THRprintf(GDKout, "#%d = new %s(%s,%s)\n", (int) i, BATgetId(b), ATOMname(b->htype), ATOMname(b->ttype));
	}

	return i;
}

void
BBPcacheit(BAT *b)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;
	bat i = b->batCacheid;
	int mode;
	BAT *bm = (BAT *) GDKmalloc(sizeof(BAT));	/* alloc before lock to prevent trim problems */

	if (i) {
		assert(i > 0);
	} else {
		i = BBPinsert(b);	/* bat was not previously entered */
	}
	if (lock)
		gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPcacheit");
	mode = (BBP_status(i) | BBPLOADED) & ~(BBPLOADING | BBPDELETING);
	BBP_status_set(i, mode, "BBPcacheit");
	BBP_lastused(i) = BBPLASTUSED(BBPstamp() + ((mode == BBPLOADED) ? 150 : 0));
	BBP_desc(i) = (BATstore *) b;

	/* fill in the mirror record */
	bm->GDKversion = b->GDKversion;
	bm->batCacheid = -i;
	bm->H = b->T;
	bm->T = b->H;
	bm->P = b->P;
	bm->U = b->U;

	/* cache it! */
	BBP_cache(i) = b;
	BBP_cache(-i) = bm;

	if (lock)
		gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPcacheit");
}

@
@%BBPuncacheit@ changes the BBP status to swapped out.  Currently only 
used in BBPfree (bat swapped out) and BBPclear (bat destroyed forever).
@c

void
BBPuncacheit(bat i)
{
	BBPuncacheit_(i, FALSE);
}

static void
BBPuncacheit_(bat i, int unloaddesc)
{
	if (i < 0)
		i = -i;
	if (BBPcheck(i, "BBPuncacheit")) {
		BAT *b = (BAT *) BBP_desc(i);

		if (b) {
			if (BBP_cache(i)) {
				BATDEBUG THRprintf(GDKout, "#uncache %d (%s)\n", (int) i, BBPname(i));

				BBP_cache(i) = BBP_cache(-i) = NULL;

				/* clearing bits can be done without the lock */
				BBP_status_off(i, BBPLOADED, "BBPuncacheit");
			}
			if (unloaddesc) {
				BBP_desc(i) = NULL;
				BATdestroy(b);
			}
		}
	}
}

@- BBPclear
@%BBPclear@ removes a BAT from the BBP directory forever.
@c
static INLINE void
bbpclear(bat i, int idx, str lock)
{
	BATDEBUG {
		THRprintf(GDKout, "#clear %d (%s)\n", (int) i, BBPname(i));
	}
	BBPuncacheit_(i, TRUE);
	BBP_status_set(i, BBPUNLOADING, "BBPclear");
	BBP_refs(i) = 0;
	BBP_lrefs(i) = 0;
	if (lock)
		gdk_set_lock(GDKcacheLock(idx), lock);

	if (BBPtmpcheck(BBP_logical(i)) == 0) {
		gdk_set_lock(GDKnameLock, "bbpclear");
		BBP_delete(i);
		gdk_unset_lock(GDKnameLock, "bbpclear");
	}
	if (BBP_logical(-i)) {
		gdk_set_lock(GDKnameLock, "bbpclear");
		BBP_delete(-i);
		gdk_unset_lock(GDKnameLock, "bbpclear");
		if (BBP_logical(-i) != BBP[ABS(i)].bak[(-i) < 0])
			GDKfree(BBP_logical(-i));
		BBP_logical(-i) = NULL;
	}
	if (BBP_logical(i) != BBP[ABS(i)].bak[(i) < 0])
		GDKfree(BBP_logical(i));
	BBP_status_set(i, 0, "BBPclear");
	BBP_logical(i) = NULL;
	BBP_next(i) = BBP_free(idx);
	BBP_free(idx) = i;
	if (lock)
		gdk_unset_lock(GDKcacheLock(idx), lock);
}

void
BBPclear(bat i)
{
	MT_Id pid = BBP_getpid();
	int lock = locked_by ? pid != locked_by : 1;

	if (BBPcheck(i, "BBPclear")) {
		bbpclear(ABS(i), pid & BBP_THREADMASK, lock ? "BBPclear" : NULL);
	}
}

@}
@- BBP rename

Each BAT has a logical name that is globally unique. Its reverse view can
also be assigned a name, that also has to be globally unique.  The batId is 
the same as the logical BAT name.

The default logical name of a BAT is tmp_X, where X is the batCacheid.
Apart from being globally unique, new logical bat names cannot be of the 
form tmp_X, unless X is the batCacheid.

Physical names consist of a directory name followed by a logical name suffix. 
The directory name is derived from the batCacheid, and is currently organized 
in a hierarchy that puts max 64 bats in each directory (see BBPgetsubdir). 

Concerning the physical suffix: it is almost always bat_X. This saves us
a whole lot of trouble, as bat_X is always unique and no conflicts can occur.
Other suffixes are only supported in order just for backward compatibility with 
old repositories (you won't see them anymore in new repositories).
@{
@c
int
BBPrename(bat bid, str nme)
{
	BAT *b = BBPdescriptor(bid);
	long_str dirname;
	bat tmpid = 0, i;
	int idx;

	if (b == NULL)
		return 0;

	/* If name stays same, do nothing */
	if (BBP_logical(bid) && strcmp(BBP_logical(bid), nme) == 0)
		return 0;

	BBPgetsubdir(dirname, ABS(bid));

	if ((tmpid = BBPnamecheck(nme)) && (bid < 0 || tmpid != bid)) {
		return BBPRENAME_ILLEGAL;
	}
	if (strlen(dirname) + strLen(nme) + 1 >= IDLENGTH) {
		return BBPRENAME_LONG;
	}
	idx = BBP_getpid() & BBP_THREADMASK;
	gdk_set_lock(GDKtrimLock(idx), "BBPrename");
	gdk_set_lock(GDKnameLock, "BBPrename");
	i = BBP_find(nme, FALSE);
	if (i != 0) {
		gdk_unset_lock(GDKnameLock, "BBPrename");
		gdk_unset_lock(GDKtrimLock(idx), "BBPrename");
		return BBPRENAME_ALREADY;
	}
	BBP_notrim = BBP_getpid();

	/* carry through the name change */
	if (BBP_logical(bid) && BBPtmpcheck(BBP_logical(bid)) == 0) {
		BBP_delete(bid);
	}
	if (BBP_logical(bid) != BBP[ABS(bid)].bak[(bid) < 0])
		GDKfree(BBP_logical(bid));
	BBP_logical(bid) = GDKstrdup(nme);
	if (tmpid == 0) {
		BBP_insert(bid);
	}
	b->batDirtydesc = 1;
	if (b->batPersistence == PERSISTENT) {
		int lock = locked_by ? BBP_getpid() != locked_by : 1;

		if (lock)
			gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPrename");
		BBP_status_on(ABS(bid), BBPRENAMED, "BBPrename");
		if (lock)
			gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPrename");
		BBPdirty(1);
	}
	gdk_unset_lock(GDKnameLock, "BBPrename");
	BBP_notrim = 0;
	gdk_unset_lock(GDKtrimLock(idx), "BBPrename");
	return 0;
}

@}

@+ BBP swapping Policy
The BAT can be moved back to disk using the routine @%BBPfree@.
It frees the storage for other BATs. After this call BAT* references
maintained for the BAT are wrong.
We should keep track of dirty unloaded BATs. They may have to be committed
later on, which may include reading them in again.

BBPswappable: may this bat be unloaded?
Only real bats without memory references can be unloaded.
@{
@h
#define BBPswappable(b) ((b) && BBP_refs((b)->batCacheid) == 0)
#define BBPtrimmable(b) (BBPswappable(b) && !isVIEW(b) && (BBP_status((b)->batCacheid)&BBPWAITING) == 0)

#endif /* _GDK_BBP_H_ */
@- 
The @%BBP_ref@ contains the amount of live references to a BAT.
These might be in recursive BATs, C or MIL variables.  The count is 
incremented with @%BBPfix@ and decremented with @%BBPunfix@.
@c
static INLINE void
BBPspin(bat i, str s, int event)
{
	if (BBPcheck(i, "BBPspin") && (BBP_status(i) & event)) {
		lng spin = LL_CONSTANT(0);

		while (BBP_status(i) & event) {
			MT_sleep_ms(1);
			spin++;
		}
		BATDEBUG THRprintf(GDKout, "#BBPspin(%d,%s,%d): " LLFMT " loops\n", (int) i, s, event, spin);
	}
}

static INLINE int
incref(bat i, int logical, int lock)
{
	int refs = 0;

	if (i == bat_nil) {
		/* Stefan: May this happen? Or should we better call GDKerror(), here? */
		/* GDKerror("BBPincref() called with bat_nil!\n"); */
		return refs;
	}
	if (i < 0)
		i = -i;
	if (BBPcheck(i, "BBPincref")) {
		if (lock) {
			gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref");

			while (BBP_status(i) & BBPUNSTABLE) {
				gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref spin wait");
				MT_sleep_ms(1);
				gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref spin wait");
			}
		}

		/* got the lock */
		if (logical) {
			refs = ++BBP_lrefs(i);
		} else {
			refs = ++BBP_refs(i);
		}
		if (lock)
			gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPfix");
	}
	return refs;
}

int
BBPincref(bat i, int logical)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;

	return incref(i, logical, lock);
}

void
BBPshare(bat parent)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;
	int fix = 0;

	if (parent < 0)
		parent = -parent;
	if (lock)
		gdk_set_lock(GDKswapLock(parent & BBP_BATMASK), "BBPshare");
	if (++BBP_cache(parent)->batSharecnt == 1)
		fix = 1;
	if (lock)
		gdk_unset_lock(GDKswapLock(parent & BBP_BATMASK), "BBPshare");
	if (fix)
		BBPincref(parent, FALSE);
}

static INLINE int
decref(bat i, int logical, int lock)
{
	int refs = 0, swap = 0, swap_unloading = 0;
	BAT *b;

	/* decrement references by one */
	if (logical) {
		if (BBP_lrefs(i) == 0) {
			GDKerror("BBPdecref: %s does not have logical references.\n", BBPname(i));
		} else {
			refs = --BBP_lrefs(i);
		}
	} else {
		if (BBP_refs(i) == 0) {
			GDKerror("BBPdecref: %s does not have pointer fixes.\n", BBPname(i));
		} else {
			refs = --BBP_refs(i);
		}
	}

	/* we destroy transients asap and unload persistent bats only if they have been made cold */
	b = BBP_cache(i);
	if (BBP_refs(i) > 0 || (BBP_lrefs(i) > 0 && BBP_lastused(i) != 0)) {
		/* bat cannot be swapped out. renew its last usage stamp for the BBP LRU policy */
		int sec = BBPLASTUSED(BBPstamp());

		if (sec > BBPLASTUSED(BBP_lastused(i)))
			BBP_lastused(i) = sec;
	} else if (b || (BBP_status(i) & BBPTMP)) {
		/* bat will be unloaded now. set the UNLOADING bit while locked so no other thread thinks its available anymore */
		if (BBP_status(i) & BBPUNLOADING) {
			/* BBPtrim was unloading this bat that just now lost its last refcnt. */
			incref(i, logical, FALSE);	/* cannot destroy it now; back off */
			swap_unloading = TRUE;
		} else {
			BBP_status_on(i, BBPUNLOADING, "BBPdecref");
			swap = TRUE;
		}
	}

	/* unlock before re-locking in unload; as saving a dirty persistent bat may take a long time */
	if (lock)
		gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPdecref");

	if (swap_unloading) {
		BBPspin(i, "BBPdecref", BBPUNLOADING);	/* wait for unload to complete */
		decref(i, logical, lock);	/* and just try again */
	} else if (swap) {
		int destroy = BBP_lrefs(i) == 0 && (BBP_status(i) & BBPDELETED) == 0;

		b = BBPquickdesc(i, TRUE);
		if (destroy) {
			BBPdestroy(b);	/* free memory (if loaded) and delete from disk (if transient but saved) */
		} else if (b) {
			BBP_unload_inc(i, "BBPdecref");
			BBPfree(b, "BBPdecref");	/* free memory of transient */
		}
	}
	return refs;
}

int
BBPdecref(bat i, int logical)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;

	if (BBPcheck(i, "BBPdecref") == 0) {
		return -1;
	}
	if (i < 0)
		i = -i;
	if (lock)
		gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPdecref");
	return decref(i, logical, lock);
}

@-
M5 often changes the physical ref into a logical reference.
This state change consist of the sequence BBPincref(b,TRUE);BBPunfix(b).
A faster solution is given below, because it does not trigger
the BBP management actions, such as garbage collecting the bats.
[first step, initiate code change]
@c
void
BBPkeepref(bat i)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;

	if (i == bat_nil)
		return;
	if (i < 0)
		i = -i;
	if (BBPcheck(i, "BBPkeepref")) {
		if (lock)
			gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPkeepref");

		while (BBP_status(i) & BBPUNLOADING) {
			if (lock)
				gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref spin wait");
			MT_sleep_ms(1);
			if (lock)
				gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref spin wait");
		}
		/* got the lock */
		++BBP_lrefs(i);
		--BBP_refs(i);
		if (lock)
			gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPfix");
		/* decref(i, FALSE, lock); */
	}
}
void
BBPreleaselref(bat i)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;

	if (i == bat_nil || BBP_lrefs(i) <= 0)
		return;
	if (i < 0)
		i = -i;
	if (BBPcheck(i, "BBPreleaselref")) {
		if (lock)
			gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPreleaselref");

		while (BBP_status(i) & BBPUNLOADING) {
			if (lock)
				gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref spin wait");
			MT_sleep_ms(1);
			if (lock)
				gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref spin wait");
		}
		/* got the lock */
		--BBP_lrefs(i);
		if (lock)
			gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPfix");
	}
}
void
BBPreleaseref(bat i)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;

	if (i == bat_nil || BBP_refs(i) <= 0)
		return;
	if (i < 0)
		i = -i;
	if (BBPcheck(i, "BBPreleaseref")) {
		if (lock)
			gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPreleaseref");

		while (BBP_status(i) & BBPUNLOADING) {
			if (lock)
				gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref spin wait");
			MT_sleep_ms(1);
			if (lock)
				gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPincref spin wait");
		}
		/* got the lock */
		--BBP_refs(i);
		if (lock)
			gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPfix");
	}
}

static INLINE void
unshare(bat parent)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;

	if (parent < 0)
		parent = -parent;
	if (lock)
		gdk_set_lock(GDKswapLock(parent & BBP_BATMASK), "BBPunshare");
	if (--BBP_cache(parent)->batSharecnt == 0) {
		(void) decref(parent, FALSE, lock);
	} else if (lock) {
		gdk_unset_lock(GDKswapLock(parent & BBP_BATMASK), "BBPunshare");
	}
}

void
BBPunshare(bat parent)
{
	unshare(parent);
}

@- 
BBPreclaim is a user-exported function; the common way to destroy a BAT the hard way. 

Return values:
-1 = bat cannot be unloaded (it has more than your own memory fix)
 0 = unloaded successfully
 1 = unload failed (due to write-to-disk failure)
@c
int
BBPreclaim(BAT *b)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;
	bat i = ABS(b->batCacheid);
	int reclaim_while_unloading = 0;
	int ret = 0;

	if (lock)
		gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPreclaim");

	BATDEBUG THRprintf(GDKout, "#BBPreclaim: bat(%d) view=%d lrefs=%d ref=%d stat=%d\n", (int) b->batCacheid, b->batSharecnt, BBP_lrefs(b->batCacheid), BBP_refs(b->batCacheid), BBP_status(b->batCacheid));

	if (BBP_refs(b->batCacheid) > 1) {
		GDKerror("BBPreclaim: %d refs > 1 (%d)\n", i, BBP_refs(i));
		ret = -1;
	} else if (BBP_status(i) & BBPUNLOADING) {
		/* BBPtrim was unloading this bat that just now is getting killed. */
		reclaim_while_unloading = TRUE;
	} else {
		/* unload whatever the LRU in the BBP */
		BBP_refs(b->batCacheid) = 0;
		BBP_status_on(i, BBPUNLOADING, "BBPreclaim");
	}
	if (lock)
		gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPreclaim");

	if (reclaim_while_unloading) {
		BBPspin(i, "BBPreclaim", BBPUNLOADING);	/* wait for unload to complete */
		return BBPreclaim(b);
	}

	/* BBPfree potentially saves the BAT. Do this after releasing the short-term lock */
	if (ret == 0) {
		int destroy = BBP_lrefs(i) == 0 && (BBP_status(i) & BBPDELETED) == 0;

		if (destroy) {
			ret = BBPdestroy(BBP_cache(i));
		} else {
			BBP_unload_inc(i, "BBPreclaim");
			ret = BBPfree(b, "BBPreclaim");
		}
	}
	return ret;
}

@-
BBPdescriptor checks whether BAT needs loading and does so if necessary. You must
have at least one fix on the BAT before calling this.
@c
BAT *
BBPdescriptor(bat i)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;
	int load = FALSE;
	bat j = ABS(i);
	BAT *b = NULL;

	if (!BBPcheck(i, "BBPdescriptor")) {
		return NULL;
	}
	assert(BBP_refs(i));
	if ((b = BBP_cache(i)) == NULL) {

		if (lock)
			gdk_set_lock(GDKswapLock(j & BBP_BATMASK), "BBPdescriptor");
		while (BBP_status(j) & BBPWAITING) {	/* wait for bat to be loaded by other thread */
			if (lock)
				gdk_unset_lock(GDKswapLock(j & BBP_BATMASK), "BBPdescriptor");
			MT_sleep_ms(1);
			if (lock)
				gdk_set_lock(GDKswapLock(j & BBP_BATMASK), "BBPdescriptor");
		}
		if (BBPvalid(j)) {
			b = BBP_cache(i);
			if (b == NULL) {
				load = TRUE;
				BBP_status_on(j, BBPLOADING, "BBPdescriptor");
			}
		}
		if (lock)
			gdk_unset_lock(GDKswapLock(j & BBP_BATMASK), "BBPdescriptor");
	}
	if (load) {
		IODEBUG THRprintf(GDKout, "#load %s\n", BBPname(i));

		b = BATload_intern(i);
		BBPin++;

		/* clearing bits can be done without the lock */
		BBP_status_off(j, BBPLOADING, "BBPdescriptor");
	}
	return b;
}



@-
In BBPsave executes unlocked; it just marks the BBP_status of the BAT to BBPsaving, so others
that want to save or unload this BAT must spin lock on the BBP_status field.
@c
int
BBPsave(BAT *b)
{
	int lock = locked_by ? BBP_getpid() != locked_by : 1;
	bat bid = ABS(b->batCacheid);
	int ret = 0;

	if (BBP_lrefs(bid) == 0 || !BATdirty(b))
		/* do nothing */
		return 0;

	if (lock)
		gdk_set_lock(GDKswapLock(bid & BBP_BATMASK), "BBPsave");

	if (BBP_status(bid) & BBPSAVING) {
		/* wait until save in other thread completes */
		BBPspin(bid, "BBPsave", BBPSAVING);
		if (lock)
			gdk_unset_lock(GDKswapLock(bid & BBP_BATMASK), "BBPsave");
	} else {
		/* save it */
		int flags = BBPSAVING;

		if (DELTAdirty(b)) {
			flags |= BBPSWAPPED;
			BBPdirty(1);
		}
		if (b->batPersistence != PERSISTENT) {
			flags |= BBPTMP;
		}
		BBP_status_on(bid, flags, "BBPsave");
		if (lock)
			gdk_unset_lock(GDKswapLock(bid & BBP_BATMASK), "BBPsave");

		IODEBUG THRprintf(GDKout, "#save %s\n", BATgetId(b));

		/* do the time-consuming work unlocked */
		if (BBP_status(bid) & BBPEXISTING)
			ret = BBPbackup(b, FALSE);
		if (ret == 0) {
			BBPout++;
			ret = (BATsave(b) == NULL);
		}
		/* clearing bits can be done without the lock */
		BBP_status_off(bid, BBPSAVING, "BBPsave");
	}
	return ret;
}


@-
TODO merge BBPfree with BATfree? Its function is to prepare a BAT for being
unloaded (or even destroyed, if the BAT is not persistent).
@c
static int BBPaddtobin(BAT *b);
static int
BBPdestroy(BAT *b)
{
	int clear = 1;
	bat hp = b->H->parentid, tp = b->T->parentid;

	if (isVIEW(b)) {	/* a physical view */
		VIEWdestroy(b);
	} else {
		/* bats that get destroyed must unfix their atoms */
		int (*hunfix) (ptr) = BATatoms[b->htype].atomUnfix;
		int (*tunfix) (ptr) = BATatoms[b->ttype].atomUnfix;
		BUN p, q;
		BATiter bi = bat_iterator(b);

		assert(b->batSharecnt == 0);
		if (hunfix) {
			DELloop(b, p, q) {
				(*hunfix) (BUNhead(bi, p));
			}
			BATloop(b, p, q) {
				(*hunfix) (BUNhead(bi, p));
			}
		}
		if (tunfix) {
			DELloop(b, p, q) {
				(*tunfix) (BUNtail(bi, p));
			}
			BATloop(b, p, q) {
				(*tunfix) (BUNtail(bi, p));
			}
		}
		clear = BBPaddtobin(b);	/* plan for re-use */
	}
	if (clear)
		BBPclear(b->batCacheid);	/* if destroyed; de-register from BBP */

	/* parent released when completely done with child */
	if (hp)
		unshare(hp);
	if (tp)
		unshare(tp);
	return 0;
}

static int
BBPfree(BAT *b, str calledFrom)
{
	bat bid = ABS(b->batCacheid), hp = b->H->parentid, tp = b->T->parentid;
	int ret;

	assert(BBPswappable(b));

	/* write dirty BATs before being unloaded */
	ret = BBPsave(b);
	if (ret == 0) {
		if (isVIEW(b)) {	/* physical view */
			VIEWdestroy(b);
		} else {
			assert(b->batSharecnt == 0);
			if (BBP_cache(bid))
				BATfree(b);	/* free memory */
		}
		BBPuncacheit_(bid, FALSE);
		if (b->batMapdirty) {
			DESCsetmodes(b, b);
			b->batMapdirty = 0;
		}
	}
	/* clearing bits can be done without the lock */
	BBP_status_off(bid, BBPUNLOADING, calledFrom);
	BBP_unload_dec(bid, calledFrom);

	/* parent released when completely done with child */
	if (ret == 0 && hp)
		unshare(hp);
	if (ret == 0 && tp)
		unshare(tp);
	return ret;
}

@}
@- Storage trimming
BBPtrim unloads the least recently used BATs to free memory resources.
It gets passed targets in bytes of physical memory and logical
virtual memory resources to free. Overhead costs are reduced by
making just one scan, analyzing the first BBPMAXTRIM bats
and keeping the result in a list for later use (the oldest bat 
now is going to be the oldest bat in the future as well).
This list is sorted on last-used timestamp. BBPtrim keeps unloading
BATs till the targets are met or there are no more BATs to unload.

In determining whether a BAT will be unloaded, first it has
to be BBPswappable, and second its resources occupied must
be of the requested type. The algorithm actually makes two passes,
in the first only clean bats are unloaded (in order of their stamp).

In order to keep this under control with multiple threads all
running out of memory at the same time, we make sure that 
@itemize
@item 
just one thread does a BBPtrim at a time (by having a BBPtrimLock set).
@item
while decisions are made as to which bats to unload (1) the BBP is
scanned, and (2) unload decisions are made. Due to these properties,
the search\&decide phase of BBPtrim acquires both GDKcacheLock (due to (1))i
and all GDKswapLocks (due to (2)). They must be released during the actual 
unloading.  (as otherwise deadlock occurs => unloading a bat may e.g. kill 
an accelerator that is a BAT, which in turn requires BBP lock acquisition).
@item
to avoid further deadlock, the update functions in BBP that hold either 
GDKcacheLock or a GDKswapLock may never cause a BBPtrim (notice that BBPtrim 
could theoretically be set off just by allocating a little piece of memory, e.g. 
GDKstrdup()). If these routines must alloc memory, they must set the BBP\_notrim 
variable, acquiring the addition GDKtrimLock, in order to prevent such deadlock.
@item
the BBPtrim is atomic; only releases its locks when all BAT unload 
work is done. This ensures that if all memory requests that triggered
BBPtrim could possible be satisfied by unloading BATs, this will succeed.
@end itemize

The scan phase was optimized further in order to stop early when
it is a priori known that the targets are met (which is the case if the
BBPtrim is not due to memory shortage but due to the ndesc quota).
Note that scans may always stop before BBPsize as the BBPMAXTRIM is a fixed
number which may be smaller. As such, a mechanism was added to resume
a broken off scan at the point where scanning was broken off rather than
always starting at BBP[1] (this does more justice to the lower numbered 
bats and will more quickly find fresh unload candidates).

We also refined the swap criterion. If the BBPtrim was initiated due to:
- too much descriptors: small bats are unloaded first (from LRU cold to hot)  
- too little memory: big bats are unloaded first (from LRU cold to hot).
Unloading-first is enforced by subtracting $2^31$ from the stamp in the
field where the candidates are sorted on.
@{
@c
#define BBPMAXTRIM 40000
#define BBPSMALLBAT 1000

typedef struct {
	int lastused;		/* bat lastused stamp; sort on this field */
	bat bid;		/* bat id */
	ssize_t cnt;		/* bat count */
	int next;		/* next position in list */
#if SIZEOF_SIZE_T == 8
	size_t dummy;		/* round up */
#endif
} bbptrim_t;

bbptrim_t bbptrim[BBPMAXTRIM];
int bbptrimfirst = BBPMAXTRIM, bbptrimlast = 0, bbpunloadtail, bbpunload, bbptrimmax = BBPMAXTRIM, bbpscanstart = 1;

static bat
BBPtrim_scan(int mem, int vm, bat bbppos, bat bbplim)
{
	bbptrimlast = 0;
	bbptrimmax = BBPMAXTRIM;
	MEMDEBUG THRprintf(GDKout, "#TRIMSCAN: mem=%d vm=%d, start=%d, limit=%d\n", mem, vm, (int) bbppos, (int) bbplim);

	if (bbppos < BBPsize)
		do {
			if (BBPvalid(bbppos)) {
				BAT *b = BBP_cache(bbppos);

				if (BBPtrimmable(b)) {
					/* when unloading for memory, treat small BATs with a preference over big ones.
					 * rationale: I/O penalty for cache miss is relatively higher for small bats 
					 */
					int swap_first = 0;
					ssize_t cnt = -1;

					if (b) {
						cnt = BATcount(b);
						swap_first = (cnt >= BBPSMALLBAT);
					}

					/* however, when we are looking to decrease the number of descriptors,
					 * try to put the small bats in front of the load list instead..
					 */

					/* subtract 2-billion to make sure the swap_first class bats are unloaded first */
					bbptrim[bbptrimlast].lastused = BBPLASTUSED(BBP_lastused(bbppos)) | (swap_first << 31);
					bbptrim[bbptrimlast].bid = bbppos;
					bbptrim[bbptrimlast].cnt = cnt;
					if (++bbptrimlast == bbptrimmax)
						break;
				}
			}
			if (++bbppos == BBPsize)
				bbppos = 1;	/* treat BBP as a circular buffer */
		} while (bbppos != bbplim);

	if (bbptrimlast > 0) {
		int i;
		GDKqsort(bbptrim, NULL, NULL, bbptrimlast, sizeof(bbptrim_t), 0, TYPE_int);
		for (i = bbptrimfirst = 0; i < bbptrimlast; i++) {
			MEMDEBUG THRprintf(GDKout, "#TRIMSCAN: %11d%c %9d=%s\t(#" SSZFMT ")\n", BBPLASTUSED(bbptrim[i].lastused), (bbptrim[i].lastused & 0x80000000) ? '*' : ' ', i, BBPname(bbptrim[i].bid), bbptrim[i].cnt);

			bbptrim[i].next = i + 1;
		}
		bbptrim[bbptrimlast - 1].next = BBPMAXTRIM;
	} else {
		bbptrimfirst = BBPMAXTRIM;
	}
	MEMDEBUG THRprintf(GDKout, "#TRIMSCAN: end at %d (size=%d)\n", bbppos, (int) BBPsize);

	return bbppos;
}


/* insert BATs to unload from bbptrim list into bbpunload list; rebuild bbptrimlist only with the useful leftovers */
static void
BBPtrim_select(size_t * memtarget, size_t * vmtarget, int dirty)
{
	int bbptrimtail = BBPMAXTRIM, next = bbptrimfirst;

	MEMDEBUG THRprintf(GDKout, "#TRIMSELECT: dirty = %d\n", dirty);

	/* make the bbptrim-list empty; we will insert the untouched elements in it */
	bbptrimfirst = BBPMAXTRIM;

	while (next != BBPMAXTRIM) {
		int cur = next;	/* cur is the entry in the old bbptrimlist we are processing */
		int untouched = BBPLASTUSED(BBP_lastused(bbptrim[cur].bid)) <= BBPLASTUSED(bbptrim[cur].lastused);
		BAT *b = BBP_cache(bbptrim[cur].bid);

		next = bbptrim[cur].next;	/* do now, because we overwrite bbptrim[cur].next below */

		MEMDEBUG if (b) {
			THRprintf(GDKout, "#TRIMSELECT: candidate=%s BAT*=" PTRFMT "\n", BBPname(bbptrim[cur].bid), PTRFMTCAST(void *)b);

			THRprintf(GDKout, "#            (cnt=" SSZFMT ", mode=%d, refs=%d, wait=%d, parent=%d,%d, lastused=%d,%d,%d)\n",
				  bbptrim[cur].cnt, b->batPersistence,
				  BBP_refs(b->batCacheid),
				  (BBP_status(b->batCacheid) & BBPWAITING) != 0,
				  VIEWhparent(b), VIEWtparent(b),
				  BBP_lastused(b->batCacheid),
				  BBPLASTUSED(bbptrim[cur].lastused),
				  bbptrim[cur].lastused);
		}
		/* recheck if conditions encountered by trimscan in the past still hold */
		if (BBPtrimmable(b) && untouched) {
			size_t memdelta = BATmemsize(b, FALSE);
			size_t vmdelta = BATvmsize(b, FALSE);
			size_t memdirty = BATmemsize(b, TRUE);
			size_t vmdirty = BATvmsize(b, TRUE);

			if (((b->batPersistence == TRANSIENT && BBP_lrefs(bbptrim[cur].bid) == 0) ||	/* needs not be saved when unloaded, OR.. */
			     (vmdirty == 0 && memdirty <= sizeof(BATstore)) ||	/* the BAT is actually clean, OR.. */
			     dirty)	/* we are allowed to cause I/O (second run).. */
			    &&	/* AND ... */
			    ((*memtarget > 0 && (memdelta > 0)) || (*vmtarget > 0 && (vmdelta > 0))))
				/* there is some reward in terms of memory requirements */
			{
				/* only then we unload! */
				MEMDEBUG {
					THRprintf(GDKout, "#TRIMSELECT: unload %s [" SZFMT "," SZFMT "] bytes [" SZFMT "," SZFMT "] dirty\n", BBPname(b->batCacheid), memdelta, vmdelta, memdirty, vmdirty);
				}
				BBP_status_on(bbptrim[cur].bid, BBPUNLOADING, "BBPtrim_select");
				BBP_unload_inc(bbptrim[cur].bid, "BBPtrim_select");
				*memtarget = *memtarget > memdelta ? *memtarget - memdelta : 0;
				*vmtarget = *vmtarget > vmdelta ? *vmtarget - vmdelta : 0;

				/* add to bbpunload list */
				if (bbpunload == BBPMAXTRIM) {
					bbpunload = cur;
				} else {
					bbptrim[bbpunloadtail].next = cur;
				}
				bbptrim[cur].next = BBPMAXTRIM;
				bbpunloadtail = cur;
			} else if (!dirty) {
				/* do not unload now, but keep around; insert at the end of the new bbptrim list */
				MEMDEBUG {
					THRprintf(GDKout, "#TRIMSELECT: keep %s [" SZFMT "," SZFMT "] bytes [" SZFMT "," SZFMT "] dirty target(mem=" SZFMT " vm=" SZFMT ")\n",
						  BBPname(b->batCacheid), memdelta, vmdelta, memdirty, vmdirty,
						  MAX(0, *memtarget), MAX(0, *vmtarget));
				}
				if (bbptrimtail == BBPMAXTRIM) {
					bbptrimfirst = cur;
				} else {
					bbptrim[bbptrimtail].next = cur;
				}
				bbptrim[cur].next = BBPMAXTRIM;
				bbptrimtail = cur;
			} else {
				/* bats that even in the second (dirty) run are not selected, should be acquitted from the trimlist until a next scan */
				MEMDEBUG THRprintf(GDKout, "#TRIMSELECT: delete %s from trimlist (does not match trim needs)\n", BBPname(bbptrim[cur].bid));
			}
		} else {
			/* BAT was touched (or unloaded) since trimscan =>  it is discarded from both lists */
			char buf[80], *bnme = BBP_logical(bbptrim[cur].bid);

			if (bnme == NULL) {
				bnme = BBPtmpname(buf, 64, bbptrim[cur].bid);
			}
			MEMDEBUG THRprintf(GDKout, "#TRIMSELECT: delete %s from trimlist (has been %s)\n", bnme, b ? "touched since last scan" : "unloaded already");
		}

		if (*memtarget == 0 && *vmtarget == 0) {
			/* we're done; glue the rest of the old bbptrim list to the new bbptrim list */
			if (bbptrimtail == BBPMAXTRIM) {
				bbptrimfirst = next;
			} else {
				bbptrim[bbptrimtail].next = next;
			}
			break;
		}
	}
	MEMDEBUG THRprintf(GDKout, "#TRIMSELECT: end\n");
}

extern int monet_exec(str);

void
BBPtrim(size_t memtarget, size_t vmtarget)
{
	int i, limit, scan, did_scan = FALSE;
	int msec = 0, bats_written = 0, bats_unloaded = 0;	/* performance info */
	MT_Id t = BBP_getpid();

	PERFDEBUG msec = GDKms();

	if (BBP_notrim == t)
		return;		/* avoid deadlock by one thread going here twice */

	for (i = 0; i <= BBP_THREADMASK; i++)
		gdk_set_lock(GDKtrimLock(i), "BBPtrim");
	BBP_notrim = t;

	/* recheck targets to see whether the work was already done by another thread */
	if (memtarget && memtarget != BBPTRIM_ALL) {
		memtarget = GDKmem_inuse();
		if (memtarget > GDK_mem_maxsize)
			memtarget -= GDK_mem_maxsize;
		else
			memtarget = 0;
	}
	if (vmtarget && vmtarget != BBPTRIM_ALL) {
		vmtarget = GDKvm_cursize();
		if (vmtarget > GDK_vm_maxsize)
			vmtarget -= GDK_vm_maxsize;
		else
			vmtarget = 0;
	}
	MEMDEBUG THRprintf(GDKout, "#BBPTRIM_ENTER: memsize=" SZFMT ",vmsize=" SZFMT "\n", GDKmem_inuse(), GDKvm_cursize());

	MEMDEBUG THRprintf(GDKout, "#BBPTRIM: memtarget=" SZFMT " vmtarget=" SZFMT "\n", memtarget, vmtarget);
	PERFDEBUG THRprintf(GDKout, "#BBPtrim(mem=%d,vm=%d)\n", memtarget > 0, vmtarget > 0);

	scan = (bbptrimfirst == BBPMAXTRIM);
	if (bbpscanstart >= BBPsize)
		bbpscanstart = 1;	/* sometimes, the BBP shrinks! */
	limit = bbpscanstart;

	while (memtarget > 0 || vmtarget > 0) {
		/* acquire the BBP locks */
		for (i = 0; i <= BBP_THREADMASK; i++)
			gdk_set_lock(GDKcacheLock(i), "BBPtrim");
		for (i = 0; i <= BBP_BATMASK; i++)
			gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPtrim");

		/* gather a list of unload candidate BATs, but try to avoid scanning by reusing previous leftovers first */
		if (scan) {
			did_scan = TRUE;
			bbpscanstart = BBPtrim_scan((memtarget > 0), (vmtarget > 0), bbpscanstart, limit);
			scan = (bbpscanstart != limit);
		} else {
			scan = TRUE;
		}

		/* decide which of the candidates to unload using LRU */
		bbpunload = BBPMAXTRIM;
		BBPtrim_select(&memtarget, &vmtarget, FALSE);	/* first try to select only clean BATs */
		if (did_scan && (memtarget > 0 || vmtarget > 0)) {
			BBPtrim_select(&memtarget, &vmtarget, TRUE);	/* if that is not enough, also unload dirty BATs */
		}

		/* release the BBP locks */
		for (i = 0; i <= BBP_BATMASK; i++)
			gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPtrim");
		for (i = 0; i <= BBP_THREADMASK; i++)
			gdk_unset_lock(GDKcacheLock(i), "BBPtrim");

		/* do the unload work unlocked */
		MEMDEBUG THRprintf(GDKout, "#BBPTRIM: %s\n", (bbpunload != BBPMAXTRIM) ? " lastused   batid name" : "no more unload candidates!");

		for (i = bbpunload; i != BBPMAXTRIM; i = bbptrim[i].next) {
			BAT *b = BBP_cache(bbptrim[i].bid);

			if (b == NULL || !(BBP_status(bbptrim[i].bid) & BBPUNLOADING)) {
				GDKwarning("BBPtrim: bat(%d) gone\n", bbptrim[i].bid);
				continue;
			}
			MEMDEBUG THRprintf(GDKout, "#BBPTRIM: %9d %7d %s\n", bbptrim[i].lastused, (int) bbptrim[i].bid, BBPname(bbptrim[i].bid));

			bats_written += (b->batPersistence != TRANSIENT && BATdirty(b));
			bats_unloaded++;
			BBPfree(b, "BBPtrim");
		}
		/* continue while we can scan for more candiates */
		if (!scan)
			break;
	}
	/* done trimming */
	MEMDEBUG THRprintf(GDKout, "#BBPTRIM_EXIT: memsize=" SZFMT ",vmsize=" SZFMT "\n", GDKmem_cursize(), GDKvm_cursize());
	PERFDEBUG THRprintf(GDKout, "#BBPtrim(did_scan=%d, bats_unloaded=%d, bats_written=%d) %d ms\n", did_scan, bats_unloaded, bats_written, GDKms() - msec);

	BBP_notrim = 0;
	for (i = BBP_THREADMASK; i >= 0; i--)
		gdk_unset_lock(GDKtrimLock(i), "BBPtrim");
}

void
BBPhot(bat i)
{
	if (i < 0)
		i = -i;
	if (BBPcheck(i, "BBPhot")) {
		int lock = locked_by ? BBP_getpid() != locked_by : 1;

		if (lock)
			gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPhot");
		BBP_lastused(i) = BBPLASTUSED(BBPstamp() + 30000);
		if (lock)
			gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPhot");
	}
}

void
BBPcold(bat i)
{
	if (i < 0)
		i = -i;
	if (BBPcheck(i, "BBPcold")) {
		MT_Id pid = BBP_getpid();
		int lock = locked_by ? pid != locked_by : 1;

		gdk_set_lock(GDKtrimLock(pid & BBP_THREADMASK), "BBPcold");
		if (lock)
			gdk_set_lock(GDKswapLock(i & BBP_BATMASK), "BBPcold");
		/* make very cold and insert on top of trim list */
		BBP_lastused(i) = 0;
		if (BBP_cache(i) && bbptrimlast < bbptrimmax) {
			bbptrim[--bbptrimmax].lastused = 0;
			bbptrim[bbptrimmax].bid = i;
			bbptrim[bbptrimmax].next = bbptrimfirst;
			bbptrimfirst = bbptrimmax;
		}
		if (lock)
			gdk_unset_lock(GDKswapLock(i & BBP_BATMASK), "BBPcold");
		gdk_unset_lock(GDKtrimLock(pid & BBP_THREADMASK), "BBPcold");
	}
}

@}
@-
BBPquickdesc loads a BAT descriptor without loading the entire BAT, of which the
result be used only for a *limited* number of purposes. Specifically, during the 
global sync/commit, we do not want to load any BATs that are not already loaded, both 
because this costs performance, and because getting into memory shortage during a commit 
is extremely dangerous, as the global sync has all the BBPlocks, so no BBPtrim() can be 
done to free memory when needed. Loading a BAT tends not to be required, since the commit 
actions mostly involve moving some pointers in the BAT descriptor. However, some column 
types do require loading the full bat. This is tested by the complexatom() routine. Such 
columns are those of which the type has an fix/unfix method, or those that have HeapDelete
methods. The HeapDelete actions are not always required and therefore the BBPquickdesc
is parametrized.
@{
@c
static int
complexatom(int t, int delaccess)
{
	if (t >= 0 && (BATatoms[t].atomFix || (delaccess && BATatoms[t].atomDel))) {
		return TRUE;
	}
	return FALSE;
}

BAT *
BBPquickdesc(bat bid, int delaccess)
{
	BAT *b = BBP_cache(bid);

	if (bid < 0) {
		GDKerror("BBPquickdesc: called with negative batid.\n");
		return NULL;
	}
	if (b) {
		return b;	/* already cached */
	}
	b = (BAT *) BBPgetdesc(bid);
	if (b == NULL || complexatom(b->htype, delaccess) || complexatom(b->ttype, delaccess)) {
		b = BATload_intern(bid);
		BBPin++;
	}
	return b;
}

@}

@+ Small BAT Cache
@T
PETER: rewrote the batcache to make it stable with views and actually faster
       than allocating new bats (I guess that was the purpose of it)

this is tuned to minimizing the needed actions to get a BAT, in particular
we will do no BBPinsert, BUN dimension modifications, BATextends().

it also covers more cases, such as views and TYPE_str

main ideas:
- have [htpe,ttpe] specific lists to O(1) get a BAT from the desired type (bin-lists)
- implement LRU when the cache is full, with O(1) delete cost by using a fifo list
- keep bats in the BBP, as zombies (by invalidating their name)
- support views as void,void bats (they have in common that they lack heaps) 
- support TYPE_str as well, zapping their built-in string hash table
- only recycle BATTINY bats. Increase BATTINY to make more BATnews use the cache.
  BATTINY=256 leading to bunheaps ~1K-2K (in balance with ~500byte BAT record)

and then benchmark the d*mn thing (also missing in the previous exercise) :

module(alarm);
bbp_batcache_minsize(wrd(256));
{ var t := time(), i := 0; while((i :+= 1) < 100000) bat(int,int); print(time() - t); }
[ 490 ]
bbp_batcache_minsize(wrd(0));
{ var t := time(), i := 0; while((i :+= 1) < 100000) bat(int,int); print(time() - t); }
[ 847 ]

so with caching *some* MIL programs can be nearly twice as fast (optimized compile),
though I expect the gains to be smaller in general.
@{
@c
#define BATCACHE_NOTYPE(t) 	(ATOMstorage(t) > TYPE_str || BATatoms[t].atomFix != NULL)
#define BATCACHE_SIZE 		1023	/* max size: 32767 */
#define BATCACHE_DIMS 		6	/* 0,1,2,4,8 byte types + str */
#define BATCACHE_BIN(h,t) 	(batcache_headbin[ATOMstorage(h)]+batcache_tailbin[ATOMstorage(t)])

int batcache_headbin[TYPE_str + 1], batcache_tailbin[TYPE_str + 1];	/* fast bin computation */

typedef signed short batcache_int;	/* make compact, whole structure is < 500 bytes: CPU cache resident  */

size_t batcache_minsize = BATTINY;

/* the list elements, use int as element pointer type (<0 means: no such element) */
typedef struct {
	bat bid;
	batcache_int bin_next, bin_prev;	/* doubly linked list that connects all BATs for a bin */
	batcache_int fifo_next, fifo_prev;	/* doubly linked list for getting the LRU BAT O(1) */
} batcache_elt_t;

typedef struct {
	MT_Lock lock;

	/* each bin starts a bin_next/prev list */
	batcache_int batbin[BATCACHE_DIMS * BATCACHE_DIMS];
	batcache_elt_t elt[BATCACHE_SIZE];	/* main storage */

	/* fifo queue */
	batcache_int first, last;

	/* free list */
	batcache_int free;	/* misuses ->fifo_next for connecting the list */
} batcache_t;

/* we have multiple bat caches for better SMP performance */
static batcache_t batcache[BBP_THREADMASK + 1];

static void
BBPinitcache(void)
{
	batcache_int i;
	int j;

	/* initialize the type remap arrays */
	for (i = 0; i < TYPE_str; i++) {
		int k = 0;

		for (j = ATOMsize(i); j; j /= 2)
			k++;
		batcache_headbin[i] = k;
		batcache_tailbin[i] = k * BATCACHE_DIMS;
	}
	batcache_headbin[i] = 5;
	batcache_tailbin[i] = 5 * BATCACHE_DIMS;	/* TYPE_str */

	/* initialize the cache */
	for (j = 0; j <= BBP_THREADMASK; j++) {
		MT_init_lock(batcache[j].lock, "GDKbinsLock");

		for (i = 0; i < BATCACHE_SIZE; i++) {
			batcache[j].elt[i].bid = 0;
			batcache[j].elt[i].fifo_next = i + 1;
			batcache[j].elt[i].fifo_prev = i - 1;
			batcache[j].elt[i].bin_next = -1;
			batcache[j].elt[i].bin_prev = -1;
		}
		batcache[j].elt[BATCACHE_SIZE - 1].fifo_next = -1;

		/* initialize the lookup mechanism */
		for (i = 0; i < BATCACHE_DIMS * BATCACHE_DIMS; i++) {
			batcache[j].batbin[i] = -1;
		}
		batcache[j].free = 0;
		batcache[j].first = -1;
		batcache[j].last = -1;
	}
}

/* throw the LRU bat out using the fifo list */
static INLINE batcache_int
batcache_del(batcache_t * bc)
{
	batcache_int i = bc->first;
	bat bid = bc->elt[i].bid;
	BAT *b = BBP_cache(bid);
	int bin = BATCACHE_BIN(b->htype, b->ttype);

	/* manage bin-list */
	batcache_int j = bc->elt[i].bin_prev;

	if (j >= 0) {
		bc->elt[j].bin_next = -1;
	} else {
		bc->batbin[bin] = -1;	/* had a single entry only */
	}
	assert(bc->elt[i].bin_next < 0);	/* LRU: we are always deleting the last */

	/* manage fifo-list */
	bc->first = bc->elt[i].fifo_next;
	if (bc->first >= 0) {
		bc->elt[bc->first].fifo_prev = -1;
	} else {
		bc->last = -1;
	}

	/* revive and free the old bat */
	assert(b->H->parentid == 0 && b->T->parentid == 0);
	BATfree(b);		/* maybe unsafe without lock ? */
	*BBP_logical(bid) = 't';
	bbpclear(bid, bc - batcache, "batcache_del");
	return i;
}

/* insert a new bat into the cache */
static INLINE int
batcache_put(batcache_t * bc, int bin, bat bid)
{
	/* get a free slot */
	batcache_int j, i;
	MT_set_lock(bc->lock, "batcache_put");

	i = bc->free;
	if (i >= 0) {
		bc->free = bc->elt[i].fifo_next;
	} else {
		i = batcache_del(bc);	/* delete a bat from the cache */
	}

	/* fill in the batcache record */
	bc->elt[i].bid = bid;
	bc->elt[i].bin_next = j = bc->batbin[bin];
	if (j >= 0)
		bc->elt[j].bin_prev = i;
	bc->elt[i].bin_prev = -1;
	bc->elt[i].fifo_next = -1;
	bc->elt[i].fifo_prev = bc->last;
	bc->batbin[bin] = i;
	if (bc->first < 0)
		bc->first = i;
	if (bc->last >= 0)
		bc->elt[bc->last].fifo_next = i;
	bc->last = i;

	*BBP_logical(bid) = '.';	/* make the bat a zombie */
	MT_unset_lock(bc->lock, "batcache_put");

	return i;
}

/* return and revive a cached bat from the cache */
static INLINE bat
batcache_get(batcache_t * bc, int bin)
{
	batcache_int i;
	int bid = 0;

	MT_set_lock(bc->lock, "batcache_get");

	i = bc->batbin[bin];
	if (i >= 0) {
		/* manage fifo-list */
		batcache_int j = bc->elt[i].fifo_prev;
		batcache_int k = bc->elt[i].fifo_next;

		bid = bc->elt[i].bid;

		if (j < 0) {
			assert(bc->first == i);
			bc->first = k;
		} else {
			bc->elt[j].fifo_next = k;
		}
		if (k < 0) {
			assert(bc->last == i);
			bc->last = j;
		} else {
			bc->elt[k].fifo_prev = j;
		}
		bc->elt[i].fifo_next = bc->free;
		bc->free = i;

		/* manage bin-list */
		bc->batbin[bin] = j = bc->elt[i].bin_next;
		if (j >= 0)
			bc->elt[j].bin_prev = -1;

		/* revive the bat */
		*BBP_logical(bid) = 't';
		BBP_refs(bid)++;

		/* clearing bits can be done without the lock */
		BBP_status_off(bid, BBPUNLOADING, "BBPrecycle");
	}
	MT_unset_lock(bc->lock, "batcache_get");

	return bid;
}

extern char *BATstring_h;
extern char *BATstring_t;

static int
BBPaddtobin(BAT *b)
{
	int bin = BATCACHE_BIN(b->htype, b->ttype);
	bat bid = b->batCacheid, hp = VIEWhparent(b), tp = VIEWtparent(b);
	char *s = BBP_logical(bid);
	BAT *m = BBP_cache(-bid);

	/* only cache simple non-saved non-renamed transient bats */
	if (m == NULL ||
	    batcache_minsize == 0 ||
	    b->batCopiedtodisk || 	
	    (b->H->heap.storage & STORE_MMAP) || 
	    (b->T->heap.storage & STORE_MMAP) ||
	    b->P->lview ||
	    BBPtmpcheck(s) == 0 ||
	    b->htype > TYPE_str ||
	    b->ttype > TYPE_str) {
		BATdelete(b);	/* handles persistent case also (file deletes) */
		return 1;
	}
	if ((hp == 0 && tp == 0) && (b->htype | b->ttype)) {
		/* additional restrictions do not hold for views and void,void bats */
		/* bat should be of the right size, types and BUN layoutt */
		if (b->U->capacity < batcache_minsize ||
		    b->U->capacity >= (batcache_minsize<<1) ||
		    BATCACHE_NOTYPE(b->htype) ||
		    BATCACHE_NOTYPE(b->ttype)) {
			BATdelete(b);	/* handles persistent case also (file deletes) */
			return 1;
		}
	}
	assert(BBP_refs(bid) == 0);
	assert(BBP_lrefs(bid) == 0);
	assert(b->batCacheid > 0);

	/* change views into void,void bats */
	if (hp || tp) {
		BATstore *bs = (BATstore *) b;

		/* cut view loose from parent */
		VIEWunlink(b);
		if (hp)
			unshare(hp);
		if (tp)
			unshare(tp);
		b->H->parentid = b->T->parentid = 0;

		/* take care of mirror views */
		b->H = &bs->H;
		b->T = &bs->T;

		/* make it a void,void bat */
		b->H->type = TYPE_void;
		b->H->key = FALSE;
		b->H->varsized = TRUE;
		b->H->shift = 0;
		b->H->width = 0;
		b->H->seq = oid_nil;

		b->T->type = TYPE_void;
		b->T->key = FALSE;
		b->T->varsized = TRUE;
		b->T->shift = 0;
		b->T->width = 0;
		b->T->seq = oid_nil;

		/* was shared with parent! */
		b->H->heap.base = b->T->heap.base = NULL;
		b->H->vheap = b->T->vheap = NULL;
	}
	/* free non-reusable stuff that has to be freed anyway */
	if (b->hident != BATstring_h) {
		if (b->hident)
			GDKfree(b->hident);
		b->hident = BATstring_h;
	}
	if (b->tident != BATstring_t) {
		if (b->tident)
			GDKfree(b->tident);
		b->tident = BATstring_t;
	}
	if (b->H->hash)
		HASHremove(b);
	if (b->T->hash)
		HASHremove(BBPcache(-bid));
	if (b->H->props) {
		PROPdestroy(b->H->props);
		b->H->props = NULL;
	}
	if (b->T->props) {
		PROPdestroy(b->T->props);
		b->T->props = NULL;
	}

	batcache_put(batcache + (BBP_getpid() & BBP_THREADMASK), bin, b->batCacheid);
	return 0;
}

BAT *
BBPrecycle(int ht, int tt, size_t cap)
{
	int bin = BATCACHE_BIN(ht, tt);
	bat bid = 0;
	BAT *b;

	if (cap > batcache_minsize || BATCACHE_NOTYPE(ht) || BATCACHE_NOTYPE(tt))
		return NULL;

	bid = batcache_get(batcache + (BBP_getpid() & BBP_THREADMASK), bin);
	b = BBP_cache(bid);
	if (b) {
		/* make the bat empty; do this extra work only if the bat is reused (not in BBPaddtobin) */
		BAT *m = BBP_cache(-bid);

		b->htype = m->ttype = ht;
        	b->H->varsized = BATatoms[b->htype].varsized;
		b->hsorted = ATOMlinear(ht) ? GDK_SORTED : 0;
		b->halign = 0;
		b->hkey = m->tkey = FALSE;
		b->hdense = 0;
		b->hseqbase = m->tseqbase = (ht == TYPE_void) ? oid_nil : 0;

		b->ttype = m->htype = tt;
        	b->T->varsized = BATatoms[b->ttype].varsized;
		b->tsorted = ATOMlinear(tt) ? GDK_SORTED : 0;
		b->talign = 0;
		b->tkey = m->hkey = FALSE;
		b->tdense = 0;
		b->tseqbase = m->hseqbase = (tt == TYPE_void) ? oid_nil : 0;

		b->batSet = FALSE;
		b->batFirst = b->batInserted = b->batDeleted = 0;
		b->batRestricted = 0;
		b->H->heap.free = 0;
		b->T->heap.free = 0;
		b->batDirty = TRUE;
		b->batStamp = 0;

		BATsetcount(b, 0);

		if (b->hheap)
			memset(b->hheap->base, 0, b->hheap->free = GDK_STRHASHTABLE * sizeof(var_t));
		if (b->theap)
			memset(b->theap->base, 0, b->theap->free = GDK_STRHASHTABLE * sizeof(var_t));
		return b;
	}
	return NULL;
}

/* query and change the minimum bat size to cache */
wrd
BBPrecycle_minsize(wrd val)
{
	if (val != wrd_nil) {
		int j;		/* flush the caches */

		for (j = 0; j <= BBP_THREADMASK; j++) {
			gdk_set_lock(batcache[j].lock, "BBPrecycle_minsize");

			while (batcache[j].first >= 0) {
				batcache_int i = batcache_del(batcache + j);

				batcache[j].elt[i].fifo_next = batcache[j].free;
				batcache[j].free = i;
			}
			gdk_unset_lock(batcache[j].lock, "BBPrecycle_minsize");
		}
		batcache_minsize = val;
	}
	val = (wrd) batcache_minsize;
	return val;
}

@}

@+ Global Commit

@- Is a bat dirty at commit time?

Used to be simple: look if it is loaded. If not, only its cached descriptor might still
be dirty. Otherwise look at the union of all dirty bits.

But, sometimes clean BATs must be made dirty, just because this is called from a commit!

This happens in a commit where we have persistent BATs that fulfill all following conditions:
@itemize
item (1) 
are updatable (non BAT_READ) 
item (2) 
are new in this commit
item (3) 
have at least one huge STORE_MMAP .priv heap
@end itemize

These BATs are a problem, as after the commit, the STORE_MMAP status should be a STORE_PRIV
(subsequent updates followed by a crash corrupt the stable heap image).

Also, if such a BAT was already clean at commit time (due to a BBPtrim or explicit save),
its descriptor would not even be saved. Recall that HEAPcheckmodes in gdk_storage.mx would 
at least have re-set the heap->storage modes in the desc correctly for subsequent BAT loads. 
Not even that will happen if the BAT is clean.

So, one thing we do is making the descriptor dirty always for such BATs. This solves the
latter problem.

Also, if there are no hot-locks on this BAT, we just unload the BAT inside the commit.
In this case, we do not modify the storage modes yet. Thus, its heaps get saved with 
an efficient msync(). After the unload, a re-load will use the correct STORE_PRIV mode. 

Only if the BAT cannot be unloaded, we hack the storage state of the BAT, such that it 
appears to be STORE_PRIV. This works, but causes a very expensive write()-based save of
the entire bat, and duplicates disk consumption for its heap.
@{
@c
static BAT *
dirty_bat(bat i, int subcommit, int *unload)
{
	if (BBPvalid(i)) {
		int unloadable = (BBP_refs(i) == 0);	/* are no other threads currently executing on this image?? */
		int newbat = (BBP_status(i) & BBPNEW);
		BAT *b;

		BBPspin(i, "dirty_bat", BBPSAVING);
		b = BBP_cache(i);
		if (b != NULL) {
			if (BBP_status(i) & BBPPERSISTENT) {
				if (unload)
					*unload = BATcheckmodes(b, newbat, unloadable) && unloadable;
				if (subcommit || BATdirty(b))
					return b;	/* the bat is loaded, persistent and dirty */
			}
		} else if (BBP_status(i) & BBPSWAPPED) {
			b = (BAT *) BBPquickdesc(i, TRUE);
			if (b) {
				if (unload)
					(void) BATcheckmodes(b, newbat, 0);
				if (subcommit || b->batDirtydesc)
					return b;	/* only the desc is loaded & dirty */
			}
		}
	}
	return NULL;
}

@}

@- backup-bat
Backup-bat moves all files of a BAT to a backup directory. Only after this
succeeds, it may be saved. If some failure occurs halfway saving, we
can thus always roll back.
@{
@c
static int
file_move(str srcdir, str dstdir, str name, str e)
{
	long_str ext;
	int ret = 0;

	strcpy(ext, e);
	ret = GDKmove(srcdir, name, ext, dstdir, name, ext);
	if (ret == 0) {
		return 0;
	}
	if (ret == 0) {
		return 0;
	} else {
		long_str path;
		struct stat st;

		GDKfilepath(path, srcdir, name, ext);
		if (stat(path, &st)) {
			/* source file does not exist; the best recovery is to give an error but continue
			 * by considering the BAT as not saved; making sure that this time it does get saved.
			 */
			return 2;	/* indicate something fishy, but not fatal */
		}
	}
	return 1;
}

/* returns 1 if the file exists */
static int
file_exists(str dir, str name, str ext)
{
	long_str path;
	struct stat st;

	GDKfilepath(path, dir, name, ext);
	return (stat(path, &st) == 0);
}

static int
heap_move(Heap *hp, str srcdir, str dstdir, str nme, str ext)
{
	/* see doc at BATsetaccess()/gdk_bat.mx for an expose on .priv heap modes */
	if (file_exists(dstdir, nme, ext)) {
		return 0;	/* dont overwrite heap with the committed state already in dstdir */
	} else if (hp->filename && hp->storage == STORE_PRIV) {
		long_str path;
		struct stat st;

		GDKfilepath(path, srcdir, nme, ext);
		if (stat(path, &st)) {
			/* in order to prevent half-saved X files surviving a recover
			 * we create a dummy file in the BACKUP(dstdir) whose precense
			 * will trigger BBPrecover to remove them. Thus, X.priv will 
			 * prevail where it otherwise wouldn't have.
			 */
			FILE *fp;
			long_str kill_ext;

			strcpy(kill_ext, ext);
			strcat(kill_ext, ".kill");
			GDKfilepath(path, dstdir, nme, kill_ext);
			fp = fopen(path, "w");
			IODEBUG THRprintf(GDKout, "#open %s = %d\n", path, fp ? 0 : -1);

			if (fp != NULL) {
				fclose(fp);
				return 0;
			} else {
				return 1;
			}
		}
		/* if X.priv already has a saved X, that one is backed up as normal.. */
	}
	return file_move(srcdir, dstdir, nme, ext);
}

@}

@- BBPprepare

this routine makes sure there is a BAKDIR/, and initiates one if not.
For subcommits, it does the same with SUBDIR.

It is now locked, to get proper file counters, and also to prevent
concurrent BBPrecovers, etc.
@T
backup_dir == 0 => no backup BBP.dir 
backup_dir == 1 => BBP.dir saved in BACKUP/
backup_dir == 2 => BBP.dir saved in SUBCOMMIT/
@{
@c
static int backup_files = 0, backup_dir = 0, backup_subdir = 0;

static int
BBPprepare(bit subcommit)
{
	int start_subcommit, ret = 0, set = 1 + subcommit;

	/* tmLock is only used here, helds usually very shortly just to protect the file counters */
	gdk_unset_lock(GDKtmLock, "BBPprepare");

	start_subcommit = (subcommit && backup_subdir == 0);
	if (start_subcommit) {
		/* starting a subcommit. Make sure SUBDIR and DELDIR are clean */
		ret = (BBPrecover_subdir() < 0);
	}
	if (backup_files == 0) {
		struct stat st;

		backup_dir = 0;
		ret = (stat(BAKDIR, &st) == 0 && BBPrecover());

		if (ret == 0) {
			/* make a new BAKDIR */
			ret = mkdir(BAKDIR, 0755);
			IODEBUG THRprintf(GDKout, "#mkdir %s = %d\n", BAKDIR, ret);
		}
	}
	if (ret == 0 && start_subcommit) {
		/* make a new SUBDIR (subdir of BAKDIR) */
		ret = mkdir(SUBDIR, 0755);
		IODEBUG THRprintf(GDKout, "#mkdir %s = %d\n", SUBDIR, ret);
	}
	if (ret == 0 && backup_dir != set) {
		/* a valid backup dir *must* at least contain BBP.dir */
		if (GDKmove(backup_dir ? BAKDIR : BATDIR, "BBP", "dir", subcommit ? SUBDIR : BAKDIR, "BBP", "dir")) {
			ret = 1;
		} else {
			backup_dir = set;
		}
	}
	/* increase counters */
	if (ret == 0) {
		backup_subdir += subcommit;
		backup_files++;
	}
	gdk_unset_lock(GDKtmLock, "BBPprepare");

	return ret ? -1 : 0;
}

int
BBPbackup(BAT *b, bit subcommit)
{
	long_str srcdir, nme;
	str s = BBP_physical(b->batCacheid);
	int ret = 0;

	if (BBPprepare(subcommit)) {
		return -1;
	}
	if (b->batCopiedtodisk == 0 || nme == NULL || b->batPersistence != PERSISTENT) {
		return 0;
	}
	/* determine location dir and physical suffix */
	GDKfilepath(srcdir, BATDIR, s, NULL);
	s = strrchr(srcdir, DIR_SEP);
	strcpy(nme, ++s);
	srcdir[s - srcdir] = 0;

	if ((b->batDirty || b->batDirtydesc) && !file_exists(BAKDIR, nme, "desc")) {
		/* file will be saved (is dirty), move the old image into backup */
		ret |= file_move(srcdir, subcommit ? SUBDIR : BAKDIR, nme, "desc");
	} else if (subcommit && (b->batDirty || b->batDirtydesc || file_exists(BAKDIR, nme, "desc"))) {
		/* file is clean. move the backup into the subcommit dir (commit should eliminate backup) */
		ret |= file_move(BAKDIR, SUBDIR, nme, "desc");
	}
	if (ret & 1)
		return -1;
@= backup
	if (@2 && @2->storage != STORE_MMAP) { /* direct mmap is unprotected (readonly usage, or has WAL protection)  */
		str ext = @2->filename && @2->storage == STORE_PRIV ? "@1.kill" : "@1";
		if ((@3) && !file_exists(BAKDIR, nme, ext)) { 
                	/* file will be saved (is dirty), move the old image into backup */
		        ret |= heap_move(@2, srcdir, subcommit ? SUBDIR : BAKDIR, nme, "@1");
       		} else if (subcommit && ((@3) || file_exists(BAKDIR, nme, ext))) {
                	/* file is clean. move the backup into the subcommit dir (commit should eliminate backup) */
			ret |= file_move(BAKDIR, SUBDIR, nme, ext);
		}
		if (ret & 1)
			return -1;
	}
@c
	@:backup(head,(&b->H->heap), b->batDirty || b->H->heap.dirty)@
	@:backup(tail,(&b->T->heap), b->batDirty || b->T->heap.dirty)@
	@:backup(hheap,b->H->vheap, (b->batDirty || (b->H->vheap && b->H->vheap->dirty)) && b->htype && b->hvarsized)@
	@:backup(theap,b->T->vheap, (b->batDirty || (b->T->vheap && b->T->vheap->dirty)) && b->ttype && b->tvarsized)@
	return 0;
}

@}

@+ Atomic Write
The atomic BBPsync() function first safeguards the old images of all files 
to be written in BAKDIR. It then saves all files. If that succeeds
fully, BAKDIR is renamed to DELDIR. The rename is considered an 
atomic action. If it succeeds, the DELDIR is removed.
If something fails, the pre-sync status can be obtained by moving
back all backed up files; this is done by BBPrecover().

The BBP.dir is also moved into the BAKDIR.

@{
@c
int
BBPsync(int cnt, bat *subcommit)
{
	int ret = 0, bbpdirty = 0;
	int t0 = 0, t1 = 0;

	PERFDEBUG t0 = t1 = GDKms();

	ret = BBPprepare(subcommit != NULL);

	/* PHASE 1: safeguard everything in a backup-dir */
	bbpdirty = BBP_dirty;
	if (OIDdirty()) {
		bbpdirty = BBP_dirty = 1;
	}
	if (ret == 0) {
		int idx = 0;

		while (++idx < cnt) {
			bat i = subcommit ? subcommit[idx] : idx;

			if (BBP_status(i) & BBPEXISTING) {
				BAT *b = dirty_bat(i, subcommit != NULL, NULL);

				if (b != NULL && BBPbackup(b, subcommit != NULL))
					break;
			}
		}
		ret = (idx < cnt);
	}
	PERFDEBUG THRprintf(GDKout, "#BBPsync (move time %d) %d files\n", (t1 = GDKms()) - t0, backup_files);

	/* PHASE 2: save the repository */
	if (ret == 0) {
		int idx = 0;

		while (++idx < cnt) {
			bat i = subcommit ? subcommit[idx] : idx;

			if (BBP_status(i) & BBPPERSISTENT) {
				int unload = FALSE;
				BAT *b = dirty_bat(i, subcommit != NULL, &unload);

				if (b != NULL) {
					if (BATsave(b) == NULL)
						break;	/* write error */
					if (unload) {
						/* BATs that became persistent and have heaps that should in the 
						   future be STORE_PRIV (but are not yet), are unloaded */
						BATfree(b);
						BBPuncacheit_(i, TRUE);
					}
				}
			}
		}
		ret = (idx < cnt);
	}

	PERFDEBUG THRprintf(GDKout, "#BBPsync (write time %d)\n", (t0 = GDKms()) - t1);

	if (ret == 0) {
		if (bbpdirty) {
			ret = BBPdir(cnt, subcommit);
		} else if (backup_dir && GDKmove((backup_dir == 1) ? BAKDIR : SUBDIR, "BBP", "dir", BATDIR, "BBP", "dir")) {
			ret = -1;	/* tried a cheap way to get BBP.dir; but it failed */
		} else {
			/* commit might still fail; we must remember that we moved BBP.dir out of BAKDIR */
			backup_dir = 0;
		}
	}

	PERFDEBUG THRprintf(GDKout, "#BBPsync (dir time %d) %d bats\n", (t1 = GDKms()) - t0, BBPsize);

	if (bbpdirty || backup_files > 0) {
		if (ret == 0) {
			char *bakdir = subcommit ? SUBDIR : BAKDIR;

			/* atomic switchover */
			/* this is the big one: this call determines
			 * whether the operation of this function
			 * succeeded, so no changing of ret after this
			 * call anymore */
			ret = rename(bakdir, DELDIR);
			if (ret && GDKremovedir(DELDIR) == 0)	/* maybe there was an old deldir */
				ret = rename(bakdir, DELDIR);
			if (ret)
				GDKsyserror("BBPsync: rename(%s,%s) failed.\n", bakdir, DELDIR);
			IODEBUG THRprintf(GDKout, "#BBPsync: rename %s %s = %d\n", bakdir, DELDIR, ret);
		}

		/* AFTERMATH */
		if (ret == 0) {
			BBP_dirty = 0;
			backup_files = subcommit ? (backup_files - backup_subdir) : 0;
			backup_dir = backup_subdir = 0;
			(void) GDKremovedir(DELDIR);
			(void) BBPprepare(0);	/* (try to) remove DELDIR and set up new BAKDIR */
		}
	}
	PERFDEBUG THRprintf(GDKout, "#BBPsync (ready time %d)\n", (t0 = GDKms()) - t1);

	return ret;
}

@}
@-
Recovery just moves all files back to their original location. this is an incremental
process: if something fails, just stop with still files left for moving in BACKUP/. 
The recovery process can resume later with the left over files.
@{
@c
static int
force_move(str srcdir, str dstdir, str name)
{
	char *p;
	long_str srcpath, dstpath, killfile;
	int ret = 0;

	if ((p = strrchr(name, '.')) != NULL && strcmp(p, ".kill") == 0) {
		struct stat st;
		ptrdiff_t len = p - name;

		strncpy(srcpath, name, len);
		srcpath[len] = '\0';
		GDKfilepath(dstpath, dstdir, srcpath, NULL);

		/* step 1: remove the X file that is going to be overwritten by X.priv */
		if (stat(dstpath, &st) == 0) {
			ret = unlink(dstpath);	/* clear destination */
			if (ret) {
				/* if it exists and cannot be removed, all this is going to fail */
				GDKsyserror("force_move: unlink(%s)\n", dstpath);
				return ret;
			}
		}

		/* step 2: now remove the .kill file. This one is crucial, otherwise we'll never finish recovering */
		GDKfilepath(killfile, srcdir, name, NULL);
		ret = unlink(killfile);
		if (ret) {
			GDKsyserror("force_move: unlink(%s)\n", killfile);
			return ret;
		}

		/* step 3: move X.priv to X. Even if this fails, HEAPload will retry to do this later */
		if (GDKcreatedir(dstdir))
			ret = 0;
		ret = GDKmove(dstdir, srcpath, "priv", dstdir, srcpath, NULL);
		if (ret)
			GDKsyserror("force_move: link(%s%c%s.priv,%s)=%d\n", srcdir, DIR_SEP, srcpath, dstpath);

		IODEBUG THRprintf(GDKout, "#link %s%c%s.priv %s = %d\n", srcdir, DIR_SEP, srcpath, dstpath, ret);

		return 0;
	}
	/* try to rename it */
	ret = GDKmove(srcdir, name, NULL, dstdir, name, NULL);

	if (ret) {
		/* two legal possible causes: file exists or dir notexist */
		GDKfilepath(dstpath, dstdir, name, NULL);
		GDKfilepath(srcpath, srcdir, name, NULL);
		ret = unlink(dstpath);	/* clear destination */
		IODEBUG THRprintf(GDKout, "#unlink %s = %d\n", dstpath, ret);

		if (GDKcreatedir(dstdir))
			ret = 0;
		ret = GDKmove(srcdir, name, NULL, dstdir, name, NULL);
		if (ret)
			GDKsyserror("force_move: link(%s,%s)=%d\n", srcpath, dstpath, ret);
		IODEBUG THRprintf(GDKout, "#link %s %s = %d\n", srcpath, dstpath, ret);
	}
	return ret;
}

int
BBPrecover(void)
{
	DIR *dirp = opendir(BAKDIR);
	struct dirent *dent;
	long_str path, dstpath;
	bat i;
	size_t j = strlen(BATDIR);
	int ret = 0, dirseen = FALSE;
	str dstdir;

	if (dirp == NULL) {
		return 0;	/* nothing to do */
	}
	memcpy(dstpath, BATDIR, j);
	dstpath[j] = DIR_SEP;
	dstpath[++j] = 0;
	dstdir = dstpath + j;
	IODEBUG THRprintf(GDKout, "#BBPrecover(start)\n");

	mkdir(LEFTDIR, 0755);

	/* move back all files */
	while ((dent = readdir(dirp)) != NULL) {
		str q = strchr(dent->d_name, '.');

		if (q == dent->d_name) {
			int uret;

			if (strcmp(dent->d_name, ".") == 0 || strcmp(dent->d_name, "..") == 0)
				continue;
			GDKfilepath(path, BAKDIR, dent->d_name, NULL);
			uret = unlink(path);
			IODEBUG THRprintf(GDKout, "#unlink %s = %d\n", path, uret);

			continue;
		} else if (strcmp(dent->d_name, "BBP.dir") == 0) {
			dirseen = TRUE;
			continue;
		}
		if (q == NULL)
			q = dent->d_name + strlen(dent->d_name);
		if ((j = q - dent->d_name) + 1 > sizeof(path)) {
			/* name too long: ignore */
			continue;
		}
		strncpy(path, dent->d_name, j);
		path[j] = 0;
		if (GDKisdigit(*path)) {
			i = strtol(path, NULL, 8);
		} else {
			i = BBP_find(path, FALSE);
			if (i < 0)
				i = -i;
		}
		if (i == 0 || i >= BBPsize || !BBPvalid(i)) {
			force_move(BAKDIR, LEFTDIR, dent->d_name);
		} else {
			BBPgetsubdir(dstdir, i);
			ret += force_move(BAKDIR, dstpath, dent->d_name);
		}
	}
	closedir(dirp);
	if (dirseen && ret == 0) {	/* we have a saved BBP.dir; it should be moved back!! */
		struct stat st;

		GDKfilepath(path, BATDIR, "BBP", "dir");
		ret = recover_dir(stat(path, &st) == 0);
	}

	if (ret == 0) {
		ret = rmdir(BAKDIR);
		IODEBUG THRprintf(GDKout, "#rmdir %s = %d\n", BAKDIR, ret);
	}
	if (ret)
		GDKerror("BBPrecover: recovery failed. Please check whether your disk is full or write-protected.\n");

	IODEBUG THRprintf(GDKout, "#BBPrecover(end)\n");

	return ret;
}

@
SUBDIR recovery is quite mindlessly moving all files back to the parent (BAKDIR).
We do recognize moving back BBP.dir and set backed_up_subdir accordingly.
@c
int
BBPrecover_subdir(void)
{
	DIR *dirp = opendir(SUBDIR);
	struct dirent *dent;
	int ret = 0;

	if (dirp == NULL) {
		return 0;	/* nothing to do */
	}
	IODEBUG THRprintf(GDKout, "#BBPrecover_subdir(start)\n");

	/* move back all files */
	while ((dent = readdir(dirp)) != NULL) {
		if (dent->d_name[0] == '.')
			continue;
		ret = GDKmove(SUBDIR, dent->d_name, NULL, BAKDIR, dent->d_name, NULL);
		if (ret == 0 && strcmp(dent->d_name, "BBP.dir") == 0)
			backup_dir = 1;
		if (ret < 0)
			break;
	}
	/* delete the directory */
	if (ret == 0) {
		ret = GDKremovedir(SUBDIR);
		if (backup_dir == 2) {
			GDKwarning("BBPrecover_subdir: %s%cBBP.dir had disappeared!", SUBDIR, DIR_SEP);
			backup_dir = 0;
		}
	}
	IODEBUG THRprintf(GDKout, "#BBPrecover_subdir(end) = %d\n", ret);

	if (ret)
		GDKerror("BBPrecover_subdir: recovery failed. Please check whether your disk is full or write-protected.\n");
	return ret;
}

@}
@- The diskscan
The BBPdiskscan routine walks through the BAT dir, cleans up leftovers, and measures disk occupancy. 
Leftovers are files that cannot belong to a BAT. in order to establish this for [ht]heap files, 
the BAT descriptor is loaded in order to determine whether these files are still required. 

The routine gathers all bat sizes in a bat that contains bat-ids and bytesizes. The return value is
the number of bytes of space freed.
@{
@c
static int
persistent_bat(bat bid)
{
	if (bid >= 0 && bid < BBPsize && BBPvalid(bid)) {
		BAT *b = BBP_cache(bid);

		if (b == NULL || b->batCopiedtodisk) {
			return TRUE;
		}
	}
	return FALSE;
}

static BAT *
getdesc(int bid)
{
	BAT *b = (BAT *) BBPgetdesc(bid);

	if (b == NULL)
		BBPclear(bid);
	return b;
}

static lng
BBPdiskscan_r(str parent)
{
	str dir = parent ? parent : BATDIR;
	DIR *dirp = opendir(dir);
	struct dirent *dent;
	long_str fullname;
	str dst = fullname, src = dir;
	lng ret, tot = LL_CONSTANT(0);

	if (dirp == NULL) {
		return LL_CONSTANT(-1);	/* nothing to do */
	}
	while (*src)
		*dst++ = *src++;
	if (dst[-1] != DIR_SEP)
		*dst++ = DIR_SEP;

	while ((dent = readdir(dirp)) != NULL) {
		str p = strchr(dent->d_name, '.');
		bat bid = strtol(dent->d_name, NULL, 8);
		int r, ok = (p && bid), delete = FALSE;
		off_t filesize;
		struct stat st;

		if (dent->d_name[0] == '.')
			continue;	/* ignore .dot files and directories (. ..) */
		if (strncmp(dent->d_name, "BBP.", 4) == 0) {
			if (parent == NULL || strncmp(parent, BAKDIR, strlen(BAKDIR)) == 0 || strncmp(parent, SUBDIR, strlen(SUBDIR)) == 0)
				continue;
		}
		strcpy(dst, dent->d_name);
		if (p == NULL) {
			ret = BBPdiskscan_r(fullname);
			if (ret >= 0) {
				tot += ret;	/* it was a directory; add subtotal */
				continue;
			}
		}
		r = stat(fullname, &st);
		if (r) {
			GDKsyserror("BBPdiskscan: stat(%s)", fullname);
			continue;
		} else {
			/* record the real disk occupancy; not just the bytesize */
			filesize = st.st_size;
		}

		/* if X exists, then X.priv can always be deleted */
		r = (int) strlen(fullname);
		if (r > 5 && strcmp(fullname + r - 5, ".priv") == 0) {
			chr bak = fullname[r - 5];

			fullname[r - 5] = 0;	/* cut off the .priv bit */
			delete = (stat(fullname, &st) == 0);
			fullname[r - 5] = bak;
		}
		if (ok == FALSE || delete == TRUE || !persistent_bat(bid)) {
			delete = TRUE;
		} else if (strncmp(p + 1, "hheap", 5) == 0) {
			BAT *b = getdesc(bid);

			delete = (b == NULL || !b->hheap) || (b->batCopiedtodisk == 0);
		} else if (strncmp(p + 1, "theap", 5) == 0) {
			BAT *b = getdesc(bid);

			delete = (b == NULL || !b->theap) || (b->batCopiedtodisk == 0);
		} else if (strncmp(p + 1, "hhash", 5) == 0) {
			BAT *b = getdesc(bid);

			delete = (b == NULL || !b->H->hash);
		} else if (strncmp(p + 1, "thash", 5) == 0) {
			BAT *b = getdesc(bid);

			delete = (b == NULL || !b->T->hash);
		} else if (strcmp(p + 1, "desc") && 
			strncmp(p + 1, "head", 4) &&
			strncmp(p + 1, "tail", 4)) {
			ok = FALSE;
		}
		if (!ok) {
			/* found an unknown file; stop pruning in this subdir */
			GDKwarning("BBPdiskscan: unexpected file %s, leaving %s.\n", dent->d_name, dir);
			break;
		}
		if (delete) {
			r = unlink(fullname);
			if (r) {
				GDKsyserror("BBPdiskscan: unlink(%s)", fullname);
			} else {
				tot += filesize;
			}
			IODEBUG THRprintf(GDKout, "#BBPcleanup: unlink(%s) = %d\n", fullname, r);
		}
		IODEBUG THRprintf(GDKout, "#BBPdiskscan: stat(%s) = %d\n", fullname, r);
	}
	closedir(dirp);
	return tot;
}

lng
BBPdiskscan(void)
{
	return BBPdiskscan_r(NULL);
}

@}
void 
BBPatom_drop(int atom)
{
	int i;
	str nme = ATOMname(atom);
	int unknown = ATOMunknown_add(nme);

	BBPlock("BBPatom_drop");
	for (i = 0; i < BBPsize; i++) {
		if (BBPvalid(i)) {
			BATstore *b = BBP_desc(i);

			if (!b) continue;
			
			if (b->B.htype == atom)
				b->B.htype = unknown; 
			if (b->B.ttype == atom)
				b->B.ttype = unknown; 
		}
	}
	BBPunlock("BBPatom_drop");
}

void 
BBPatom_load(int atom)
{
	str nme;
	int i, unknown;

	BBPlock("BBPatom_load");
 	nme = ATOMname(atom);
 	unknown = ATOMunknown_find(nme);
	ATOMunknown_del(unknown);
	for (i = 0; i < BBPsize; i++) {
		if (BBPvalid(i)) {
			BATstore *b = BBP_desc(i);

			if (!b) continue;

			if (b->B.htype == unknown)
				b->B.htype = atom; 
			if (b->B.ttype == unknown)
				b->B.ttype = atom; 
		}
	}
	BBPunlock("BBPatom_load");
}
@}

