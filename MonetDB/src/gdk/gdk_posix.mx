@' The contents of this file are subject to the MonetDB Public License
@' Version 1.1 (the "License"); you may not use this file except in
@' compliance with the License. You may obtain a copy of the License at
@' http://monetdb.cwi.nl/Legal/MonetDBLicense-1.1.html
@'
@' Software distributed under the License is distributed on an "AS IS"
@' basis, WITHOUT WARRANTY OF ANY KIND, either express or implied. See the
@' License for the specific language governing rights and limitations
@' under the License.
@'
@' The Original Code is the MonetDB Database System.
@'
@' The Initial Developer of the Original Code is CWI.
@' Portions created by CWI are Copyright (C) 1997-2005 CWI.
@' All Rights Reserved.

@f gdk_posix
@a Niels Nes, Peter Boncz
@* System Independent Layer

GDK is built on Posix. Exceptions are made for memory mapped files and 
anonymous virtual memory, for which somewhat higher-level functions are 
defined here.
Most of this file concerns itself with emulation of Posix functionality on 
the WIN32 native platform.
@-
@{
@h
#ifndef GDK_POSIX_H
#define GDK_POSIX_H

#include "gdk_system.h"

#include <sys/types.h>

#ifdef HAVE_MALLOC_H
# include <malloc.h>		/* mallopt, mallinfo, and  malloc, free etc. */
#endif

#if TIME_WITH_SYS_TIME
# include <sys/time.h>
# include <time.h>
#else
# if HAVE_SYS_TIME_H
#  include <sys/time.h>
# else
#  include <time.h>
# endif
#endif

#if defined(HAVE_WINSOCK_H) && defined(NATIVE_WIN32)
#include <winsock.h>		/* for timeval */
#endif

#ifdef NATIVE_WIN32
#include <io.h>
#include <direct.h>
#endif

/* Some systems (SGI, Sun) call malloc before we get a chance to call
   mallopt, and mallopt should be called before the first call to
   malloc.  Therefore we do as if we don't have mallopt, even though
   in reality we do.
 */
#ifdef HAVE_MALLOPT
#undef HAVE_MALLOPT
#endif

#ifndef HAVE_MALLINFO
#ifndef M_MXFAST
#define M_MXFAST	1	/* set size of blocks to be fast */
#endif
#ifndef M_NLBLKS
#define M_NLBLKS	2	/* set number of block in a holding block */
#endif
#ifndef M_GRAIN
#define M_GRAIN		3	/* set number of sizes mapped to one, for */
			   /* small blocks */
#endif
#ifndef M_KEEP
#define M_KEEP		4	/* retain contents of block after a free */
			   /* until another allocation */
#endif
#ifndef HAVE_STRUCT_MALLINFO
struct mallinfo {
	int arena;		/* total space in arena */
	int ordblks;		/* number of ordinary blocks */
	int smblks;		/* number of small blocks */
	int hblks;		/* number of holding blocks */
	int hblkhd;		/* space in holding block headers */
	int usmblks;		/* space in small blocks in use */
	int fsmblks;		/* space in free small blocks */
	int uordblks;		/* space in ordinary blocks in use */
	int fordblks;		/* space in free ordinary blocks */
	int keepcost;		/* cost of enabling keep option */
};
#endif

#define mallinfo() 		{0}
#define mallopt(cmd,value)	0

#endif /* ! HAVE_MALLINFO */

gdk_export struct mallinfo MT_mallinfo(void);

@- locking, sleep
@h
#define F_TLOCK 2		/* test and lock a region for exclusive use */
#define F_ULOCK 0		/* unlock a previously locked region */
#define F_LOCK 1		/* lock a region for exclusive use */

gdk_export int MT_lockf(char *filename, int mode, off_t off, off_t len);
gdk_export void MT_sleep_ms(unsigned int ms);

@- virtual memory
@h
#define MT_VMUNITLOG 	16
#define MT_VMUNITSIZE 	(1 << MT_VMUNITLOG)

#ifdef DEBUG_ALLOC
gdk_export int MT_alloc_register(void *p, size_t size, char mode);
gdk_export int MT_alloc_print(void);
gdk_export int MT_alloc_table(void);
#else
#define MT_alloc_register(p, size, mode) (void)p; (void)size; (void)mode
#define MT_alloc_print()
#define MT_alloc_table()
#endif 

/* make sure POSIX_MADV_* and posix_madvise() are defined somehow */
#ifdef HAVE_SYS_MMAN_H
# include <sys/mman.h>
#endif
#ifndef HAVE_POSIX_MADVISE
# ifdef HAVE_MADVISE
#  define posix_madvise madvise
#  ifndef MADV_RANDOM
#   define MADV_RANDOM	0
#  endif
#  ifndef POSIX_MADV_NORMAL
#   define POSIX_MADV_NORMAL     MADV_NORMAL
#   define POSIX_MADV_RANDOM     MADV_RANDOM
#   define POSIX_MADV_SEQUENTIAL MADV_SEQUENTIAL
#   define POSIX_MADV_WILLNEED   MADV_WILLNEED
#   define POSIX_MADV_DONTNEED   MADV_DONTNEED
#  endif
# else
#  define posix_madvise(x,y,z)	0
#  ifndef POSIX_MADV_NORMAL
#   define POSIX_MADV_NORMAL     0
#   define POSIX_MADV_RANDOM     0
#   define POSIX_MADV_SEQUENTIAL 0
#   define POSIX_MADV_WILLNEED   0
#   define POSIX_MADV_DONTNEED   0
#  endif
# endif
#endif

/* in case they are still not defined, define these values as
   something that doesn't do anything */
#ifndef POSIX_MADV_NORMAL
#define POSIX_MADV_NORMAL 0
#endif
#ifndef POSIX_MADV_RANDOM
#define POSIX_MADV_RANDOM 0
#endif
#ifndef POSIX_MADV_SEQUENTIAL
#define POSIX_MADV_SEQUENTIAL 0
#endif
#ifndef POSIX_MADV_WILLNEED
#define POSIX_MADV_WILLNEED 0
#endif
#ifndef POSIX_MADV_DONTNEED
#define POSIX_MADV_DONTNEED 0
#endif

/* the new mmap modes, mimic default MADV_* madvise POSIX constants */
#define MMAP_NORMAL     	POSIX_MADV_NORMAL	/* no further special treatment */
#define MMAP_RANDOM     	POSIX_MADV_RANDOM	/* expect random page references */
#define MMAP_SEQUENTIAL 	POSIX_MADV_SEQUENTIAL	/* expect sequential page references */
#define MMAP_WILLNEED   	POSIX_MADV_WILLNEED	/* will need these pages */
#define MMAP_DONTNEED   	POSIX_MADV_DONTNEED	/* don't need these pages */

#define MMAP_READ		1024	/* region is readable (default if ommitted) */
#define MMAP_WRITE		2048	/* region may be written into */
#define MMAP_COPY		4096	/* writable, but changes never reach file */
#define MMAP_ASYNC		8192	/* asynchronous writes (default if ommitted) */
#define MMAP_SYNC		16384	/* writing is done synchronously */

/* in order to be sure of madvise and msync modes, pass them to mmap() call as well */

/* a hook function to add any initialization required for the MT_ functionality */
gdk_export char *MT_heapbase;
gdk_export char *MT_heapcur(void);

gdk_export void MT_init_posix(int alloc_map);
gdk_export size_t MT_getrss();

gdk_export void *MT_mmap(char *path, int mode, off_t off, size_t len, void *fixed);
gdk_export int MT_munmap(void *p, size_t len);
gdk_export int MT_msync(void *p, size_t len, int mode);
gdk_export int MT_madvise(void *p, size_t len, int advise);

gdk_export int MT_mmap_trim(size_t lim, void *err);
gdk_export void MT_mmap_pin(void *p, size_t len);
gdk_export void MT_mmap_unpin(void *p, size_t len);

gdk_export void *MT_vmalloc(size_t size, size_t * maxsize);
gdk_export void MT_vmfree(void *p, size_t size);
gdk_export void *MT_vmrealloc(void *voidptr, size_t oldsize, size_t newsize, size_t oldmaxsize, size_t * newmaxsize);
gdk_export int MT_path_absolute(char *path);

@}

@+ Posix under WIN32 
WIN32 actually supports many Posix functions directly.  Some it does not, though. 
For some functionality we move in Monet from Posix calls to MT_*() calls, which translate easier 
to WIN32.  Examples are MT_mmap() , MT_sleep_ms() and MT_path_absolute(). Why? In the case
of mmap() it is much easier for WIN32 to get a filename parameter rather than a file-descriptor. 
That is the reason in the case of mmap() to go for a MT_mmap() solution.

For some other functionality, we do not need to abandon the Posix interface, though. Two cases can be distinguished.
Missing functions in WIN32 are directly implemented (e.g. dlopen()/dlsym()/dlclose()).
Posix functions in WIN32 whose functionality should be changed a bit. Examples are 
stat()/rename()/mkdir()/rmdir() who under WIN32 do not work if the path ends with a directory 
separator, but should work according to Posix. We remap such functions using a define
to an equivalent win_*() function (which in its implementation calls through to the WIN32 function).
@{
@h
#ifdef NATIVE_WIN32

#define RTLD_LAZY	1
#define RTLD_NOW	2
#define RTLD_GLOBAL	4

gdk_export void *dlopen(const char *file, int mode);
gdk_export int dlclose(void *handle);
gdk_export void *dlsym(void *handle, const char *name);
gdk_export char *dlerror(void);
gdk_export int gettimeofday(struct timeval *tv, int *ignore_zone);
gdk_export int win_stat(const char *, struct stat *);
gdk_export int win_rmdir(const char *);
gdk_export int win_mkdir(const char *, const int mode);

#define stat(x,y)	win_stat(x,y)
#define mkdir		win_mkdir
#define rmdir		win_rmdir

#define NAME_MAX 255

#ifndef HAVE_OPENDIR
struct DIR {
	char *dir_name;
	int just_opened;
	unsigned int find_file_handle;
	char *find_file_data;
};

typedef struct DIR DIR;
struct direct {
	char d_name[NAME_MAX + 1];
	int d_namelen;
};
#endif

#ifndef HAVE_FTRUNCATE
gdk_export int ftruncate(int fd, off_t size);
#endif
#ifndef HAVE_OPENDIR
gdk_export DIR *opendir(const char *dirname);
gdk_export struct direct *readdir(DIR *dir);
gdk_export void rewinddir(DIR *dir);
gdk_export int closedir(DIR *dir);
#endif

#endif

#endif /* GDK_POSIX_H */
@c
#include "gdk.h"

#include <stdio.h>

#ifdef HAVE_FCNTL_H
# include <fcntl.h>
#endif


#ifndef MAP_NORESERVE
# define MAP_NORESERVE 		MAP_PRIVATE
#endif

#define MMAP_ADVISE		7
#define MMAP_WRITABLE		(MMAP_WRITE|MMAP_COPY)

/* DDALERT: AIX4.X 64bits needs HAVE_SETENV==0 due to a AIX bug, but it probably isn't detected so by configure */

#ifndef HAVE_SETENV
int
setenv(const char *name, const char *value, int overwrite)
{
	int ret = 0;

	if (overwrite || getenv(name) == NULL) {
		char *p = (char *) GDKmalloc(2 + strlen(name) + strlen(value));

		strcpy(p, name);
		strcat(p, "=");
		strcat(p, value);
		ret = putenv(p);
		/* GDKfree(p); LEAK INSERTED DUE TO SOME WEIRD CRASHES */
	}
	return ret;
}
#endif

char *MT_heapbase = NULL;


/* Crude VM buffer management that keep a list of all memory mapped regions.
 *
 * a.k.a. "helping stupid VM implementations that ignore VM advise"
 *
 * The main goal is to be able to tell the OS to please stop buffering all memory
 * mapped pages when under pressure. A major problem is materialization of large
 * results newly created memory mapped files. Operating systems tend to cache
 * all dirty pages, such that when memory is out, all pages are dirty and cannot
 * be unloaded quickly. The VM panic occurs and comatose OS states may be observed.
 * This is in spite of our use of madvise(MADV_SEQUENTIAL). That is; we would want 
 * that the OS drops pages after we've passed them. That does not happen; pages are 
 * retained and pollute the buffer cache.
 *
 * Regrettably, at this level, we don't know anything about how Monet is using the 
 * mmapped regions. Monet code is totally oblivious of any I/O; that's why it is 
 * so easy to create CPU efficient code in Monet.
 *
 * The current solution focuses on large writable maps. These often represent
 * newly created BATs, that are the result of some (running) operator. We 
 * assume two things here:
 * - the BAT is created in sequential fashion (always almost true)
 * - afterwards, this BAT is used in sequential fashion (often true)
 *
 * A VMtrim thread keeps an eye on the RSS (memory pressure) and large writable
 * memory maps. If RSS approaches mem_maxsize(), it starts to *worry*, and starts
 * to write dirty data from these writable maps to disk in 128MB tiles. So, if 
 * memory pressure rises further in the near future, the OS has some optiont to release 
 * memory pages cheaply (i.e. without needing I/O). This is also done explicitly by the 
 * VM-thread: when RSS exceeds mem_maxsize() is explicitly asks the OS to release pages.
 * The reason is that Linux is not smart enough to do even this. Anyway..
 *
 * The way to free pages explicitly in Linux is to call posix_fadvise(..,MADV_DONTNEED). 
 * Particularly, posix_madvise(..,POSIX_MADV_DONTNEED) which is supported and documented 
 * doesn't work on Linux. But we do both posix_madvise and posix_fadvise, so on other unix
 * systems that don't support posix_fadvise, posix_madvise still might work.
 * On Windows, to our knowledge, there is no way to tell it stop buffering
 * a memory mapped region. msync (FlushViewOfFile) does work, though. So let's
 * hope the VM paging algorithm behaves better than Linux which just runs off
 * the cliff and if MonetDB does not prevent RSS from being too high, enters coma.
 *
 * We will only eb able to sensibly test this on Windows64. On Windows32, mmap sizes
 * do not significantly exceed RAM sizes so MonetDB swapping actually will not happen
 * (of course, you've got this nasty problem of VM fragemntation and failing mmaps instead).
 *
 * In principle, page tiles are saved sequentially, and behind it, but never overtaking
 * it, is an "unload-cursor" that frees the pages if that is needed to keep RSS down.
 * There is a tweak in the algorithm, that re-sets the unload-cursor if it seems
 * that all tiles to the end have been saved (whether a tile is actually saved is
 * determined by timing the sync action). This means that the producing operator
 * is ready creating the BAT, and we assume it is going to be used sequentially afterwards.
 * In that case, we should start unloading right after the 'read-cursor', that is, 
 * from the start.
 *
 * EXAMPLE
 * D = dirty tile
 * s = saved tile (i.e. clean)
 * u = unloaded tile
 * L = tile that is being loaded
 *
 *           +--> operator produces  BAT 
 * (1) DDDDDD|......................................| end of reserved mmap
 *                      ____|RSS
 *                     |
 *                     | at 3/4 of RSS consumed we start to worry
 *                     +--> operator produces BAT 
 * (2) DDDDDDDDDDDDDDDD|............................|
 *                    s<----------------------------- VM backwards save thread 
 *                    |
 *                    + first tile of which saving costs anything
 *
 *                        +--> operator produces BAT 
 * (3) DDDDDDDDDDDDDDDss|D|.........................|
 *     VM-thread save ->| 
 *
 * When the RSS target is exceeded, we start unloading tiles..
 *
 *                     +-->  VM-thread unload starts at *second* 's'
 *                     |
 *                     |    +--> operator produces BAT 
 * (4) DDDDDDDDDDDDDDDsus|DD|........................|
 *     VM-thread save -->|  | RSS = Full!
 *                    
 *                                  +-- 0 => save costs nothing!!
 *     VM-thread save ------------->|        assume bat complete 
 * (5) DDDDDDDDDDDDDDDsuuuuuuuuussss0................|
 *                    |<-------- re-set unload cursor
 *                    +--- first tile was not unloaded. 
 *                     
 * later.. some other operator sequentially reads the bat
 * first part is 'D', that is, nicely cached.
 *
 *     ---read------->|
 * (6) DDDDDDDDDDDDDDDsuuuuuuuuussss0................|
 *
 * now we're hitting the unloaded region. the query becomes
 * I/O read bound here (typically 20% CPU utilization).
 *
 *     ---read-------->|
 * (7) DDDDDDDDDDDDDDDuLuuuuuuuussss0................|
 *                   /  \
 *      unload cursor    load cursor
 *
 *     ---read---------------->|
 * (8) DDDDDDDDDDDDDDDuuuuuuuuuLssss0................|
 *                           /  \
 *              unload cursor    load cursor
 *
 *     ---read--------------------->| done
 * (9) DDDDDDDDDDDDDDDuuuuuuuuuLssss0................|
 *                              ****
 *                              last part still cached 
 *
 * note: if we would not have re-setted the unload cursor (5)
 *       the last part would have been lost due to continuing
 *       RSS pressure from the 'L' read-cursor.
 *
 * If multiple write-mmaps exist, we do unload-tile and save-tile
 * selection on a round-robin basis among them.
 *
 * Of course, this is a simple solution for simple cases only.
 * (a) if the bat is produced is too fast, (or your disk is too slow)
 *     we will RSS will exceeds its limit and Linux will go into swapping. 
 * (b) if your data is not produced and read sequentially.
 *     Examples are sorting or clustering on huge datasets.
 * (c) if RSS pressure is due to large read-maps, rather than 
 *     intermediate results. 
 *
 * Two crude suggestions:
 * - If we are under RSS pressure without unloadable tiles and with 
 *   savable tiles, we should consider suspending *all* other threads 
 *   until we manage to unload a tile.
 * - if there are no savable tiles (or in case of read-only maps)
 *   we could resort to saving and unloading random tiles.
 * 
 * To do better, our BAT algorithms should provide even more detailed
 * advise on their access patterns, which may even consist of pointers
 * to the cursors (i.e. pointers to b->batBuns->free or the cursors
 * in radix-cluster), which an enhanced version of this thread might
 * take into account.
 */

#ifdef HAVE_PTHREAD_H
/* pthread.h on Windows includes config.h if HAVE_CONFIG_H is set */
#undef HAVE_CONFIG_H
#include <pthread.h>
#endif
#ifdef HAVE_SEMAPHORE_H
#include <semaphore.h>
#endif

typedef struct {
	char path[128];		/* mapped file, retained for debugging */
	char *base;		/* base address */
	size_t len;		/* length of map */
	size_t first_tile;	/* from here we started saving tiles */
	size_t save_tile;	/* next tile to save */
	size_t unload_tile;	/* next tile to unload */
	int last_tile;			
	int fd;			/* open fd (==-1 for anon vm), retained to give posix_fadvise */
	int pincnt;		/* incremented while a MIL command uses heap with a random pattern */
	int writable;
	int next;
} MT_mmap_t;

#ifdef HAVE_POSIX_FADVISE
static int do_not_use_posix_fadvise = 0;
#endif

#define MT_MMAP_TILE (1<<27)
#define MT_MMAP_BUFSIZE 100
MT_mmap_t MT_mmap_tab[MT_MMAP_BUFSIZE];
int MT_mmap_cur = -1, MT_mmap_first = -1, MT_mmap_free = 0;

pthread_mutex_t MT_mmap_lock;

static void
MT_mmap_empty(int i)
{
	MT_mmap_tab[i].path[0] = 0;
	MT_mmap_tab[i].base = NULL;
	MT_mmap_tab[i].len = 0;
	MT_mmap_tab[i].writable = 0;
	MT_mmap_tab[i].fd = -1;
	MT_mmap_tab[i].pincnt = -1;
}

static void
MT_mmap_init()
{
	int i;

	/* create lock */
	pthread_mutex_init(&MT_mmap_lock, 0);

	for (i = 0; i < MT_MMAP_BUFSIZE - 1; i++) {
		MT_mmap_tab[i].next = i + 1;
		MT_mmap_empty(i);
	}
	MT_mmap_tab[i].next = -1;
}

/* returns previous element (to facilitate deletion) */
static int
MT_mmap_find(void *base)
{
	/* maybe consider a hash table iso linked list?? */
	int i, prev = MT_MMAP_BUFSIZE;

	for (i = MT_mmap_first; i >= 0; i = MT_mmap_tab[i].next) {
		if (MT_mmap_tab[i].base == base) {
			return prev;
		}
		prev = i;
	}
	return i;
}

static int
MT_mmap_idx(void *base, size_t len)
{
	if (len > MT_MMAP_TILE) {
		int i = MT_mmap_find(base);

		if (i >= 0) {
			if (i == MT_MMAP_BUFSIZE) {
				return MT_mmap_first;
			} else {
				return MT_mmap_tab[i].next;
			}
		}
	}
	return -1;
}

static int
MT_mmap_new(char *path, void *base, size_t len, int fd, int writable)
{
	(void) pthread_mutex_lock(&MT_mmap_lock);
	if (len > MT_MMAP_TILE && MT_mmap_free >= 0) {
		int i = MT_mmap_free;

		MT_mmap_free = MT_mmap_tab[i].next;
		MT_mmap_tab[i].next = MT_mmap_first;
		MT_mmap_first = i;
		if (MT_mmap_cur == -1) MT_mmap_cur = i;
#ifdef MMAP_DEBUG
		stream_printf(GDKerr, "MT_mmap_new: %s fd=%d\n", path, fd);
#endif
		strncpy(MT_mmap_tab[i].path, path, 128);
		MT_mmap_tab[i].base = base;
		MT_mmap_tab[i].len = len;
		MT_mmap_tab[i].save_tile = 1;
		MT_mmap_tab[i].last_tile = 0;
		MT_mmap_tab[i].first_tile = 0;
		MT_mmap_tab[i].unload_tile = 0;
		MT_mmap_tab[i].writable = writable;
		MT_mmap_tab[i].fd = fd;
		MT_mmap_tab[i].pincnt = 0;
		fd = -1;
	}
	(void) pthread_mutex_unlock(&MT_mmap_lock);
	return fd;
}

static void
MT_mmap_del(void *base, size_t len)
{
	if (len > MT_MMAP_TILE) {
		int victim = 0, prev;

		(void) pthread_mutex_lock(&MT_mmap_lock);
		prev = MT_mmap_find(base);
		if (prev >= 0) {
			int ret;

			if (prev == MT_MMAP_BUFSIZE) {
				victim = MT_mmap_first;
				MT_mmap_first = MT_mmap_tab[MT_mmap_first].next;
			} else if (prev) {
				victim = MT_mmap_tab[prev].next;
				MT_mmap_tab[prev].next = MT_mmap_tab[victim].next;
			}
			if (MT_mmap_cur == victim) {
				MT_mmap_cur = MT_mmap_first;
			}
#ifdef HAVE_POSIX_FADVISE
			if (!do_not_use_posix_fadvise &&
			    MT_mmap_tab[victim].fd >= 0) {
				/* tell the OS quite clearly that you want to drop this */
				ret = posix_fadvise(MT_mmap_tab[victim].fd, 0LL, MT_mmap_tab[victim].len, POSIX_FADV_DONTNEED);
#ifdef MMAP_DEBUG
				stream_printf(GDKerr, "MT_mmap_del: posix_fadvise(%s,fd=%d,%uMB,POSIX_FADV_DONTNEED) = %d\n", MT_mmap_tab[victim].path, MT_mmap_tab[victim].fd, (unsigned int) (MT_mmap_tab[victim].len >> 20), ret);
#endif
			}
#endif
			ret = close(MT_mmap_tab[victim].fd);
#ifdef MMAP_DEBUG
			stream_printf(GDKerr, "MT_mmap_del: close(%s fd=%d) = %d\n", MT_mmap_tab[victim].path, MT_mmap_tab[victim].fd, ret);
#endif

			MT_mmap_tab[victim].next = MT_mmap_free;
			MT_mmap_empty(victim);
			MT_mmap_free = victim;
			(void) ret;
		}
		(void) pthread_mutex_unlock(&MT_mmap_lock);
	}
}

static int
MT_fadvise(void *base, size_t len, int advice)
{
	int ret = 0;

#ifdef HAVE_POSIX_FADVISE
	if (!do_not_use_posix_fadvise) {
		int i;

		(void) pthread_mutex_lock(&MT_mmap_lock);
		i = MT_mmap_idx(base, len);
		if (i >= 0) {
			if (MT_mmap_tab[i].fd >= 0) {
				ret = posix_fadvise(MT_mmap_tab[i].fd, 0, len, advice);
#ifdef MMAP_DEBUG
				stream_printf(GDKerr, "MT_fadvise: posix_fadvise(%s,fd=%d,%uMB,%d) = %d\n", MT_mmap_tab[i].path, MT_mmap_tab[i].fd, (unsigned int) (len >> 20), advice, ret);
#endif
			}
		}
		(void) pthread_mutex_unlock(&MT_mmap_lock);
	}
#else
	(void) base;
	(void) len;
	(void) advice;
#endif
	return ret;
}


static void
MT_mmap_unload_tile(int i, size_t off, stream* err) {
	/* tell Linux to please stop caching this stuff */
	int ret = posix_madvise(MT_mmap_tab[i].base+off, MT_MMAP_TILE, POSIX_MADV_DONTNEED);
	if (err) {
		stream_printf(err, "MT_mmap_unload_tile: posix_madvise(%s,off=%uMB,%uMB,fd=%d,POSIX_MADV_DONTNEED) = %d\n", 
			MT_mmap_tab[i].path, (unsigned int) (off>>20), (unsigned int) (MT_MMAP_TILE>>20), MT_mmap_tab[i].fd, ret);
	}
#ifdef HAVE_POSIX_FADVISE
	if (!do_not_use_posix_fadvise) {
		/* tell the OS quite clearly that you want to drop this */
		ret = posix_fadvise(MT_mmap_tab[i].fd, off, MT_MMAP_TILE, POSIX_FADV_DONTNEED);
		if (err) {
			stream_printf(err, "MT_mmap_unload_tile: posix_fadvise(%s,off=%uMB,%uMB,fd=%d,POSIX_MADV_DONTNEED) = %d\n", 
				MT_mmap_tab[i].path, (unsigned int) (off>>20), (unsigned int) (MT_MMAP_TILE>>20), MT_mmap_tab[i].fd, ret);
		}
	}
#endif
}

static int
MT_mmap_save_tile(int i, size_t tile, stream* err) {
	int t, ret;

	/* save to disk an 128MB tile, and observe how long this takes */
	if (err) {
		stream_printf(err, "MT_mmap_save_tile: msync(%s,off=%uM,%u,SYNC)...\n", 
			MT_mmap_tab[i].path, (unsigned int) (tile>>20), (unsigned int) (MT_MMAP_TILE>>20));
	}
 	t = GDKms();
	ret = MT_msync(MT_mmap_tab[i].base+tile, MT_MMAP_TILE, MMAP_SYNC);
	t = GDKms() - t;
	if (err) {
		stream_printf(err, "MT_mmap_save_tile: msync(%s,tile=%uM,%uM,SYNC) = %d (%dms)\n", 
			MT_mmap_tab[i].path, (unsigned int) (tile>>20), (unsigned int) (MT_MMAP_TILE>>20), ret, t);
	}
	if (t > 200) {
		/* this took time; so we should report back on our actions and await new orders */
		if (MT_mmap_tab[i].save_tile == 1) {
			MT_mmap_tab[i].first_tile = tile;
			/* leave first tile for later sequential use pass (start unloading after it) */
			MT_mmap_tab[i].unload_tile = tile + MT_MMAP_TILE; 
		} 
		MT_mmap_tab[i].save_tile = tile + MT_MMAP_TILE; 
		(void) pthread_mutex_unlock(&MT_mmap_lock);
		return 1; 
	}
	return 0;
}

/* round-robin next. this is to ensure some fairness if multiple large results are produced simultaneously */
int MT_mmap_next(int i) {
	if (i != -1) { 
		i = MT_mmap_tab[i].next;
		if (i == -1) i = MT_mmap_first;
	}
	return i;
}

int
MT_mmap_trim(size_t target, void *fp)
{
	stream *err = (stream *) fp;
	size_t off, rss = MT_getrss();
	int i, worry = (rss*4 > target*3);

	(void) pthread_mutex_lock(&MT_mmap_lock);
	if (err) {
		stream_printf(err, "MT_mmap_trim(%u MB): rss = %u MB\n", (unsigned int) ((target) >> 20), (unsigned int) (rss >> 20));
	}
	if (rss > target) {				
		/* try to selectively unload pages from the writable regions */
		size_t delta = ((rss - target) + 4*MT_MMAP_TILE - 1) & ~(MT_MMAP_TILE-1);
		for(i = MT_mmap_next(MT_mmap_cur); delta && i != MT_mmap_cur; i = MT_mmap_next(i)) {
			if (MT_mmap_tab[i].fd >= 0 && MT_mmap_tab[i].writable) {
				if (MT_mmap_tab[i].unload_tile >= MT_mmap_tab[i].save_tile)
					MT_mmap_tab[i].unload_tile = MT_mmap_tab[i].first_tile;
				while(MT_mmap_tab[i].unload_tile < MT_mmap_tab[i].save_tile) {
					MT_mmap_unload_tile(i, MT_mmap_tab[i].unload_tile, err);
					MT_mmap_tab[i].unload_tile += MT_MMAP_TILE;
					if ((delta -= MT_MMAP_TILE) == 0) break;
				}
			}
		}
	} 
	if (worry) {
		/* schedule background saves of tiles */
		for(i = MT_mmap_next(MT_mmap_cur); i != MT_mmap_cur; i = MT_mmap_next(i)) {
			if (MT_mmap_tab[i].fd >= 0 && MT_mmap_tab[i].writable && 
			   (MT_mmap_tab[i].pincnt == 0 || MT_mmap_tab[i].len > target)) 
			{
				if (MT_mmap_tab[i].save_tile == 1) {
					/* first run, walk backwards until we hit an unsaved tile */
					for(off=MT_mmap_tab[i].len; off>=MT_MMAP_TILE; off-=MT_MMAP_TILE)
						if (MT_mmap_save_tile(i, off, err)) return 1;
				} else { 
					/* save the next tile */
					for(off=MT_mmap_tab[i].save_tile; off+MT_MMAP_TILE<MT_mmap_tab[i].len; off+=MT_MMAP_TILE) {
						if (MT_mmap_save_tile(i, off, err)) return 1;
					}
					/* we seem to have run through all savable tiles */
					if (!MT_mmap_tab[i].last_tile) {
						MT_mmap_tab[i].last_tile = 1;
						MT_mmap_tab[i].unload_tile = MT_mmap_tab[i].first_tile;
					}
				}
			}
		} 
		MT_mmap_cur = i;
	}
	(void) pthread_mutex_unlock(&MT_mmap_lock);
	return (worry);
}

void
MT_mmap_pin(void *base, size_t len)
{
	int i;

	(void) pthread_mutex_lock(&MT_mmap_lock);
	i = MT_mmap_idx(base, len);
	if (i >= 0) {
		MT_mmap_tab[i].pincnt++;
	}
	(void) pthread_mutex_unlock(&MT_mmap_lock);

}

void
MT_mmap_unpin(void *base, size_t len)
{
	int i;

	(void) pthread_mutex_lock(&MT_mmap_lock);
	i = MT_mmap_idx(base, len);
	if (i >= 0) {
		MT_mmap_tab[i].pincnt--;
	}
	(void) pthread_mutex_unlock(&MT_mmap_lock);
}

#ifndef NATIVE_WIN32
#ifdef HAVE_POSIX_FADVISE
#ifdef HAVE_UNAME
#include <sys/utsname.h>
#endif
#endif

void
MT_init_posix(int alloc_map)
{
#ifdef HAVE_POSIX_FADVISE
#ifdef HAVE_UNAME
	struct utsname ubuf;

	/* do not use posix_fadvise on Linux systems running a 2.4 or
	   older kernel */
	do_not_use_posix_fadvise = uname(&ubuf) == 0 &&
		strcmp(ubuf.sysname, "Linux") == 0 &&
		strncmp(ubuf.release, "2.4", 3) <= 0;
#endif
#endif
	MT_heapbase = (char *) sbrk(0);

#ifdef DEBUG_ALLOC
	static void MT_alloc_init(void);

	if (alloc_map)
		MT_alloc_init();
#else
	(void)alloc_map;
#endif
	MT_mmap_init();
}

size_t
MT_getrss()
{
	/* get RSS  -- linux only for the moment */
	static char MT_mmap_procfile[128] = { 0 };
	int fd;

	if (MT_mmap_procfile[0] == 0) {
		sprintf(MT_mmap_procfile, "/proc/%d/stat", getpid());
	}
	fd = open(MT_mmap_procfile, O_RDONLY);
	if (fd >= 0) {
		char buf[1024], *r = buf;
		size_t i, sz = read(fd, buf, 1024);

		close(fd);
		if (sz > 0) {
			for (i = 0; i < 23; i++) {
				while (*r && (*r == ' ' || *r == '\t'))
					r++;
				while (*r && (*r != ' ' && *r != '\t'))
					r++;
			}
			while (*r && (*r == ' ' || *r == '\t'))
				r++;
			return ((size_t) atol(r)) * MT_pagesize();
		}
	}
	return 0;
}


char *
MT_heapcur(void)
{
	return (char *) sbrk(0);
}

void *
MT_mmap(char *path, int mode, off_t off, size_t len, void *fixed)
{
	int fd = open(path, O_CREAT | ((mode & MMAP_WRITABLE) ? O_RDWR : O_RDONLY));
	void *ret = (void *) -1L;

	if (fd >= 0) {
		ret = mmap(fixed, len, ((mode & MMAP_WRITABLE) ? PROT_WRITE : 0) | PROT_READ, ((mode & MMAP_COPY) ? MAP_PRIVATE : MAP_SHARED) | (fixed ? MAP_FIXED : 0), fd, off);

		if (ret != (void *) -1L) {
			fd = MT_mmap_new(path, ret, len, fd, (mode & MMAP_WRITABLE));
			if (mode & MMAP_ADVISE) {
				(void) MT_madvise(ret, len, mode & MMAP_ADVISE);
			}
		}
		if (fd >= 0)
			close(fd);
	}
	return ret;
}

int
MT_munmap(void *p, size_t len)
{
	int ret = munmap(p, len);

#ifdef MMAP_DEBUG
	stream_printf(GDKerr, "munmap(" LLFMT "," LLFMT ",%d) = %d\n", (long long) p, (long long) len, ret);
#endif
	MT_mmap_del(p, len);
	return ret;
}

int
MT_msync(void *p, size_t len, int mode)
{
	int ret = msync(p, len, (mode & MMAP_SYNC) ? MS_SYNC : ((mode & MMAP_ASYNC) ? MS_ASYNC : MS_INVALIDATE));

#ifdef MMAP_DEBUG
	stream_printf(GDKerr, "msync(" LLFMT "," LLFMT ",%s) = %d\n", (long long) p, (long long) len, (mode & MMAP_SYNC) ? "MS_SYNC" : ((mode & MMAP_ASYNC) ? "MS_ASYNC" : "MS_INVALIDATE"), ret);
#endif
	if (ret < 0)
		return errno;
	return ret;
}

int
MT_madvise(void *p, size_t len, int advise)
{
	int ret = posix_madvise(p, len, advise);

#ifdef MMAP_DEBUG
	stream_printf(GDKerr, "posix_madvise(" LLFMT "," LLFMT ",%d) = %d\n", (long long) p, (long long) len, advise, ret);
#endif
	if (MT_fadvise(p, len, advise))
		ret = -1;
	return ret;
}

struct mallinfo
MT_mallinfo(void)
{
	struct mallinfo _ret;

#ifdef HAVE_MALLINFO
	_ret = mallinfo();
#else
	memset(&_ret, 0, sizeof(_ret));
#endif
	if (_ret.uordblks + _ret.fordblks > _ret.arena) {
		MT_alloc_register(MT_heapbase, _ret.arena, 'H');
	}
	return _ret;
}

int
MT_path_absolute(char *pathname)
{
	return (*pathname == DIR_SEP);
}

#ifdef WIN32
#include <windows.h>
#endif

#else /* WIN32 native */

#ifndef BUFSIZ
#define BUFSIZ 1024
#endif

#undef _errno
#undef stat
#undef rmdir
#undef mkdir

#undef NAME_MAX

#include <windows.h>

#ifdef _MSC_VER
#include <io.h>
#endif /* _MSC_VER */

#define MT_SMALLBLOCK 256

void
MT_init_posix(int alloc_map)
{
	MT_heapbase = 0;
/*
	_set_sbh_threshold(MT_SMALLBLOCK);
*/
#ifdef DEBUG_ALLOC
	static void MT_alloc_init(void);

	if (alloc_map)
		MT_alloc_init();
#else
	(void)alloc_map;
#endif
	MT_mmap_init();
}

size_t
MT_getrss()
{
	MEMORYSTATUSEX statex;
	GlobalMemoryStatusEx (&statex);
	return statex.dwMemoryLoad;
}

char *
MT_heapcur(void)
{
	return (char *) 0;
}

void *
MT_mmap(char *path, int mode, off_t off, size_t len, void *fixed)
{
	void *ret = NULL;
	int mode0 = GENERIC_READ;
	int mode1 = FILE_SHARE_READ;
	int mode2 = mode & MMAP_ADVISE;
	int mode3 = PAGE_READONLY;
	int mode4 = FILE_MAP_READ;
	SECURITY_ATTRIBUTES sa;
	HANDLE h1, h2;

	if (mode & MMAP_WRITABLE) {
		mode0 |= GENERIC_WRITE;
		mode1 |= FILE_SHARE_WRITE;
	}
	if (mode2 == MMAP_RANDOM || mode2 == MMAP_DONTNEED) {
		mode2 = FILE_FLAG_RANDOM_ACCESS;
	} else if (mode2 == MMAP_SEQUENTIAL || mode2 == MMAP_WILLNEED) {
		mode2 = FILE_FLAG_SEQUENTIAL_SCAN;
	} else {
		mode2 = FILE_FLAG_NO_BUFFERING;
	}
	if (mode & MMAP_SYNC) {
		mode2 |= FILE_FLAG_WRITE_THROUGH;
	}
	if (mode & MMAP_COPY) {
		mode3 = PAGE_WRITECOPY;
		mode4 = FILE_MAP_COPY;
	} else if (mode & MMAP_WRITE) {
		mode3 = PAGE_READWRITE;
		mode4 = FILE_MAP_WRITE;
	}
	sa.nLength = sizeof(SECURITY_ATTRIBUTES);
	sa.bInheritHandle = TRUE;
	sa.lpSecurityDescriptor = 0;

	h1 = CreateFile(path, mode0, mode1, &sa, OPEN_ALWAYS, mode2, NULL);
	if (h1 == INVALID_HANDLE_VALUE) {
		GDKsyserror("MT_mmap: CreateFile('%s', %d, %d, &sa, %d, %d, NULL) failed\n", path, mode0, mode1, OPEN_ALWAYS, mode2);
		return (void *) -1L;
	}

	h2 = CreateFileMapping(h1, &sa, mode3, (DWORD) ((((gdk_int64) off + (gdk_int64) len) >> 32) & LL_CONSTANT(0xFFFFFFFF)), (DWORD) ((off + len) & LL_CONSTANT(0xFFFFFFFF)), NULL);
	if (h2 == NULL) {
		GDKsyserror("MT_mmap: CreateFileMapping(%x, &sa, %d, %d, %u, NULL) failed\n", h1, mode3, (DWORD) ((((gdk_int64) off + (gdk_int64) len) >> 32) & LL_CONSTANT(0xFFFFFFFF)), (DWORD) ((off + len) & LL_CONSTANT(0xFFFFFFFF)));
		CloseHandle(h1);
		return (void *) -1L;
	}
	if (fixed) {
		ret = MapViewOfFileEx(h2, mode4, (DWORD) ((gdk_int64) off >> 32), (DWORD) off, len, fixed);
	} else {
		ret = MapViewOfFile(h2, mode4, (DWORD) ((gdk_int64) off >> 32), (DWORD) off, len);
	}
	if (ret == NULL) {
		GDKsyserror("MT_mmap: MapViewOfFile%s(%x, %d, %d, %u, %d, %x) failed\n", h2, (fixed ? "Ex" : ""), mode4, (DWORD) ((gdk_int64) off >> 32), (DWORD) off, len, fixed);
		ret = (void *) -1L;
	}
	CloseHandle(h1);
	CloseHandle(h2);
	return ret;
}

int
MT_munmap(void *p, size_t dummy)
{
	(void)dummy;
	return (UnmapViewOfFile(p) ? 0 : -1);
	/*       Windows' UnmapViewOfFile returns success!=0, error== 0,
	 * while Unix's   munmap          returns success==0, error==-1. */
}

int
MT_msync(void *p, size_t len, int mode)
{
	(void)mode;
	return (FlushViewOfFile(p, len) ? 0 : -1);
	/*       Windows' FlushViewOfFile returns success!=0, error== 0,
	 * while Unix's   msync           returns success==0, error==-1. */
}

int
MT_madvise(void *p, size_t len, int advise)
{
	(void)p; 
	(void)len; 
	(void)advise;
	return 0;		/* would -1 be better? */
}

#ifndef _HEAPOK			/* MinGW */
#define _HEAPEMPTY      (-1)
#define _HEAPOK         (-2)
#define _HEAPBADBEGIN   (-3)
#define _HEAPBADNODE    (-4)
#define _HEAPEND        (-5)
#define _HEAPBADPTR     (-6)
#endif

struct mallinfo
MT_mallinfo(void)
{
	struct mallinfo _ret;
	_HEAPINFO hinfo;
	int heapstatus;

	hinfo._pentry = NULL;
	memset(&_ret, 0, sizeof(struct mallinfo));

	while ((heapstatus = _heapwalk(&hinfo)) == _HEAPOK) {
		_ret.arena += hinfo._size;
		if (hinfo._size > MT_SMALLBLOCK) {
			_ret.smblks++;
			if (hinfo._useflag == _USEDENTRY) {
				_ret.usmblks += hinfo._size;
				MT_alloc_register(hinfo._pentry, hinfo._size, 'H');
			} else {
				_ret.fsmblks += hinfo._size;
				MT_alloc_register(hinfo._pentry, hinfo._size, 'h');
			}
		} else {
			_ret.ordblks++;
			if (hinfo._useflag == _USEDENTRY) {
				_ret.uordblks += hinfo._size;
				MT_alloc_register(hinfo._pentry, hinfo._size, 'H');
			} else {
				_ret.fordblks += hinfo._size;
				MT_alloc_register(hinfo._pentry, hinfo._size, 'h');
			}
		}
	}
	if (heapstatus == _HEAPBADPTR || heapstatus == _HEAPBADBEGIN || heapstatus == _HEAPBADNODE) {

		stream_printf(GDKerr, "mallinfo(): heap is corrupt.");
	}
	_heapmin();
	return _ret;
}

int
MT_path_absolute(char *pathname)
{
	char *drive_end = strchr(pathname, ':');
	char *path_start = strchr(pathname, '\\');

	if (path_start == NULL) {
		return 0;
	}
	return (path_start == pathname || drive_end == (path_start - 1));
}


#ifndef HAVE_FTRUNCATE
int
ftruncate(int fd, off_t size)
{
	HANDLE hfile;
	unsigned int curpos;

	if (fd < 0)
		return -1;

	hfile = (HANDLE) _get_osfhandle(fd);
	curpos = SetFilePointer(hfile, 0, NULL, FILE_CURRENT);
	if (curpos == 0xFFFFFFFF || SetFilePointer(hfile, (LONG) size, NULL, FILE_BEGIN) == 0xFFFFFFFF || !SetEndOfFile(hfile)) {
		int error = GetLastError();

		if (error && error != ERROR_INVALID_HANDLE)
			SetLastError(ERROR_OPEN_FAILED);	/* enforce EIO */
		return -1;
	}

	return 0;
}
#endif

#ifndef HAVE_OPENDIR
DIR *
opendir(const char *dirname)
{
	DIR *result = NULL;
	char *mask;
	unsigned int k;

	if (dirname == NULL)
		return NULL;

	result = (DIR *) malloc(sizeof(DIR));
	result->find_file_data = malloc(sizeof(WIN32_FIND_DATA));
	result->dir_name = strdup(dirname);

	k = strlen(result->dir_name);
	if (k && result->dir_name[k - 1] == '\\') {
		result->dir_name[k - 1] = '\0';
		k--;
	}
	mask = malloc(strlen(result->dir_name) + 3);
	sprintf(mask, "%s\\*", result->dir_name);

	result->find_file_handle = (unsigned int) FindFirstFile(mask, (LPWIN32_FIND_DATA) result->find_file_data);
	free(mask);

	if (result->find_file_handle == (unsigned int) INVALID_HANDLE_VALUE) {
		free(result->dir_name);
		free(result->find_file_data);
		free(result);
		SetLastError(ERROR_OPEN_FAILED);	/* enforce EIO */
		return NULL;
	}
	result->just_opened = TRUE;

	return result;
}

static char *
basename(const char *file_name)
{
	register char *base;

	if (file_name == NULL)
		return NULL;

	base = strrchr(file_name, '\\');
	if (base)
		return base + 1;

	if (isalpha(file_name[0]) && file_name[1] == ':')
		return (char *) file_name + 2;

	return (char *) file_name;
}


struct direct *
readdir(DIR *dir)
{
	static struct direct result;

	if (dir == NULL)
		return NULL;

	if (dir->just_opened)
		dir->just_opened = FALSE;
	else {
		if (!FindNextFile((HANDLE) dir->find_file_handle, (LPWIN32_FIND_DATA) dir->find_file_data)) {
			int error = GetLastError();

			if (error) {
				if (error != ERROR_NO_MORE_FILES)
					SetLastError(ERROR_OPEN_FAILED);	/* enforce EIO */
				return NULL;
			}
		}
	}
	strcpy(result.d_name, basename(((LPWIN32_FIND_DATA) dir->find_file_data)->cFileName));
	result.d_namelen = strlen(result.d_name);

	return &result;
}

void
rewinddir(DIR *dir)
{
	char *mask;

	if (dir == NULL)
		return;

	if (!FindClose((HANDLE) dir->find_file_handle))
		stream_printf(GDKerr, "rewinddir(): FindClose() failed\n");

	mask = malloc(strlen(dir->dir_name) + 3);
	sprintf(mask, "%s\\*", dir->dir_name);
	dir->find_file_handle = (unsigned int) FindFirstFile(mask, (LPWIN32_FIND_DATA) dir->find_file_data);
	free(mask);

	if (dir->find_file_handle == (unsigned int) INVALID_HANDLE_VALUE) {
		SetLastError(ERROR_OPEN_FAILED);	/* enforce EIO */
		return;
	}
	dir->just_opened = TRUE;
}

int
closedir(DIR *dir)
{
	if (dir == NULL)
		return -1;

	if (!FindClose((HANDLE) dir->find_file_handle)) {
		SetLastError(ERROR_OPEN_FAILED);	/* enforce EIO */
		return -1;
	}

	free(dir->dir_name);
	free(dir->find_file_data);
	free(dir);

	return 0;
}
#endif /* HAVE_OPENDIR */

static int nodays[12] = { 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31 };

#define LEAPYEAR(y) ((((y)%4)==0 && ((y)%100)!=0) || ((y)%400)==0)
#define NODAYS(m,y) (((m)!=2)?nodays[(m)-1]:LEAPYEAR(y)?29:28)

int
gettimeofday(struct timeval *tv, int *ignore_zone)
{
	unsigned int year, day, month;
	SYSTEMTIME st;

	(void) ignore_zone;
	GetSystemTime(&st);
	day = 0;
	for (year = 1970; year < st.wYear; year++)
		day += LEAPYEAR(year) ? 366 : 365;

	for (month = 1; month < st.wMonth; month++)
		day += NODAYS(month, st.wYear);

	day += st.wDay;
	tv->tv_sec = 60 * (day * 24 * 60 + st.wMinute) + st.wSecond;
	tv->tv_usec = 1000 * st.wMilliseconds;
	return 0;
}

void *
dlopen(const char *file, int mode)
{
	(void) mode;
	if (file != NULL) {
		return (void *) LoadLibrary(file);
	}
	return NULL;
}

int
dlclose(void *handle)
{
	if (handle != NULL) {
		return FreeLibrary((HINSTANCE) handle);
	}
	return -1;
}

void *
dlsym(void *handle, const char *name)
{
	if (handle != NULL) {
		return (void *) GetProcAddress((HINSTANCE) handle, name);
	}
	return NULL;
}

char *
dlerror(void)
{
	static char msg[1024];

	FormatMessage(FORMAT_MESSAGE_FROM_SYSTEM, NULL, GetLastError(), 0, msg, sizeof(msg), NULL);
	return msg;
}

/* dir manipulations fail in WIN32 if file name contains trailing
 * slashes; work around this */
static char *
reduce_dir_name(const char *src, char *dst, size_t cap)
{
	size_t len = strlen(src);
	char *buf = dst;

	if (len >= cap)
		buf = malloc(len + 1);
	while (--len > 0 && src[len - 1] != ':' && src[len] == DIR_SEP)
		;
	for (buf[++len] = 0; len > 0; buf[len] = src[len])
		len--;
	return buf;
}

int
win_stat(const char *pathname, struct stat *st)
{
	char buf[128], *p = reduce_dir_name(pathname, buf, sizeof(buf));
	int ret = stat(p, st);

	if (p != buf)
		free(p);
	return ret;
}

int
win_rmdir(const char *pathname)
{
	char buf[128], *p = reduce_dir_name(pathname, buf, sizeof(buf));
	int ret = rmdir(p);

	if (p != buf)
		free(p);
	return ret;
}

int
win_mkdir(const char *pathname, const int mode)
{
	char buf[128], *p = reduce_dir_name(pathname, buf, sizeof(buf));
	int ret = mkdir(p);

	(void)mode;
	if (p != buf)
		free(p);
	return ret;
}

typedef struct {
	int w;			/* windows version of error */
	const char *s;		/* text of windows version */
	int e;			/* errno version of error */
} win_errmap_t;

#ifndef EBADRQC
#define EBADRQC 56
#endif
#ifndef ENODATA
#define ENODATA 61
#endif
#ifndef ENONET
#define ENONET 64
#endif
#ifndef ENOTUNIQ
#define ENOTUNIQ 76
#endif
#ifndef ECOMM
#define ECOMM 70
#endif
#ifndef ENOLINK
#define ENOLINK 67
#endif
win_errmap_t win_errmap[] = {
	{ERROR_INVALID_FUNCTION, "ERROR_INVALID_FUNCTION", EBADRQC},
	{ERROR_FILE_NOT_FOUND, "ERROR_FILE_NOT_FOUND", ENOENT},
	{ERROR_PATH_NOT_FOUND, "ERROR_PATH_NOT_FOUND", ENOENT},
	{ERROR_TOO_MANY_OPEN_FILES, "ERROR_TOO_MANY_OPEN_FILES", EMFILE},
	{ERROR_ACCESS_DENIED, "ERROR_ACCESS_DENIED", EACCES},
	{ERROR_INVALID_HANDLE, "ERROR_INVALID_HANDLE", EBADF},
	{ERROR_NOT_ENOUGH_MEMORY, "ERROR_NOT_ENOUGH_MEMORY", ENOMEM},
	{ERROR_INVALID_DATA, "ERROR_INVALID_DATA", EINVAL},
	{ERROR_OUTOFMEMORY, "ERROR_OUTOFMEMORY", ENOMEM},
	{ERROR_INVALID_DRIVE, "ERROR_INVALID_DRIVE", ENODEV},
	{ERROR_NOT_SAME_DEVICE, "ERROR_NOT_SAME_DEVICE", EXDEV},
	{ERROR_NO_MORE_FILES, "ERROR_NO_MORE_FILES", ENFILE},
	{ERROR_WRITE_PROTECT, "ERROR_WRITE_PROTECT", EROFS},
	{ERROR_BAD_UNIT, "ERROR_BAD_UNIT", ENODEV},
	{ERROR_SHARING_VIOLATION, "ERROR_SHARING_VIOLATION", EACCES},
	{ERROR_LOCK_VIOLATION, "ERROR_LOCK_VIOLATION", EACCES},
	{ERROR_SHARING_BUFFER_EXCEEDED, "ERROR_SHARING_BUFFER_EXCEEDED", ENOLCK},
	{ERROR_HANDLE_EOF, "ERROR_HANDLE_EOF", ENODATA},
	{ERROR_HANDLE_DISK_FULL, "ERROR_HANDLE_DISK_FULL", ENOSPC},
	{ERROR_NOT_SUPPORTED, "ERROR_NOT_SUPPORTED", ENOSYS},
	{ERROR_REM_NOT_LIST, "ERROR_REM_NOT_LIST", ENONET},
	{ERROR_DUP_NAME, "ERROR_DUP_NAME", ENOTUNIQ},
	{ERROR_BAD_NETPATH, "ERROR_BAD_NETPATH", ENXIO},
	{ERROR_FILE_EXISTS, "ERROR_FILE_EXISTS", EEXIST},
	{ERROR_CANNOT_MAKE, "ERROR_CANNOT_MAKE", EPERM},
	{ERROR_INVALID_PARAMETER, "ERROR_INVALID_PARAMETER", EINVAL},
	{ERROR_NO_PROC_SLOTS, "ERROR_NO_PROC_SLOTS", EAGAIN},
	{ERROR_BROKEN_PIPE, "ERROR_BROKEN_PIPE", EPIPE},
	{ERROR_OPEN_FAILED, "ERROR_OPEN_FAILED", EIO},
	{ERROR_NO_MORE_SEARCH_HANDLES, "ERROR_NO_MORE_SEARCH_HANDLES", ENFILE},
	{ERROR_CALL_NOT_IMPLEMENTED, "ERROR_CALL_NOT_IMPLEMENTED", ENOSYS},
	{ERROR_INVALID_NAME, "ERROR_INVALID_NAME", ENOENT},
	{ERROR_WAIT_NO_CHILDREN, "ERROR_WAIT_NO_CHILDREN", ECHILD},
	{ERROR_CHILD_NOT_COMPLETE, "ERROR_CHILD_NOT_COMPLETE", EBUSY},
	{ERROR_DIR_NOT_EMPTY, "ERROR_DIR_NOT_EMPTY", ENOTEMPTY},
	{ERROR_SIGNAL_REFUSED, "ERROR_SIGNAL_REFUSED", EIO},
	{ERROR_BAD_PATHNAME, "ERROR_BAD_PATHNAME", EINVAL},
	{ERROR_SIGNAL_PENDING, "ERROR_SIGNAL_PENDING", EBUSY},
	{ERROR_MAX_THRDS_REACHED, "ERROR_MAX_THRDS_REACHED", EAGAIN},
	{ERROR_BUSY, "ERROR_BUSY", EBUSY},
	{ERROR_ALREADY_EXISTS, "ERROR_ALREADY_EXISTS", EEXIST},
	{ERROR_NO_SIGNAL_SENT, "ERROR_NO_SIGNAL_SENT", EIO},
	{ERROR_FILENAME_EXCED_RANGE, "ERROR_FILENAME_EXCED_RANGE", EINVAL},
	{ERROR_META_EXPANSION_TOO_LONG, "ERROR_META_EXPANSION_TOO_LONG", EINVAL},
	{ERROR_INVALID_SIGNAL_NUMBER, "ERROR_INVALID_SIGNAL_NUMBER", EINVAL},
	{ERROR_THREAD_1_INACTIVE, "ERROR_THREAD_1_INACTIVE", EINVAL},
	{ERROR_BAD_PIPE, "ERROR_BAD_PIPE", EINVAL},
	{ERROR_PIPE_BUSY, "ERROR_PIPE_BUSY", EBUSY},
	{ERROR_NO_DATA, "ERROR_NO_DATA", EPIPE},
	{ERROR_PIPE_NOT_CONNECTED, "ERROR_PIPE_NOT_CONNECTED", ECOMM},
	{ERROR_MORE_DATA, "ERROR_MORE_DATA", EAGAIN},
	{ERROR_DIRECTORY, "ERROR_DIRECTORY", EISDIR},
	{ERROR_PIPE_CONNECTED, "ERROR_PIPE_CONNECTED", EBUSY},
	{ERROR_PIPE_LISTENING, "ERROR_PIPE_LISTENING", ECOMM},
	{ERROR_NO_TOKEN, "ERROR_NO_TOKEN", EINVAL},
	{ERROR_PROCESS_ABORTED, "ERROR_PROCESS_ABORTED", EFAULT},
	{ERROR_BAD_DEVICE, "ERROR_BAD_DEVICE", ENODEV},
	{ERROR_BAD_USERNAME, "ERROR_BAD_USERNAME", EINVAL},
	{ERROR_NOT_CONNECTED, "ERROR_NOT_CONNECTED", ENOLINK},
	{ERROR_OPEN_FILES, "ERROR_OPEN_FILES", EAGAIN},
	{ERROR_ACTIVE_CONNECTIONS, "ERROR_ACTIVE_CONNECTIONS", EAGAIN},
	{ERROR_DEVICE_IN_USE, "ERROR_DEVICE_IN_USE", EAGAIN},
	{ERROR_INVALID_AT_INTERRUPT_TIME, "ERROR_INVALID_AT_INTERRUPT_TIME", EINTR},
	{ERROR_IO_DEVICE, "ERROR_IO_DEVICE", EIO},
};

#define GDK_WIN_ERRNO_TLS 13

@h
#define _errno		win_errno

gdk_export int *win_errno(void);

@c
int *
win_errno(void)
{
	/* get address of thread-local Posix errno; refresh its value from WIN32 error code */
	int i, err = GetLastError() & 0xff;
	int *result = TlsGetValue(GDK_WIN_ERRNO_TLS);

	if (result == NULL) {
		result = (int *) malloc(sizeof(int));
		*result = 0;
		TlsSetValue(GDK_WIN_ERRNO_TLS, result);
	}
	for (i = 0; win_errmap[i].w != 0; ++i) {
		if (err == win_errmap[i].w) {
			*result = win_errmap[i].e;
			break;
		}
	}
	SetLastError(err);
	return result;
}
#endif

#ifndef WIN32

#define MT_PAGESIZE(s)		((((s-1)/MT_pagesize())+1)*MT_pagesize())

#if defined(MAP_ANON)
#define MMAP_FLAGS(f)		f|MAP_ANON
#define MMAP_FD			-1
#define MMAP_OPEN_DEV_ZERO	int fd = 1
#define MMAP_CLOSE_DEV_ZERO	(void)fd
#else
#define MMAP_FLAGS(f)		f
#define MMAP_FD			fd
#define MMAP_OPEN_DEV_ZERO	int fd = open("/dev/zero", O_RDWR, 0666)
#define MMAP_CLOSE_DEV_ZERO	close(fd)
#endif

void *
MT_vmalloc(size_t size, size_t * maxsize)
{
	MMAP_OPEN_DEV_ZERO;
	char *q, *r = (char *) -1L;

	if (fd < 0) {
		return NULL;
	}
	size = MT_PAGESIZE(size);
	*maxsize = MT_PAGESIZE(*maxsize);
	if (*maxsize > size) {
		r = (char *) mmap(NULL, *maxsize, PROT_NONE, MMAP_FLAGS(MAP_PRIVATE | MAP_NORESERVE), MMAP_FD, 0);
	}
	if (r == (char *) -1L) {
		*maxsize = size;
		q = (char *) mmap(NULL, size, PROT_READ | PROT_WRITE, MMAP_FLAGS(MAP_PRIVATE), MMAP_FD, 0);
	} else {
		q = (char *) mmap(r, size, PROT_READ | PROT_WRITE, MMAP_FLAGS(MAP_PRIVATE | MAP_FIXED), MMAP_FD, 0);
	}
	if (q != (char *) -1L) {
		(void) MT_mmap_new("anonymous vm", q, *maxsize, -1, 1);
	}
	MMAP_CLOSE_DEV_ZERO;
	return (void *) ((q == (char *) -1L) ? NULL : q);
}

void
MT_vmfree(void *p, size_t size)
{
	MT_mmap_del(p, size);
	size = MT_PAGESIZE(size);
	munmap(p, size);
}

void *
MT_vmrealloc(void *voidptr, size_t oldsize, size_t newsize, size_t oldmaxsize, size_t * newmaxsize)
{
	char *p = (char *) voidptr;
	char *q = (char *) -1L;

	/* sanitize sizes */
	oldsize = MT_PAGESIZE(oldsize);
	newsize = MT_PAGESIZE(newsize);
	oldmaxsize = MT_PAGESIZE(oldmaxsize);
	*newmaxsize = MT_PAGESIZE(*newmaxsize);
	if (*newmaxsize < newsize) {
		*newmaxsize = newsize;
	}

	if (oldsize > newsize) {
		munmap(p + oldsize, oldsize - newsize);
	} else if (oldsize < newsize) {
		if (newsize < oldmaxsize) {
			MMAP_OPEN_DEV_ZERO;
			if (fd >= 0) {
				q = (char *) mmap(p + oldsize, newsize - oldsize, PROT_READ | PROT_WRITE, MMAP_FLAGS(MAP_PRIVATE | MAP_FIXED), MMAP_FD, (off_t) oldsize);
				MMAP_CLOSE_DEV_ZERO;
			}
		}
		if (q == (char *) -1L) {
			q = (char *) MT_vmalloc(newsize, newmaxsize);
			if (q != NULL) {
				memcpy(q, p, oldsize);
				MT_vmfree(p, oldmaxsize);
				return q;
			}
		}
	}
	*newmaxsize = MAX(oldmaxsize, newsize);
	return p;
}

#if defined(HAVE_LOCKF) && defined(__MACH__)
/* lockf() seems to be there, but I didn't find any header file that declares the prototype ... */
extern int lockf(int fd, int cmd, off_t len);
#endif

#ifndef HAVE_LOCKF
/* Cygwin implementation: struct flock is there, but lockf() is
   missing.
 */
static int
lockf(int fd, int cmd, off_t len)
{
	struct flock l;

	if (cmd == F_LOCK || cmd == F_TLOCK)
		l.l_type = F_WRLCK;
	else if (cmd == F_ULOCK)
		l.l_type = F_UNLCK;
	l.l_whence = SEEK_CUR;
	l.l_start = 0;
	l.l_len = len;
	return fcntl(fd, cmd == F_TLOCK ? F_SETLKW : F_SETLK, &l);
}
#endif
/* return -1 when locking failed */
int
MT_lockf(char *filename, int mode, off_t off, off_t len)
{
	int ret = -1, fd = open(filename, O_CREAT | O_RDWR, 0662);

	if (fd < 0)
		return fd;
	if (lseek(fd, off, SEEK_SET) == off)
		ret = (lockf(fd, mode, len) == 0) ? 1 : -1;
	/* do not close else we lose the lock we want */
	return ret;
}

void
MT_sleep_ms(unsigned int ms)
{
#ifdef HAVE_NANOSLEEP
	struct timespec ts;

	ts.tv_sec = (time_t) (ms / 1000);
	ts.tv_nsec = 1000000 * (ms % 1000);
	while (nanosleep(&ts, &ts) == -1 && errno == EINTR)
		;
#else
	struct timeval tv;

	tv.tv_sec = ms / 1000;
	tv.tv_usec = ms % 1000;
	(void) select(0, NULL, NULL, NULL, &tv);
#endif
}

#else /* WIN32 */

#define MT_PAGESIZE(s)		(((((s)-1) >> 12) + 1) << 12)
#define MT_SEGSIZE(s)		((((((s)-1) >> 16) & 65535) + 1) << 16)

#ifndef MEM_TOP_DOWN
#define MEM_TOP_DOWN 0
#endif

void *
MT_vmalloc(size_t size, size_t * maxsize)
{
	void *p, *a = NULL;
	int mode = 0;

	size = MT_PAGESIZE(size);
	if (*maxsize < size) {
		*maxsize = size;
	}
	*maxsize = MT_SEGSIZE(*maxsize);
	if (*maxsize < 1000000) {
		mode = MEM_TOP_DOWN;	/* help NT in keeping memory defragmented */
	}
	if (*maxsize > size) {
		a = (void *) VirtualAlloc(NULL, *maxsize, MEM_RESERVE | mode, PAGE_NOACCESS);
		if (a == NULL) {
			*maxsize = size;
		}
	}
	p = (void *) VirtualAlloc(a, size, MEM_COMMIT | mode, PAGE_READWRITE);
	if (p == NULL) {
		stream_printf(GDKerr, "VirtualAlloc(" PTRFMT "," SZFMT ",MEM_COMMIT,PAGE_READWRITE): failed\n", PTRFMTCAST a, size);
	}
	return p;
}


void
MT_vmfree(void *p, size_t size)
{
	if (VirtualFree(p, size, MEM_DECOMMIT) == 0)
		stream_printf(GDKerr, "VirtualFree(" PTRFMT "," SZFMT ",MEM_DECOMMIT): failed\n", PTRFMTCAST p, size);
	if (VirtualFree(p, 0, MEM_RELEASE) == 0)
		stream_printf(GDKerr, "VirtualFree(" PTRFMT ",0,MEM_RELEASE): failed\n", PTRFMTCAST p);
}

void *
MT_vmrealloc(void *v, size_t oldsize, size_t newsize, size_t oldmaxsize, size_t * newmaxsize)
{
	char *p = (char *) v, *a = p;

	/* sanitize sizes */
	oldsize = MT_PAGESIZE(oldsize);
	newsize = MT_PAGESIZE(newsize);
	oldmaxsize = MT_PAGESIZE(oldmaxsize);
	*newmaxsize = MT_PAGESIZE(*newmaxsize);
	if (*newmaxsize < newsize) {
		*newmaxsize = newsize;
	}

	if (oldsize > newsize) {
		size_t ret = VirtualFree(p + newsize, oldsize - newsize, MEM_DECOMMIT);

		if (ret == 0)
			stream_printf(GDKerr, "VirtualFree(" PTRFMT "," SSZFMT ",MEM_DECOMMIT): failed\n", PTRFMTCAST(p + newsize), (ssize_t) (oldsize - newsize));
	} else if (oldsize < newsize) {
		a = (char *) VirtualAlloc(p, newsize, MEM_COMMIT, PAGE_READWRITE);
		if (a != p) {
			char *q = a;

			if (a == NULL) {
				q = MT_vmalloc(newsize, newmaxsize);
			}
			if (q != NULL) {
				memcpy(q, p, oldsize);
				MT_vmfree(p, oldmaxsize);
			}
			if (a == NULL)
				return q;
		}
	}
	*newmaxsize = MAX(oldmaxsize, newsize);
	return a;
}

int
MT_lockf(char *filename, int mode, off_t off, off_t len)
{
	int ret = 1, illegalmode = 0;
	OVERLAPPED ov;
	OSVERSIONINFO os;
	HANDLE fh = CreateFile(filename,
			       GENERIC_READ | GENERIC_WRITE, 0,
			       NULL, OPEN_ALWAYS, FILE_ATTRIBUTE_NORMAL, NULL);

	os.dwOSVersionInfoSize = sizeof(OSVERSIONINFO);
	GetVersionEx(&os);
	memset(&ov, 0, sizeof(ov));
	ov.Offset = (unsigned int) off;
#if 0
	ov.OffsetHigh = off >> 32;
#else
	ov.OffsetHigh = 0;	/* sizeof(off) == 4, i.e. off >> 32 is not possible */
#endif

	if (fh == NULL) {
		return -1;
	}
	if (mode == F_ULOCK) {
		if (os.dwPlatformId != VER_PLATFORM_WIN32_WINDOWS)
			ret = UnlockFileEx(fh, 0, 0, len, &ov);
	} else if (mode == F_TLOCK) {
		if (os.dwPlatformId != VER_PLATFORM_WIN32_WINDOWS)
			ret = LockFileEx(fh, LOCKFILE_FAIL_IMMEDIATELY | LOCKFILE_EXCLUSIVE_LOCK, 0, 0, len, &ov);
	} else if (mode == F_LOCK) {
		if (os.dwPlatformId != VER_PLATFORM_WIN32_WINDOWS)
			ret = LockFileEx(fh, LOCKFILE_EXCLUSIVE_LOCK, 0, 0, len, &ov);
	} else {
		illegalmode = 1;
	}
	CloseHandle(fh);
	if (illegalmode) {
		SetLastError(ERROR_INVALID_DATA);
	}
	return ret ? 0 : -1;
}

void
MT_sleep_ms(unsigned int ms)
{
	Sleep(ms);
}


@-
cygnus1.1.X has a bug in the semaphore routines. we work around it by directly using the WIN32 primitives.
@c
#ifndef NATIVE_WIN32

int
sem_init(sem_t * sem, int pshared, unsigned int value)
{
	(void) pshared;
	*sem = (sem_t) CreateSemaphore(NULL, value, 128, NULL);
	return (*sem) ? 0 : -1;
}

int
sem_destroy(sem_t * sem)
{
	return CloseHandle((HANDLE) * sem) ? 0 : -1;
}

int
sem_wait(sem_t * sem)
{
	return (WaitForSingleObject((HANDLE) * sem, (unsigned int) INFINITE) != WAIT_FAILED) ? 0 : -1;
}

int
sem_post(sem_t * sem)
{
	return (ReleaseSemaphore((HANDLE) * sem, 1, NULL) == 0) ? -1 : 0;
}
#endif
#endif
@}

@+ Memory fragmentation monitoring
On 32-bits systems, Monet's aggressive use of virtual memory may bring it into
trouble as the limits of what is addressable in a 32-bits system are reached 
(an 32-bits OS only allows 2 to 4GB of memory to be used). In order to aid debugging
situations where VM allocs fail (due to memory fragmentation), a monitoring
system was established. To this purpose, a map is made for the VM addresses
between 0 and 3GB, in tiles of MT_VMUNITSIZE (64KB). These tiles have a byte
value from the following domain:

@table @samp
@item 0-9 
thread stack space of thread <num>
@item B 
in use for a large BAT heap.
@item b 
free (last usage was B)
@item S 
in use for a malloc block
@item s 
free (last usage was S)
@item P 
in use for the BBP array
@item p 
free (last usage was P)
@item M 
in use as memory mapped region
@item m 
free (last usage was M)
@item C 
in use as MIL context buffer
@item c 
free (last usage was C)
@end table

The MT_alloc_printmap condenses the map by printing a char for each MB,
hence combining info from 16 tiles. On NT, we can check in real-time which 
tiles are actually in use (in case our own tile administration is out-of-sync 
with reality, eg due to a memory leak). This real-life usage is printed in a 
second line with encoding .=free, *=inuse, X=unusable. On Unix systems,
*=inuse is not testable (unless with complicated signal stuff). On 64-bits
systems, this administration is dysfunctional. 
@{
@c
#ifdef DEBUG_ALLOC

#if SIZEOF_VOID_P == 4
unsigned char MT_alloc_map[65536] = { 0 };
#endif

int
MT_alloc_register(void *addr, size_t size, char mode)
{
#if SIZEOF_VOID_P == 4
	size_t p = (size_t) addr;

	if (size > 0) {
		size_t i, base = p >> 16;

		size = (size - 1) >> 16;
		assert(p && ((lng) p) + size < (LL_CONSTANT(1) << 32));
		for (i = 0; i <= size; i++)
			MT_alloc_map[base + i] = (MT_alloc_map[base + i] & 128) | mode;
	}
#else
	(void) addr;
	(void) size;
	(void) mode;
#endif
	return 0;
}

#define INUSEMODE(x) ((x >= '0' && x <= ('9'+4)) || (x >= 'A' && x <= 'Z'))

int
MT_alloc_print(void)
{
#if SIZEOF_VOID_P == 4
#ifdef WIN32
	char *p = NULL;
#endif
	int i, j, k;

	if (MT_alloc_map[0] == 0)
		return 0;

	for (i = 0; i < 40; i++) {
		stream_printf(GDKout, "%02d00MB ", i);
		for (j = 0; j < 100; j++) {
			int mode = '.';

			for (k = 0; k < 16; k++) {
				int m = MT_alloc_map[k + 16 * (j + 100 * i)] & 127;

				if (mode == '.' || INUSEMODE(m))
					mode = m;
			}
			stream_printf(GDKout, "%c", mode);
		}
#ifdef WIN32
		stream_printf(GDKout, "\n       ");
		for (j = 0; j < 100; j++) {
			int mode = '.';

			for (k = 0; k < 16; k++, p += 1 << 16)
				if (!IsBadReadPtr(p, 1)) {
					mode = '*';
				} else if (MT_alloc_map[k + 16 * (j + 100 * i)] & 128) {
					mode = 'X';
				}
			stream_printf(GDKout, "%c", mode);
		}
#endif
		stream_printf(GDKout, "\n");
	}
#endif
	return 0;
}

@-
The memory table dump can also be produced in tuple format
to enable front-ends to analyze it more easily.
@c
struct {
	char tag;
	char *color;
	char *info;
} Encoding[] = {
	{
	'.', "0x00FFFDFE", "free"}, {
	'0', "0x000035FC", "thread stack space of thread 0"}, {
	'1', "0x000067FE", "thread stack space of thread 1"}, {
	'2', "0x000095FE", "thread stack space of thread 2"}, {
	'3', "0x0000BDFC", "thread stack space of thread 3"}, {
	'4', "0x0000DCF8", "thread stack space of thread 4"}, {
	'5', "0x002735FC", "thread stack space of thread 5"}, {
	'6', "0x002767FE", "thread stack space of thread 6"}, {
	'7', "0x002795FE", "thread stack space of thread 7"}, {
	'8', "0x0027BDFC", "thread stack space of thread 8"}, {
	'9', "0x0027DCF8", "thread stack space of thread 9"}, {
	'B', "0x0000672D", "in use for a large BAT heap."}, {
	'b', "0x004EF2A7", "free (last usage was B)"}, {
	'S', "0x00B4006E", "in use for a malloc block"}, {
	's', "0x00F2BDE0", "free (last usage was S)"}, {
	'P', "0x00F26716", "in use for the BBP array"}, {
	'p', "0x00F2BD16", "free (last usage was P)"}, {
	'M', "0x00959516", "in use as memory mapped region"}, {
	'm', "0x00CEDC16", "free (last usage was M)"}, {
	'C', "0x004EFDC7", "in use as MIL context buffer"}, {
	'c', "0x00FFFD2D", "free (last usage was M)"}, {
	0, "0x00FFFDFE", "free"}
};

int
MT_alloc_table(void)
{
#if SIZEOF_VOID_P == 4
#ifdef WIN32
	char *p = NULL;
#endif
	int i, j, k;

	if (MT_alloc_map[0] == 0)
		return 0;

	stream_printf(GDKout, "# addr\tX\tY\tcolor\tmode\tcomment\t# name\n");
	stream_printf(GDKout, "# str\tint\tint\tcolor\tstr\tstr\t# type\n");
	for (i = 0; i < 40; i++) {
		for (j = 0; j < 100; j++) {
			int mode = '.';

			for (k = 0; k < 16; k++) {
				int m = MT_alloc_map[k + 16 * (j + 100 * i)] & 127;

				if (mode == '.' || INUSEMODE(m))
					mode = m;
			}
			for (k = 0; k >= 0; k++)
				if (Encoding[k].tag == mode || Encoding[k].tag == 0) {
					if (mode == 0)
						mode = ' ';
					stream_printf(GDKout, "[ \"%d\",\t%d,\t%d,\t", k + 16 * (j + 100 * i), j, i);
					stream_printf(GDKout, "\"%s\",\t", Encoding[k].color);

					stream_printf(GDKout, "\"%c\",\t\"%s\"\t]\n", mode, Encoding[k].info);
					break;
				}
		}
		stream_flush(GDKout);
#ifdef WIN32
		stream_printf(GDKout, "\n       ");
		for (j = 0; j < 100; j++) {
			int mode = '.';

			for (k = 0; k < 16; k++, p += 1 << 16)
				if (!IsBadReadPtr(p, 1)) {
					mode = '*';
				} else if (MT_alloc_map[k + 16 * (j + 100 * i)] & 128) {
					mode = 'X';
				}
			stream_printf(GDKout, "%c", mode);
		}
		stream_printf(GDKout, "\n");
#endif
	}
#endif
	return 0;
}

static void
MT_alloc_init(void)
{
#if SIZEOF_VOID_P == 4
	char *p = NULL;
	int i;

	for (i = 0; i < 65536; i++, p += MT_VMUNITSIZE) {
		int mode = '.';

#ifdef WIN32
		if (!VirtualAlloc(p, MT_VMUNITSIZE, MEM_RESERVE, PAGE_NOACCESS)) {
			mode |= 128;
		} else {
			VirtualFree(p, 0, MEM_RELEASE);
		}
#else
		MMAP_OPEN_DEV_ZERO;
		void *q = (char *) mmap(p, MT_VMUNITSIZE, PROT_NONE, MMAP_FLAGS(MAP_NORESERVE), MMAP_FD, 0);

		MMAP_CLOSE_DEV_ZERO;
		if (q != p)
			mode |= 128;
		if (q != (char *) -1L)
			munmap(q, MT_VMUNITSIZE);
#endif
		MT_alloc_map[i] = mode;
	}
#endif
}
#endif /* DEBUG_ALLOC */

@}
