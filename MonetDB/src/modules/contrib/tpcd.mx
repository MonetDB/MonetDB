@' The contents of this file are subject to the MonetDB Public
@' License Version 1.0 (the "License"); you may not use this file
@' except in compliance with the License. You may obtain a copy of
@' the License at
@' http://monetdb.cwi.nl/Legal/MonetDBLicense-1.0.html
@' 
@' Software distributed under the License is distributed on an "AS
@' IS" basis, WITHOUT WARRANTY OF ANY KIND, either express or
@' implied. See the License for the specific language governing
@' rights and limitations under the License.
@' 
@' The Original Code is the Monet Database System.
@' 
@' The Initial Developer of the Original Code is CWI.
@' Portions created by CWI are Copyright (C) 1997-2004 CWI.
@' All Rights Reserved.
@' 
@' Contributor(s):
@' 		Martin Kersten <Martin.Kersten@cwi.nl>
@' 		Peter Boncz <Peter.Boncz@cwi.nl>
@' 		Niels Nes <Niels.Nes@cwi.nl>
@' 		Stefan Manegold  <Stefan.Manegold@cwi.nl>

@f tpcd
@t TPCD Specific Extensions
@a Peter Boncz
@v 1.0
@* Introduction
@T
This module is written for Annita Wilschut, so that she can continue 
to experiment with the MOA translator on the TCPD benchmark.
It contains {\bf arithmetic} in MEL commands, because the current
general arithmetic operations in the Monet kernel cannot be optimized.
These can. 
\\
Furthermore the first \{op\}() {\bf set aggregates}.
In the future, any command X on a unary BAT[Y,void] can be invoked in  
\{X\}(bat[oid,Y]) form, where the operation is executed for every
different oid on the collection of Y values they encode.
As with the [X]() operators, users can provide specific versions
in extension modules. This module contains efficient merge- and 
hash-based \{X\}() versions of {\tt sum}, {\tt max}, {\tt min}, {\tt avg}
and {\tt count}.
\\
There is a very fast semijoin, called the {\bf Vectorized Semijoin}.
It is supported by two search accelerators, the {\em datavaector},
and the {\em vectoridx}. More opertions using these data structures
were added later, like {\em Vectorized Join} and {\bf Vectorized Range 
Selections}.

@* Module Definition
@m
.MODULE tpcd;

.USE cluster;


@- Set-Aggregates
@m
.COMMAND {sum}(BAT[oid,int], BAT[oid,any]) : BAT[oid,int] = sum_int; 
	"grouped tail sum"
.COMMAND {sum}(BAT[int,int], BAT[int,any]) : BAT[int,int] = sum_int; 
	"grouped tail sum"
.COMMAND {sum}(BAT[oid,flt], BAT[oid,any]) : BAT[oid,flt] = sum_flt; 
	"grouped tail sum"
.COMMAND {sum}(BAT[int,flt], BAT[int,any]) : BAT[int,flt] = sum_flt; 
	"grouped tail sum"
.COMMAND {min}(BAT[oid,int], BAT[oid,any]) : BAT[oid,int] = min_int; 
	"grouped tail minimum"
.COMMAND {min}(BAT[int,int], BAT[int,any]) : BAT[int,int] = min_int; 
	"grouped tail minimum"
.COMMAND {min}(BAT[oid,flt], BAT[oid,any]) : BAT[oid,flt] = min_flt; 
	"grouped tail minimum"
.COMMAND {min}(BAT[int,flt], BAT[int,any]) : BAT[int,flt] = min_flt; 
	"grouped tail minimum"
.COMMAND {max}(BAT[oid,int], BAT[oid,any]) : BAT[oid,int] = max_int; 
	"grouped tail maximum"
.COMMAND {max}(BAT[int,int], BAT[int,any]) : BAT[int,int] = max_int; 
	"grouped tail maximum"
.COMMAND {max}(BAT[oid,flt], BAT[oid,any]) : BAT[oid,flt] = max_flt; 
	"grouped tail maximum"
.COMMAND {max}(BAT[int,flt], BAT[int,any]) : BAT[int,flt] = max_flt; 
	"grouped tail maximum"
.COMMAND {avg}(BAT[oid,int], BAT[oid,any]) : BAT[oid,int] = avg_int; 
	"grouped tail average"
.COMMAND {avg}(BAT[int,int], BAT[int,any]) : BAT[int,int] = avg_int; 
	"grouped tail average"
.COMMAND {avg}(BAT[oid,flt], BAT[oid,any]) : BAT[oid,flt] = avg_flt; 
	"grouped tail average"
.COMMAND {avg}(BAT[int,flt], BAT[int,any]) : BAT[int,flt] = avg_flt; 
	"grouped tail average"

@- Supporting OLAP Workloads
@T
The Decomposed Storage Model employed in Monet stores each attribute in a 
relationship (or class, for that matter), in an independent binary table. 
It has OIDs in the head, and 
attribute values in the tail. This storage model favors thin selections on 
wide tables, since you only need to access data you are interested in. 
Attributes you don not use, are just left untocuhed.  DSM also 
favors object evolution, since attributes are independent and thus can easily 
be added and deleted, and it favors main memory algorithms, because what it
does is {\em vertically fragmenting} the whole relationship; therefore 
reducing chunk sizes. Another important point here is that DSM allows
you to specify {\em a different clustering order} for each individual attribute.
\\
In the case of very large heaps of data that cannot be held in main memory, 
database systems tend to work best when data is clustered. In case of 
simple -- linear attributes -- {\em sorting} data is the best way of clustering.
This in turn leads to the use of {\em binary search} for selections,
and, {\em merge-algorithms}, for joins and semijoins, as the methods
of choice.
\\
OLAP as found in the TPCD benchmark queries typically consist of two phases:
\begin{itemize}
\item first, they select an interesting subset of objects; 
using some attributes for selection purposes. Let us say that there 
are $S$ attributes selected on, and call them the {\em selection-attributes} 
of a query.

\item then, in a second phase, computation of expressions on other attributes 
takes place, or aggregation on other attributes takes place. Let us suppose
that there are $V$ such attributes, and call these the {\em value-attributes}
of a query. 
\end{itemize}

The {\em selection-attributes} are typically fetched with (range-)selections,
so it would pay off to have their tables sorted on these attribute values, 
in order to employ binary search. Combining selected objects from multiple 
selects is in general not expensive, since this occurs in main memory. 
The result of the selection phase, is a collection of objects.
\\
In order to do computation and aggregation on the {\em value-attributes} 
of these selected objects, one needs to do a semijoin between the 
{\em value-attribute} table and the made selection.
Since we want to use a merge-semijoin for this, the optimal situation 
would be to have the table with {\em value-attributes} sorted on OID
order.

@- Data Vectors 
@T
Associates a BAT B[OID,attr] with two unary BATs: 
\begin{itemize}
\item a BAT E[OID,void], called the {\em extent}. This contains the head
values of the BAT, but in sorted order. 
\item it creates a new BAT V[void,attr],  called the {\em vector}. It
contains the tail values of the BAT, in the order as if B's tuples would 
have been ordered on head.
\end{itemize}
@m
.ACCELERATOR datavector(any);
      .BUILD     = DVbuild;
      .DESTROY   = DVdestroy;
      .INSERT    = DVinsert;
      .DELETE    = DVdelete;
      .SAVE      = DVsave;
      .REPAIR	 = DVrepair;
.END;

.COMMAND extent(BAT[oid,any::1]) : BAT[oid,void] = DVextent;
    "If there is a datavector on the tail of the BAT, this returns
     the associated extent BAT, else it gives an error."

.COMMAND vector(BAT[oid,any::1]) : BAT[void,any::1] = DVvector;
    "If there is a datavector on the tail of the BAT, this returns
     the associated unary vector BAT, else it gives an error."

@
@- Physical Design Choices
@T
Form the above, one would conclude that the clustering-requirements are 
contradicting: {\em selection-attributes} require sorting on tail, 
{\em value-attributes} require sorting on OID. And attributes have
different roles (value or selection) in different queries.
\\
One solution would be to store all BATs twice: one sorted on head, one 
on tail. This doubles disk needs, and entails duplication of all updates
on both versions.
\\ 
The solution employed here is to store all attributes ordered on tail
(to favor fast selections), and introduce a new {\em search-accelerator}
called the {\em datavector}. It consists of a unary BAT holding all tail 
values in OID order. For each relation or class, there is also
one unary table called the {\em extent}, contains a sorted list of all
OIDs of the relation. The {\em extent}, and all unary BATs, are then kept 
{\em aligned} with the Monet kernel support for this. That is, the system 
assures, that the X-th element in the extent corresponds with the X-th 
element in each unary BAT.
@
@- Vectorized (Semi) Join
@T
In order for this to be of any use, this module introduces  a special
semijoin as a very efficient operation in the phase of getting to
the {\em value-attributes} of an OLAP query. 
@m
.COMMAND dvsemijoin(BAT[oid,void], BAT[void,any::1], BAT[oid,any::2]) : 
    BAT[oid,any::1] = DVsemijoin; 
    "Performs semijoin(A.extent,A.vector,B) using a datavector on A.tail. 
     Creates a vextoridx on the head of B; if not present already."
@T
Doing a semijoin(A,B), with
B a the selected OIDs in the head, and A the BAT[OID,attr] containing the
{\em value-attribute} table, proceeds as follows:
\begin{itemize} 
\item first, all OIDs of B are looked up in the extent. The extent is sorted,
and a very fast 'probe-based' binary lookup is employed for this. This means
that for each OID we look up its position in the extent. The computed integer
index numbers for each OID are retained in a second accelerator, called the
{\em vectoridx}.
\end{itemize} 
@m
.ACCELERATOR vectoridx(oid);
      .BUILD     = DVIbuild;
      .DESTROY   = DVIdestroy;
      .INSERT    = DVIinsert;
      .DELETE    = DVIdelete;
      .SAVE      = DVIsave;
.END;
@T
\begin{itemize} 
\item then, instead of using A for the semijoin, we work with V, which is 
the unary BAT[void,attr] that is associated with the tail coluimn
a through the {\em datavector}.
For all OIDs in B, we get the value in V at the previously computed index
number. The thus found [OID,attr] combinations form the result of the semijoin.
\end{itemize} 
The {\em vectoridx} construction will typically take place once, when the
first semijoin is executed. Later semijoins will reuse the {\em vectoridx}
structure. Construction of it only acesses the extent BAT. In the worst case, 
when all its pages have to be touched, this will go through 
\mbox{$card * 4$} bytes of storage.  
\\
For each value attribute involved there is then the fast semijoin.
This just touches the unary {\em datavector} BAT, so in the worst case
takes up \mbox{$V * card * atomsize$} (atomsize == typically 4) bytes.
\\
In all, the OLAP query using these accelerators costs 
\mbox{$log(card)*S + card*(4 + V*atomsize)$} bytes of IO, as opposed to 
\mbox{$card*(S+V)*(4+atomsize)$} bytes in the case of normal DSM without 
ordering. There can easily be a factor 4 of difference in execution time!

A function similar to vectorized semijoin can be derived for joins:
@
@m
.COMMAND dvjoin(BAT[oid,void], BAT[void,any::1], BAT[oid,any::2]) : 
    BAT[any::1,any::2] = DVjoin; 
    "Performs join(A.extent,A.vector,B) using a datavector on A.tail. 
     Creates a vextoridx on the head of B; if not present already."

@- Vectorized Selections
@T
As said, we will store data ordered on attribute values, and then create
a datavector accelator on those ordered tail values. The datavector is 
then ordered on extent OIDs. \\
When selecting on tail values, you usually want to select on the normal
table, since you have it already ordered on tail. In later stages, however, 
when you want to use merge-algorithms , you will probably want 
to sort the result on OIDs. If the result is likely to be large, this
sorting can become very expensive. In such cases (very low selectivity),
the optimizer may prefer to select from the datavector and extent directly, 
using a full scan over them, since the resulting tuples will then be sorted 
on OID already (which saves doing the sorting step).\\
Therefore, special vectorized range selection operations are introduced. 
They perform full scan over both extent and vector, and produce 
a result ordered on OID:
@
@- MIL Initializations
@T
These are pieces of script with {\em standard constants} for the 
arithmetics.
\\
Then, there are the definitions of a number of {\em unary operators}: 
like {\tt uselect(A,B)}, {\tt usemijoin(A,B)},{\tt uunique(A,B)}:
{\tt udiff(A,B)}, {\tt uintersect(A,B)}, {\tt uunion(A,B)}. They all have
their usual semantics, but return a unary BAT with only the head-values of
the normal result.
\\
Operations that are often used on basetables ({\em select, join, semijoin})
are covered by generalized procs:   
\begin{itemize}
\item {\tt gselect(A,B)}:
there is a choice between a selection on a sorted BAT, or the full scan 
over the vector and extent. The latter is only done when the selectivity
is perceived to be low. Memory advise is given for the respective cases.
\item {\tt guselect(A,B)}:
the same as {\tt gselect(A,B)}, but producing an unary BAT as result.
\item {\tt gjoin(A,B)}:
if there is a datavector associated with one of the operands, the 
vectorized version of the algorithm is used, otherwise the standard
join is employed. In that case, we try to use the mergejoin, so if 
the one of the operands is much smaller and unsorted, and the bigger 
is sorted, we sort the smaller as a preprocessing step.
Memory advise is given for the respective cases.
\item {\tt gsemijoin(A,B)}:
if there is a datavector associated with one of the operands, the 
vectorized version of the algorithm is used, otherwise the standard
semijoin is employed.  In that case, we try to use the mergesemijoin, 
so if the one of the operands is much smaller and unsorted, and the bigger 
is sorted, we sort the smaller as a preprocessing step.
Memory advise is given for the respective cases.
\item {\tt gujoin(A,B)}:
\item {\tt gusemijoin(A,B)}:
the same as {\tt gsemijoin(A,B)}, but producing an unary BAT as result.
\end{itemize}
@mil
    const PI := 3.14;
    const E := 2.71;

    proc debug() : void {
        printf($(1..));
        printf("\n");
    }
    proc gsum(bat[any,any] a) : int {
        if (a.accpresent() = datavector) {
            debug("!sum: on vector of %s", str(a));
            return a.vector().sum();
        }
        return a.sum();
    }
    proc avg(..any..) : any { 
        return sum($1)/count($1); 
    }
    proc "{count}"(..any..) : int { 
        return histogram($1.reverse());
    }
    proc uunique(..bat[any,any]..) : bat[any,any] {
        return $1.project().kunique($2);
    }
    proc udiff(..bat[any,any]..) : bat[any,any]  {
        return $1.project().kdiff($2);
    }
    proc uintersect(..bat[any,any]..) : bat[any,any]  {
        return $1.project().kintersect($2);
    }
    proc uunion(..bat[any,any]..) : bat[any,any]  {
        return $1.project().kunion($2);
    }
    proc gorder(bat[any,any] b) : bat[any,any] {
        if (b.count() < 20000) {
            return b.order();
        }
        var h := b.reverse().histogram();
        return b.order(h);
    }
    proc gsemijoin(bat[any,any] a,bat[any,any] b) : bat[any,any]  {
        if (and(a.ordered(), not(b.ordered()), a.count() > (5 * b.count()))) {
            debug("!semijoin: order b: %s", str(b));
            b.madvise(BUF_WILLNEED, BUF_WILLNEED, BUF_WILLNEED);
            b.gorder();
        } else {
            b.madvise(BUF_SEQUENTIAL, BUF_SEQUENTIAL, BUF_SEQUENTIAL);
	}
        if (a.reverse().accpresent() = datavector) {
            var e := a.extent();
            var v := a.vector();
            e.madvise(BUF_SEQUENTIAL);
            v.madvise(BUF_SEQUENTIAL, BUF_SEQUENTIAL,BUF_SEQUENTIAL);
            b.madvise(BUF_SEQUENTIAL, BUF_SEQUENTIAL,BUF_SEQUENTIAL);
            debug("!semijoin: vectorized impl %s,%s", str(e),str(v));
            var bn := dvsemijoin(e,v,b); 
            e.unload();
            v.unload();
            return bn;
        }
        if (and(b.ordered(), not(a.ordered()), b.count() > (5 * a.count()))) { 
            a.madvise(BUF_WILLNEED, BUF_WILLNEED, BUF_WILLNEED);
            debug("!semijoin: order a: %s", str(a));
            a.gorder();
        } else {
            a.madvise(BUF_SEQUENTIAL, BUF_SEQUENTIAL, BUF_SEQUENTIAL);
        }
        return semijoin(a,b);
    }
    proc gjoin;
    proc gjoin(bat[any,any] a,bat[any,any] b) : bat[any,any]  {
  	var merge := and((a.count() > 200000), (b.count() > 200000));
	var ao := a.reverse().ordered();
	var bo := b.ordered();

        if (b.reverse().accpresent() = datavector) {
            debug("!join: revert for vector on %s", str(b));
            return gjoin(b.reverse(),a.reverse()).reverse();
        } 
        if (a.accpresent() = datavector) {
            var e := a.reverse().extent();
            var v := a.reverse().vector();

            e.madvise(BUF_RANDOM);
            v.madvise(BUF_SEQUENTIAL,nil,BUF_SEQUENTIAL);
            debug("!join: vectorized impl %s,%s", str(e),str(v));
            var bn := dvjoin(e,v,b); 
            e.unload();
            v.unload();
            return bn;
	}
        a.madvise(BUF_SEQUENTIAL, BUF_SEQUENTIAL, BUF_SEQUENTIAL);
        b.madvise(BUF_SEQUENTIAL, BUF_SEQUENTIAL, BUF_SEQUENTIAL);
	if (and(ao, bo)) {
            debug("!join: direct mergejoin %s,%s", str(a),str(b));
	    return join(a,b);
 	}
        if (and(merge, not(bo))) {
           b.madvise(BUF_WILLNEED, BUF_WILLNEED, BUF_WILLNEED);
           debug("!join: order b: %s", str(b));
           b.gorder();
        }
        if (and(merge, not(ao))) {
           a.madvise(BUF_WILLNEED, BUF_WILLNEED, BUF_WILLNEED);
           debug("!join: order a: %s", str(a));
           a.reverse().gorder();
        }
        debug("!join: mergejoin(%s,%s)", str(a), str(b));
        return join(a,b);
    }
    proc gselect(bat[any,any] b, ..any..)  : bat[any,any] {
        var lo := $2;

        if ($0 = 2) {
            return b.select(lo);
	}
        var hi := $3;

        if (and((b.reverse().accpresent() = datavector), (b.ttype() < str))) {
            if (lo.type() = void) lo := b.min(); 
            if (hi.type() = void) hi := b.max(); 
            if (((flt(hi - lo) / flt(b.max() - b.min())) * flt(b.count())) > 90000.0){
                var e := b.extent();
                b.unload();
                e.madvise(BUF_SEQUENTIAL);
                v.madvise(BUF_SEQUENTIAL,nil,BUF_SEQUENTIAL);
                debug("!select: vectorized impl %s,%s", str(e),str(v));
                var bn := uselect(e,v,lo,hi);
                e.unload();
                v.unload();
                return bn;
            }
        }
        if (b.reverse().ordered()) {
            b.madvise(BUF_SEQUENTIAL, BUF_SEQUENTIAL, BUF_SEQUENTIAL);
        }
        return b.uselect(lo,hi);
    }
    proc guselect(bat[any,any] b, ..any..)  : bat[any,any] {
        var lo := $2;

        if ($0 = 2) {
            return b.uselect(lo);
	}
        var hi := $3;

        if (and((b.reverse().accpresent() = datavector), (b.ttype() < str))) {
            if (lo.type() = void) lo := b.min(); 
            if (hi.type() = void) hi := b.max(); 
            if (((flt(hi - lo) / flt(b.max() - b.min())) * flt(b.count())) > 90000.0){
                var v := b.vector();
                var e := b.extent();
                b.unload();
                e.madvise(BUF_SEQUENTIAL);
                v.madvise(BUF_SEQUENTIAL,nil,BUF_SEQUENTIAL);
                debug("!uselect: vectorized impl %s,%s", str(e),str(v));
                var bn := uselect(e,v,lo,hi);
                e.unload();
                v.unload();
                return bn;
            }
        }
        if (b.reverse().ordered()) {
            b.madvise(BUF_SEQUENTIAL, BUF_SEQUENTIAL, BUF_SEQUENTIAL);
        }
        return b.select(lo,hi);
    }
@m
.END tpcd;

@{
@{
@* Aggregate Computation 
@c
#include "tpcd.proto.h"

@+ Set-Aggregate Implementation
@= sum
INLINE int
sum_@1(BAT **ret, BAT *b, BAT *e){
	BAT *bn, *u=e;
	BUN p, q, r;
	int xx, yy;
	@1 zero = (@1) 0;

	if (!e->hkey) {
		u = BATkunique(e);
	}
 	bn = BATnew(b->htype, BATttype(b), BATcount(u));	
	BATloopFast(u, p, q, xx) {
		BUNfastins(bn, BUNhloc(u,p), &zero);
	}
	ALIGNsetH(bn, u);
	if (e != u) {
		BBPreclaim(u);
	}

	BATprepareHash(bn);
	BATloopFast(b, p, q, xx) {
		oid *h = (oid*) BUNhloc(b,p);
		@1 *t = (@1*) BUNtloc(b,p);

		HASHloop_oid(bn, bn->hhash, yy, h, r) {
			@1 *dst = (@1*) BUNtloc(bn, r);
			if (*dst != @1_nil) {
				if (*t == @1_nil) {	
					*dst = @1_nil;
				} else {
					*dst += *t;
				}
			}
		}
	}
	*ret = bn;
	return GDK_SUCCEED;
}
@c
@:sum(int)@
@:sum(flt)@

@= extreme
INLINE int
@2_@1(BAT **ret, BAT *b, BAT *e){
	BAT *bn, *u=e;
	BUN p, q, r;
	int xx, yy;
	@1 special = @4;

	if (!e->hkey) {
		u = (BAT*) BATukunique(e);
	}
 	bn = BATnew(b->htype, BATttype(b), BATcount(u));	
	BATloopFast(u, p, q, xx) {
		BUNfastins(bn, BUNhloc(u,p), &special);
	}
	ALIGNsetH(bn, u);
	if (e != u) {
		BBPreclaim(u);
	}
	BATprepareHash(bn);
	BATloopFast(b, p, q, xx) {
		oid *h = (oid*) BUNhloc(b,p);
		@1 *t = (@1*) BUNtloc(b,p);

		HASHloop_oid(bn, bn->hhash, yy, h, r) {
			@1 *dst = (@1*) BUNtloc(bn, r);
			if (*dst != @1_nil) {
				if (*t == @1_nil) {	
					*dst = @1_nil;
				} else if (*t @3 *dst) {
					*dst = *t;
				}
			}
		}
	}
	BATloopFast(bn, p, q, xx) {
		@1 *t = (@1*) BUNtloc(bn,p);
		if (*t == special) *t = @1_nil;
	}
	*ret = bn;
	return GDK_SUCCEED;
}
@c
#include <limits.h>
#include <float.h>

@:extreme(int,min,<,INT_MAX)@
@:extreme(flt,min,<,FLT_MAX)@
@:extreme(int,max,>,INT_MIN)@
@:extreme(flt,max,>,FLT_MIN)@

@= avg
INLINE int
avg_@1(BAT **ret, BAT *b, BAT *e){
	BAT *bn, *u=e;
	BUN p, q, r;
	int xx, yy;
	int *cnt, zero = 0;

	if (!e->hkey) {
		u = BATkunique(e);
	}
	cnt = (int*) GDKmalloc(BATcount(u)*sizeof(int)); 
	memset(cnt, 0, BATcount(u)*sizeof(int));

 	bn = BATnew(b->htype, BATttype(b), BATcount(u));	
	BATloopFast(u, p, q, xx) {
		BUNfastins(bn, BUNhloc(u,p), &zero);
	}
	ALIGNsetH(bn, u);
	if (e != u) {
		BBPreclaim(u);
	}

	BATprepareHash(bn);
	BATloopFast(b, p, q, xx) {
		oid *h = (oid*) BUNhloc(b,p);
		@1 *t = (@1*) BUNtloc(b,p);

		HASHloop_oid(bn, bn->hhash, yy, h, r) {
			@1 *dst = (@1*) BUNtloc(bn, r);
			if (*dst != @1_nil) {
				if (*t == @1_nil) {	
					*dst = @1_nil;
				} else {
					*dst += *t;
				}
				cnt[yy-1]++;
			}
		}
	}
	yy = 0;
	BATloopFast(bn, p, q, xx) {
		@1 *dst = (@1*) BUNtloc(bn, p);
		if (cnt[yy] == 0) {
			*dst = @1_nil;
		} else {
			*dst /= cnt[yy];
		} yy++;
	}
	*ret = bn;
	return GDK_SUCCEED;
}
@c
@:avg(int)@
@:avg(flt)@


@* Vector Index 
@T
The vector index is an array of integers for each element in the BAT.
The integer number indicates a position in an associated {\em extent} BAT.
@
Data about this extent, like its @%batCacheid@, name, and alignment
information.
@+ Type Definition
@c
extern int ACC_vectoridx;

typedef struct {
	bat extent;
	char extent_name[IDLENGTH];
	oid extent_align;
	int vectorsize;
	int vector[1]; /* TRICKY really dynamic: vector[vectorsize] */
} vectoridx;

@+ Implementation 
@c
vectoridx* vectoridx_new(Heap* hp, BAT *e, int cnt) {
	vectoridx *vi;
	if (HEAPalloc(hp, sizeof(vectoridx)+cnt*sizeof(int),1) < 0) return NULL;
	hp->free = hp->size;
	vi = (vectoridx*) hp->base;
	sprintf(vi->extent_name, "%s", e->batId);
	vi->extent = e->batCacheid;
	vi->extent_align = e->halign;
	vi->vectorsize = cnt;
	return vi;
}
void
DVIbuild(
   Heap*	hp,	/* INOUT: vectoridx accelerator heap */
   int*		param,	/* an extra parameter */
   BAT*		b	/* IN: BAT with head-column. */
){
	BAT *extent = BATdescriptor(*(bat*)param);	
	oid *first, *cur;
	int xx, yy=0;
	vectoridx *vi;
	BUN p, q;
@-
Check things
@c
	if (extent==NULL) {
	    GDKerror("DVIbuild: you must provide an extent BAT.\n");
	    return;
	}
	if ((BAThordered(extent)&1) == 0) {
	    GDKerror("DVIbuild: extent must be sorted.\n");
	    return;
	}
	if (extent->htype != TYPE_oid) {
	    GDKerror("DVIbuild: illegal extent.\n");
	    return;
	}
@-
Allocate memory for the index. Fill the structure.
@c
	vi = vectoridx_new(hp, extent, BATcount(b));
	if (vi == NULL) {
	    GDKerror("DVIbuild: could not allocate vectorindex.\n");
	    return;
	}
	cur = first = (oid*) BUNfirst(extent);
@-
Merge-walk through 'b', and 'extent', knowing that both are ordered
and assuming that 'extent' is a superset of 'b'.
@c
	if (BATordered(b) && (3*BATcount(b) > BATcount(extent))) {
	    oid* end = (oid*) BUNlast(extent); 
	    BATloopFast(b, p, q, xx) {
		while(*cur - *(oid*) BUNhloc(b,p)) 
		    if (++cur >= end) goto error;
		vi->vector[yy++] = cur - first;
	    }
@-
Walk through 'b' and use very fast probe-based binary lookup in the extent.
On dense BATs this is just a computed array lookup in 'extent'.
@c
	} else { 
	    BATreverse(extent);
	    BATloopFast(b, p, q, xx) {
		cur = (oid*) SORTfndfirst_int(extent, BUNhloc(b,p));
		if (cur == NULL) {
error:		    GDKerror("DVIbuild: %s has %d, %s not.\n",
			b->batId, *(oid*)BUNhloc(b,p), extent->batId);
		    ACCdestroy(ACC_vectoridx, b, hp);
		    return;
		}
@-
Then we compute the position of the found OID in 'extent'. It is stored
in the vectoridx.
@c
		vi->vector[yy++] = cur - first;
	    }
	}
}

void
DVIdestroy(
   Heap*	hp,	/* INOUT: accelerator heap. */
   BAT*		b	/* IN: BAT with accelerated head column. */
){
	(void) b;
	HEAPfree(hp);
}

void
DVIinsert(
   Heap*	hp,	/* INOUT: vectoridx accelerator heap. */
   size_t	idx,	/* IN: BUNindex() of value. */
   BAT*		b,	/* IN: BAT with accelerated head column. */
   oid*     	o	/* IN: atomic value that was inserted. */
){
	(void) idx; (void) o;
	GDKwarning("DVIinsert: destroys vector index\n");
	ACCdestroy(ACC_vectoridx, b, hp);
}

void
DVIdelete(
   Heap*	hp,	/* INOUT: vectoridx accelerator heap. */
   size_t	idx,	/* IN: BUNindex() of value. */
   BAT*		b,	/* IN: BAT with accelerated head column. */
   oid*     	o	/* IN: atomic value that was deleted. */
){
	(void) idx; (void) o;
	GDKwarning("DVIdelete: destroys vector index\n");
	ACCdestroy(ACC_vectoridx, b, hp);
}

void
DVIsave(
   Heap*	hp,	/* INOUT: accelerator heap. */
   BAT*		b	/* IN: BAT with accelerated head column. */
){
	(void) hp; (void) b;
	/* vectoridx *v = (vectoridx*) hp->base; */
}



@* Data Vectors
@+ Type Definition 
@c
extern int ACC_datavector;

typedef struct {
	bat vector;
	char vector_name[IDLENGTH];
	bat extent;
	char extent_name[IDLENGTH];
} datavector;

@+ Implementation
@c
void
DVbuild(
   Heap*	hp,	/* INOUT: vector accelerator heap */
   int*		param,	/* an extra parameter */
   BAT*		b	/* IN: BAT with head-column. */
){
	BAT *bn, *extent = BATdescriptor(*(bat*)param);	
	int bunsize, atomsize, xx, yy=0;
	datavector *v;
	vectoridx *vi;
	BUN p, q;
@-
Create a new BAT, which will contain the data vector.
@c
	if (HEAPalloc(hp, sizeof(datavector), 1) < 0) {
		GDKerror("DVbuild: could not allocate datavector\n");
		return;
	}
	hp->free = hp->size;
	v = (datavector*) hp->base;
	sprintf(v->vector_name, "%s_v", b->batId);
	sprintf(v->extent_name, "%s", extent->batId);
	v->extent = extent->batCacheid;
@-
Important! Check whether Santa Claus left the vector under the chimney.
@c
	if ((v->vector = BBPindex(v->vector_name)) != 0) {
	    bn = BBPdescriptor(v->vector);
	    if (!ALIGNsynced(bn, extent)) {
		GDKwarning("DVbuild: aligning %s and %s..\n", 
			extent->batId, bn->batId); 
		ALIGNsynced(bn, extent);
	    } 
	    return;
	}
@-
Look up the oid positions in the extent.
@c
	ACCbuild(ACC_vectoridx, BATmirror(b), &b->taccelerator, (ptr) param);
	if (b->taccelerator.base == NULL) {
	    return;
	}
	vi = (vectoridx*) b->taccelerator.base;

	bn = BATnew(TYPE_void, b->htype, BATcount(b));
	BATrename(bn, v->vector_name);
	BATmode(bn, PERSISTENT);
	v->vector = bn->batCacheid;
	atomsize = ATOMsize(b->htype);
	bunsize = BUNsize(bn);
	bn->batBuns->free = (BATcount(b)+1)*bunsize;
@-
Create a tail column, filled with the contents of bn, but
ordered like 'extent'.
@c
	if (b->hvarsized) {
	    BATloopFast(b, p, q, xx) {
		ATOMput((bn)->ttype, &(bn)->theap, 
			BUNptr(bn,1+vi->vector[yy]), BUNhvar(b,p));
		yy++;
	    }
	} else if (atomsize == sizeof(int)) {
	    BATloopFast(b, p, q, xx) {
		*(int*) BUNptr(bn,1+vi->vector[yy]) = *(int*) BUNhloc(b,p);
		yy++;
	    }
	} else {
	    BATloopFast(b, p, q, xx) {
		char *dst = (char*) BUNptr(bn,1+vi->vector[yy]);
		char *end = dst + atomsize;
		char *src = (char*) BUNhloc(b,p);
		while(dst < end) *dst++ = *src++;
		yy++;
	    }
 	}
	bn->hsorted = 1;
	bn->tsorted = 0;
	ALIGNsetH(bn, extent); 
	return;
bunins_failed:
	HEAPfree(hp);
	BBPreclaim(bn);
}

void
DVdestroy(
   Heap*	hp,	/* INOUT: accelerator heap. */
   BAT*		b	/* IN: BAT with accelerated head column. */
){
	datavector *v = (datavector*) hp->base;	
	str vector_name;

	if (v == NULL) {
	    GDKerror("DVdestroy: null vector reference.\n");
	    return;
	} else if (BBPdescriptor(v->vector) == 0) {
	    GDKerror("DVdestroy: invalid reference %d to vector %s\n",
		v->vector, v->vector_name);
	} else {
	    vector_name = BBPname(v->vector);
	    if (vector_name && *vector_name &&
		!strcmp(vector_name,v->vector_name)) 
	    {
		BBPunfix(v->vector);
	    } else {
		GDKerror("DVdestroy: unary index %s was lost previously\n", 
			v->vector_name);
	    }
	}
	HEAPfree(hp);
	b->hacctype = 0;
}

void
DVinsert(
   Heap*	hp,	/* INOUT: vector accelerator heap. */
   size_t	idx,	/* IN: BUNindex() of value. */
   BAT*		b,	/* IN: BAT with accelerated head column. */
   ptr      	p	/* IN: atomic value that was inserted. */
){
	(void) idx; (void) p;
	GDKwarning("DVinsert: destroys vector\n");
	ACCdestroy(ACC_datavector, b, hp);
}

void
DVdelete(
   Heap*	hp,	/* INOUT: vector accelerator heap. */
   size_t	idx,	/* IN: BUNindex() of value. */
   BAT*		b,	/* IN: BAT with accelerated head column. */
   ptr      	p	/* IN: atomic value that was deleted. */
){
	(void) idx; (void) p;
	GDKwarning("DVdelete: destroys vector\n");
	ACCdestroy(ACC_datavector, b, hp);
}

void
DVsave(
   Heap*	hp,	/* INOUT: accelerator heap. */
   BAT*		b	/* IN: BAT with accelerated head column. */
){
	(void) hp; (void) b;
	/* datavector *v = (datavector*) hp->base; */
}

int
DVrepair(
   Heap*     	hp,	/* put return BAT[oid,any] here. */
   BAT*     	b	/* pointer to BAT[oid,any] record.*/
){
	datavector *v = (datavector*) hp->base;	
	str vector_name = BBPname(v->vector);
	str extent_name = BBPname(v->extent);

	if (vector_name && *vector_name &&
	    extent_name && *extent_name &&
	    strcmp(vector_name,v->vector_name) == 0 &&
	    strcmp(extent_name,v->extent_name) == 0 &&
	    ALIGNsynced(BBPdescriptor(v->vector),
	    		BBPdescriptor(v->extent))) 
	{
		return GDK_SUCCEED;	
	}
	ACCdestroy(ACC_datavector, b, hp);
	return GDK_FAIL;	
}

@+ Information Functions
@c
int
DVextent(
   BAT**     	retval,	/* put pointer to BAT[oid,any] record here. */
   BAT*     	b	/* pointer to BAT[oid,any] record.*/
){
    if (b->tacctype == ACC_datavector) {
	datavector *v = (datavector*) b->taccelerator.base;
	BAT *extent;
	if (v==NULL || v->extent==0 || !(extent=BBPdescriptor(v->extent))) {
		GDKerror("DVextent: illegal datavector.\n");
		return GDK_FAIL;
	}
	*retval = extent;
	return GDK_SUCCEED;
    }
    GDKerror("DVextent: no datavector on the tail.\n");
    return GDK_FAIL;
}

int
DVvector(
   BAT**     	retval,	/* put pointer to BAT[oid,any] record here. */
   BAT*     	b	/* pointer to BAT[oid,any] record.*/
){
    if (b->tacctype == ACC_datavector) {
	datavector *v = (datavector*) b->taccelerator.base;
	BAT *vector;
	if (v==NULL || v->vector==0 || !(vector=BATdescriptor(v->vector))) {
		GDKerror("DVvector: illegal datavector.\n");
		return GDK_FAIL;
	}
	*retval = vector;
	return GDK_SUCCEED;
    }
    GDKerror("DVvector: no datavector on the tail.\n");
    return GDK_FAIL;
}

@* Vectorized Operations
Here both the @%datavector@ and @%vectoridx@ accelerators are used 
to speed up semijoins.

@+ Vectorized Semijoin
@= dvprepare
	int bunsize, xx, yy=0;
	BUN base, heapbase, p, q;
	vectoridx *vi = (vectoridx*) r->haccelerator.base;

	if (!ALIGNsynced(e, v)) {
		GDKerror("DV@1: incompatible extent %s and vector %s.\n",
			e->batId, v->batId);
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	if (r->hacctype == ACC_vectoridx && vi->extent != e->batCacheid) {
		GDKwarning("DV@1: destroying incompatible vectoridx\n");
		ACCdestroy(ACC_vectoridx, r, &r->haccelerator);
	}
	if (r->hacctype == ACC_vectoridx && vi->extent_align != e->halign) {
		GDKwarning("DV@1: destroying misaligned vectoridx\n");
		ACCdestroy(ACC_vectoridx, r, &r->haccelerator);
	}
	if (r->hacctype != ACC_vectoridx) {
		bat id = e->batCacheid;
		ACCbuild(ACC_vectoridx, r, &r->haccelerator, (ptr)(&id));
	}
	if (r->hacctype != ACC_vectoridx) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	vi = (vectoridx*) r->haccelerator.base;
	base = BUNfirst(v);
	heapbase = v->theap.base;
	bunsize = BUNsize(v);
@c
int
DVsemijoin(
   BAT**     	retval,	/* put pointer to BAT[oid,any] record here. */
   BAT*     	e,	/* pointer to BAT[oid,any] record.*/
   BAT*     	v,	/* pointer to BAT[oid,any] record.*/
   BAT*     	r	/* pointer to BAT[oid,any] record.*/
){
	BAT *bn = BATnew(TYPE_oid, BATttype(v), BATcount(r));
	@:dvprepare(semijoin)@

	if (v->tvarsized) {
	    BATloopFast(r, p, q, xx) {
		BUNfastins(bn, BUNhloc(r,p), 
			heapbase + *(int*) (base + vi->vector[yy]*bunsize));
		yy++;
	    }
	} else {
	    BATloopFast(r, p, q, xx) {
		BUNfastins(bn, BUNhloc(r,p), base + vi->vector[yy]*bunsize);
		yy++;
	    }
	}
	*retval = bn;
	BATkey(bn, 1);
	bn->hsorted = r->hsorted;
	bn->tsorted = 0;
	ALIGNsetH(bn, r);
	return GDK_SUCCEED;
}
@+ Vectorized Join
Is implemented reusing the semijoin code. We look up the OID values
of the right BAT in the extent. We then fetch positionally from the vector.
The only difference with semijoin is that the result consists of 
[vector,right-tail] tuples, instead of [OID,vector] tuples.

@= joinins
    if (v->tvarsized) {
	BATloopFast(r, p, q, xx) {
	    BUNfastins(bn, heapbase + *(int*) (base + vi->vector[yy]*bunsize),
			   BUNt@1(r,p)); 
	    yy++;
  	}
    } else {
	BATloopFast(r, p, q, xx) {
	    BUNfastins(bn, base + vi->vector[yy]*bunsize, BUNt@1(r,p));
	    yy++;
	}
   }
@c
int
DVjoin(
   BAT**     	retval,	/* put pointer to BAT[oid,any] record here. */
   BAT*     	e,	/* pointer to BAT[oid,any] record.*/
   BAT*     	v,	/* pointer to BAT[oid,any] record.*/
   BAT*     	r	/* pointer to BAT[oid,any] record.*/
){
	BAT *bn = BATnew(BATttype(v), BATttype(r), BATcount(r));
	@:dvprepare(join)@

	if (r->tvarsized) {
	   @:joinins(var)@
	} else {
	   @:joinins(loc)@
	}
	*retval = bn;
	bn->hsorted = 0;
	bn->tsorted = r->tsorted;
	ALIGNsetT(bn, r);
	return GDK_SUCCEED;
}
@}
@}
