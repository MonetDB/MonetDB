@' The contents of this file are subject to the MonetDB Public
@' License Version 1.0 (the "License"); you may not use this file
@' except in compliance with the License. You may obtain a copy of
@' the License at
@' http://monetdb.cwi.nl/Legal/MonetDBLicense-1.0.html
@'
@' Software distributed under the License is distributed on an "AS
@' IS" basis, WITHOUT WARRANTY OF ANY KIND, either express or
@' implied. See the License for the specific language governing
@' rights and limitations under the License.
@'
@' The Original Code is the Monet Database System.
@'
@' The Initial Developer of the Original Code is CWI.
@' Portions created by CWI are Copyright (C) 1997-2004 CWI.
@' All Rights Reserved.

@a M.L. Kersten, P. Boncz, A.P. de Vries -- $Id$
@f xtables
@t Cross Table Module
@v 2.2
@T
{\bf Copyright \copyright\ 1996-1999 by Data Distilleries, All Rights Reserved.

No part of this work covered by the copyright hereon may be reproduced or used
in any form or by no means- graphic, electronic, or mechanical, including
photocopying, recording, taping, or information storage and retrieval system -
without permission of the copyright owner. }

@{
@+ Document control
@T
(June 2001:) Arjen made the size of the hashtable parametric, which is needed
for text corpora (many different clusters (>30000), widely varying size (Zipfian)).
(May 2001:) Arjen added some minor changes to better handle `clustered' groups;
for now, limited applicability to a derived property, using
$$ \mathrm{sorted} \land \mathrm{keyed} \\implies \mathrm{clustered} $$

(before:)
\begin{verbatim}
New implementation of xtables *from*scratch*. Got fed up with histolink. 
This is simple stable and fast. good memory performance.
\end{verbatim}
@
@}

@* Introduction
@T
Data mining applications require efficient construction and manipulation 
of cross-tables.
The Monet kernel already provides for a histogram function over a single BAT.
This module provides extensions to support identification of groups in a
multi-dimensional space.

The prime limitation is that an underlying database of {oid->any} BATs
is assumed. This enables representation of each group using an oid,
and the value representation of the group can be accordingly be
retrieved easily. An optimized implementation in which we use positional
integer id's (as embodied by Monet's void type) is also available.

@+ Algorithms
@T
There are several approaches to build a cross table. The one chosen here
is aimed at incremental construction, such that re-use of intermediates
becomes possible. Starting with the first dimension, a BAT is derived to
represent the various groups, called a {\bf CT BAT} or cross-table BAT.

@- Cross Table (CT)
@T
A cross table is an <oid,oid> BAT where the first (head) denotes a tuple in
the cross table and the second (tail) marks all identical lists.
The tail-oids contain group identifiers; that is, {\em this value is equal
{\bf iff} two tuples belong to the same group}. The group identifiers are
chosen from the domain of the tuple-identifiers. This simplifies 
getting back to the original tuples, when talking about a group.
If the tuple-oid of 'John' is chosen as a group-id, you might view this
as saying that each member of the group is 'like John' with respect
to the grouping-criterion.

@- going deeper
@T
Successively the subgroups can be identified by modifying the CT BAT or
to derive a new CT BAT for the subgroups. After all groups have been
identified this way, a BAT histogram operation can be used to obtain
the counts of each data cube. Other aggregation operations using the MIL
set aggregate construct \{X\}(bat) (see the 
@[<a href="../../../FrontEnds/mil/index.html#mod_3_2_0">MIL Reference Manual</a>@)  
can be used as well; note for instance that histogram == \{count\}(b.reverse). 

@* Module Definition
The Monet interface module specification is shown below.
@m
.MODULE xtables;

.ATOM mapentry[32,8];
.END;
.ATOM idxentry[16,8];
.END;

@-
basic multi-attribute group operations, that take the incremental approach. 
CT[re]group and CTderive implement GROUPBY, CTrefine implements ORDERBY.
@m
.COMMAND CTgroup(BAT[oid,any] attr) : BAT[oid,oid] = CTgroup;
        "Cross tabulation group initialization.
         Returned head values are identical as in 'attr'. Tail values are from
         the same domain and indicate unique groups in 'attr' tail column."
 
.COMMAND _CTgroup(BAT[oid,any] attr, int N, int rng) : BAT[oid,oid] = CTgroup_custom;
        "Cross tabulation group initialization like CTgroup, but with user
         provided #bits in hashmask and #distinct values in range."
 
.COMMAND CTderive(BAT[oid,any] ct, BAT[oid,any] attr) : BAT[oid,oid] = CTderive;
        "Cross tabulation group extension step.
         Returned head values are identical as in 'ct'. Tail values are from
         the same domain and indicate further refinement of the groups in 'ct',
         taking into account also the tail-values in 'attr'."

.COMMAND CTgroup(BAT[oid,any] ct, BAT[oid,any] attr) : BAT[oid,oid] = CTderive;
        "binary grouping; a synonym for CTderive(ct,attr)"

.COMMAND CTregroup(BAT[oid,oid] ct, BAT[oid,any] attr) : BAT[oid,oid] 
		= CTregroup; "modifies ct and returns it doing a group"

.COMMAND CTrefine(BAT[oid,any] b, BAT[oid,any] a) : BAT[oid,oid] = CTrefine;
	"refine the ordering of a tail-ordered BAT by sub-ordering on the 
         values of a second bat 'a' (where the heads of a and b match 1-1).  
         The effect of this is similar to (hash-based) CTderive, with the 
	 distinction that the group ids respect the ordering of the group values."

.COMMAND CTrefine_rev(BAT[oid,any] b, BAT[oid,any] a) : BAT[oid,oid] = CTrefine_rev;
	"reverse sorting version of CTrefine"
@-
histogram support. during group computation we already compute histograms
for quick re-use. They are attached to the CT by means of a histolink.
@m
.ACCELERATOR histolink(any);
      .BUILD     = CTbuild_hl;
      .DESTROY   = CTdestroy_hl;
      .INSERT    = CTupdate_hl;
      .DELETE    = CTupdate_hl;
.END;

.COMMAND histogram(BAT[oid,any::1]) : BAT[any::1,int] = CThistogram;
        "compute a histogram on the tail. Optimized for retrieving the
         already present histogram on a cross-table computed by CTgroup."

.COMMAND tunique(BAT[oid,any::1]) : BAT[any::1,void] = CTtunique;
        "get all unique tail values. Optimized for getting all unique
         oids from a cross-table computed by CTgroup."

.COMMAND CTsubhisto(BAT[void,bit] sel, BAT[void,oid] grp, BAT[oid,any] domain) 
	: BAT[oid,int] = CTsubhisto; 
	"optimized sub-histogram for synced grp[void,oid] and sel[void,bit] 
         bats on a previously known domain "

.COMMAND CThistosum(BAT[oid,oid] ct, BAT[oid,int] histo) : BAT[oid,int] = CThistosum;
        "Produce sum over old histogram table"

.END xtables;

@mil

     PROC CTorderby(bat[oid,any] b, bat[oid,any] a) : bat[oid,oid] {
         if (not(b.reverse().ordered())) {
                b := b.reverse().sort().reverse();
         }
         return CTrefine(b,a);
     }

     PROC CTorderby_rev(bat[oid,any] b, bat[oid,any] a) : bat[oid,oid] {
         if (not(b.reverse().ordered())) {
                b := b.reverse().sort().reverse();
         }
         return CTrefine_rev(b,a);
     }

     PROC CTsubgroup(BAT[oid,oid] ct, BAT[oid,any] b, BAT[oid,any] sel) : BAT[oid,oid] {
     		return ct.CTgroup(b.semijoin(sel));
	}

     PROC {count}(BAT[oid,bit] sel, BAT[oid,oid] ct, BAT[oid,int] dom) : BAT[oid,int] {
		return sel.[ifthen](ct).reverse().{count}(dom);
	}

     PROC {count}(BAT[void,bit] sel, BAT[void,oid] ct, BAT[oid,int] dom) : BAT[oid,int] {
		return CTsubhisto(sel, ct, dom);
	}

     proc orderby_table(str fcn, str spec, ..bat[any::1,any]..) : void {
             var meta := new(str,str,$0);
             var first := int(spec) + 2;
             var b := $(first);
             var s := b.reverse().sort().reverse();
     
             while(true) {   
                     var idx := spec.search(',');
                     if (idx < 0) break;
                     spec := spec.string(idx + 1);   
                     s := s.CTrefine($(int(spec) + 2));
             }
             var bb := s.mirror().outerjoin(b);
             var i := 2;
             meta.insert("int", str(first - 2));
             while((i :+= 1) <= $0) {
                     if (i = first) {
                         meta.insert("BAT", str(bb));
                     } else {
                         meta.insert("BAT", str($(i)));
                     }
             }
             call(fcn, meta);
     }
     
     proc table(str spec, bat[any::1,any] b, ..bat[any::1,any]..) : void {
             orderby_table("table", $(1..));
     }
     
     proc print(str spec, bat[any::1,any] b, ..bat[any::1,any]..) : void {
             orderby_table("print", $(1..));
     }

@- Histogram reuse
@T
An accelerator structure is used to establish a link between a BAT
column, and a histogram on it. In this way, already computed histograms
can be reused.

All CT grouping operations, CTgroup(attr), CTgroup(ct,attr) and
CTsubgroup(ct,attr,sel), let their internally generated histogram on
the group-ids of the newly computed CT BAT behind using
the {\tt histolink} accelerator, so the overloaded histogram(ct)
will be able to pick it up without computation.

@* Example Script
The cross tables are checked using the scripts defined below.
@mil

# First some procs for convenience: 
# - xtable_print(group[oid,oid], str, attr1[oid,a1], attr2[oid,a2], ...)
# 	print group count table 
# - xtable_realloc(bat, capacity):  bat
#	allocate more memory and return a copy of a bat
# - xtable_test(str title, bat x, bat y, bat z)
#	 major test script on xtable functionality

proc xtable_print(..any..) : void {
    printf("\n# %s (ct: %d elts, %d groups, %d bytes)\n",
                $2, $3.count(), $1.count(), $3.batsize());
    table($1.col_name("count"), $(4..));
}

proc xtable_histo_direct(..any..) : void {
    var histo := $2.histogram();
    histo.xtable_print($(1..), histo.mirror().col_name("gid"));
}

proc xtable_histo_remap(..any..) : void {
    var map := new(oid,oid).key(true).reverse().insert($2);
    map.join($2.histogram()).xtable_print($(1..), map.col_name("gid"));
}

proc xtable_realloc(bat[any::1,any::2] b, int cap) : bat[any::1,any::2] {
    var inf := b.info();
    var ht := monet_atomtbl.find(inf.find("head"));
    var tt := monet_atomtbl.find(inf.find("tail"));
    var bn := new(ht, tt, cap);
    bn.insert(b);
    if (ht = void) bn.seqbase(b.reverse().fetch(0));
    if (tt = void) bn.seqbase(b.fetch(0));
    return bn;
}

proc xtable_test(str title, bat x, bat y, bat z) : void {
    printf("# xtable_test: %s grouping (%d tuples, %d bytes)\n\n",
                title, x.count(), x.batsize() + y.batsize() + z.batsize());
    x.col_name("x"); y.col_name("y"); z.col_name("z");
    var yy := [+](y,dbl(1.0)).col_name("yy");

    print(x.slice(0,7).col_name("x"),y,yy,z);
    printf("synced(x,y) := %s\n", synced(x,y).str());
    printf("synced(x,z) := %s\n", synced(x,z).str());

    # test group 
    var b1 := CTgroup(x);
    xtable_histo_direct("group(x)", b1, x);

    # test derive
    b1 := CTgroup(b1,y).access(BAT_WRITE);
    xtable_histo_direct("group(x,y)", b1, x, y);

    # test orderby
{   var b2 := CTrefine(x.reverse().sort().reverse(), y);
    xtable_histo_remap("orderby(x,y)", b2, x, y); }
 
    # test subgroup
{   var b2 := {count}([<=](y,dbl(2.0)),b1,b1.histogram()).col_name("count");
    b2.xtable_print("subhisto(x,y|1-2)", x, y, b2.mirror().col_name("gid")); }
    b1 := CTsubgroup(b1,y, y.uselect(dbl(1.0),dbl(2.0)));
    xtable_histo_direct("group(x,y|1-2)", b1, x, y);

    # test regroup
    CTregroup(b1,z);
    xtable_histo_direct("regroup(x,y,z)", b1, x, y, z);
}

PROC test_xtables() : void {
    setoid(oid(20000000));

    var x := new(oid,chr);
    x.insert(1@0,'a');
    x.insert(2@0,'a');
    x.insert(3@0,'b');
    x.insert(4@0,'b');
    x.insert(5@0,'b');
    x.insert(6@0,'b');
    x.insert(7@0,'c');
    x.insert(8@0,'c');
     
    var y:= new(oid,dbl);
    y.insert(1@0,dbl(1.0));
    y.insert(2@0,dbl(2.0));
    y.insert(3@0,dbl(2.0));
    y.insert(4@0,dbl(2.0));
    y.insert(5@0,dbl(2.0));
    y.insert(6@0,dbl(3.0));
    y.insert(7@0,dbl(1.0));
    y.insert(8@0,dbl(1.0));
     
    var z:= new(oid,str);
    z.insert(1@0,"c");
    z.insert(2@0,"a");
    z.insert(3@0,"b");
    z.insert(4@0,"c");
    z.insert(5@0,"a");
    z.insert(6@0,"a");
    z.insert(7@0,"c");
    z.insert(8@0,"a");
    
    xtable_test("small hash", x, y, z);
     
    # create vectorized versions
    x := x.reverse().project().reverse().copy();
    y := y.reverse().project().reverse().copy();
    z := z.reverse().project().reverse().copy();
     
    # activate voids
    x.seqbase(1@0);
    y.seqbase(1@0);
    z.seqbase(1@0);
     
    xtable_test("small vectorized", x, y, z);
    
    # deactivate voids
    x.seqbase(oid(nil));
    y.seqbase(oid(nil));
    z.seqbase(oid(nil));
 
    print("growing vectorized tables from 8 to 512k elements...");
    x := x.xtable_realloc(512*1024);
    y := y.xtable_realloc(512*1024);
    z := z.xtable_realloc(512*1024);
    var i := 17;
    while((i := i - 1) > 0) {
            x.insert(x.copy());
            y.insert(y.copy());
            z.insert(z.copy());
            print(x.count());
    }
    print("done!");
 
    # materialize the oids again
    x := [oid](x.reverse().mark(oid(1))).reverse();
    y := [oid](y.reverse().mark(oid(1))).reverse();
    z := [oid](z.reverse().mark(oid(1))).reverse();
    y := y.access(BAT_WRITE);
    z := z.access(BAT_WRITE);
    y.insert(0@0,dbl(nil));
    z.insert(0@0,str(nil));
    y.delete(0@0);
    z.delete(0@0);
     
    xtable_test("big hash", x, y, z);
     
    # let the Mserver find out that the head columns are equal
    y := x.mirror().join(y);
    z := x.mirror().join(z);
 
    xtable_test("big synced", x, y, z);
    
    # activate voids
    x := x.reverse().mark(1@0).reverse();
    y := y.reverse().mark(1@0).reverse();
    z := z.reverse().mark(1@0).reverse();
    
    xtable_test("big vectorized", x, y, z);
}
    
@{
@* Implementation Code
@c
#include "algebra.h"
#include "xtables.proto.h"

@+ HistoLink
The histolink accelerator establishes a link between a BAT column and a 
histogram on it. In this way, once computed, a histogram can be reused.
@c
typedef struct {
        bat histogram;
	oid alignment;
} histolink;


@- Implementation
@c
int hl_stamp = 0;

static bat build_hl(Heap* hp, bat histo, BAT *b) {
        histolink *hl;
        BAT *h = NULL;
	bat created = 0;
 
        if (histo) {
                h = BBP_cache(histo);
        }
        if (h == NULL) {
		h = BATorder(BAThistogram(BATmirror(b)));
		if (h == NULL) return 0;
                created = histo = h->batCacheid;
        } else {
		if (b->halign == 0) {
       			b->halign = OIDnew(1);
		}
		h->halign = NOID_AGGR(b->halign);
	        BATkey(h, TRUE);
        	h->tsorted = 0;
        	if ((h->hsorted = BAThordered(b)) == TRUE) {
        		if (BATcount(h) == BATcount(b)) {
                		ALIGNsetH(h,BATmirror(b));
        		}
		} else if (BATorder(h) == NULL) {
			BBPreclaim(h);
			return 0;
		}
	}
	if (strncmp(BBPname(histo), "tmp_", 4) == 0) {
        	char buf[256], *p=buf+4;
        	int l=256;
        	l = batToStr(&p, &l, &b->batCacheid);
        	buf[0] = 'h'; buf[1] = 'i'; buf[2] = 's'; 
		buf[3] = 't'; buf[4] = 'o';
        	if  (buf[5] == '~') buf[5] = '_';
        	p[l-1] = 0;
        	BBPrename(histo, buf);
	}
	h->batStamp = hl_stamp++;  
        if (HEAPalloc(hp, sizeof(histolink), 1) >= 0) {
        	hl = (histolink*) hp->base;
        	hl->histogram = histo;
        	if (h->halign == 0) h->halign = OIDnew(1);
        	hl->alignment = h->halign;
        	b->hacctype = ACC_histolink;
		BBPincref(histo, TRUE);
	} 
	return created;
}

void
CTbuild_hl(
   Heap*        hp,     /* INOUT: vector accelerator heap */
   bat*         param,  /* an extra parameter */
   BAT*         b       /* IN: BAT with head-column. */
){
	bat created = build_hl(hp, param?*param:0, b);
	if (created) {
		BBPunfix(created);
	}
}

static void destroy_hl(Heap* hp, BAT* b, bat histo) {
	if (histo) {
		BBPdecref(histo, TRUE);
		BBPunfix(histo);
	}
	if (hp->base) {
		HEAPfree(hp);
		hp->base = NULL;
	}
        b->hacctype = 0;
}

static bat check_hl(Heap* hp, BAT* b) {
        histolink *hl = (histolink*) hp->base;
	bat histo = 0;

        if (hl && hl->histogram && BBPfix(hl->histogram) > 0) {
		b = BBPdescriptor(histo = hl->histogram);
		if (b && hl->alignment == b->halign) {
			return hl->histogram;
		}
		GDKwarning("CTcheck_hl: histogram was changed!");
	} else {
		GDKwarning("CTcheck_hl: histogram disappeared!");
	}
	destroy_hl(hp, b, histo);
	return 0;
}

int
CTcheck_hl(
   Heap*        hp,     /* put return BAT[oid,any] here. */
   BAT*         b       /* pointer to BAT[oid,any] record.*/
){
	bat histo = check_hl(hp, b);
	if (histo) {
		BBPunfix(histo);
		return GDK_SUCCEED;
	}
	return GDK_FAIL;
}

void
CTdestroy_hl(
   Heap*        hp,     /* INOUT: accelerator heap. */
   BAT*         b       /* IN: BAT with accelerated head column. */
){
	bat histo = check_hl(hp, b);
	destroy_hl(hp, b, histo);
}

void
CTupdate_hl(
   Heap*        hp,     /* INOUT: accelerator heap. */
   size_t	idx,
   BAT*         b,      /* IN: BAT with accelerated head column. */
   ptr		val)
{
	(void) idx; (void) val;
	CTdestroy_hl(hp, b);
}

int 
CThistogram(
    BAT**	h, 
    BAT* 	b
){
	bat histo = 0;

	if (b->tacctype && b->tacctype != ACC_histolink) {
		/* another accelerator already exists; leave it */
		*h = BAThistogram(b);
		return *h?GDK_SUCCEED:GDK_FAIL;
	} 
	if (b->tacctype == ACC_histolink) {
		histo = check_hl(&b->taccelerator,b);
	}
	if (histo == 0) {
		if (b->tacctype) {
			ACCdestroy(b->tacctype, BATmirror(b), &b->taccelerator);
		}
		histo = build_hl(&b->taccelerator, 0, BATmirror(b));
		if (histo == 0) return GDK_FAIL;
	}
	*h = VIEWcreate(BBP_cache(histo));
	BBPunfix(histo);

	return GDK_SUCCEED;
}

int
CTtunique(
    BAT**       h,
    BAT*        b
){
	bat histo = 0;

	if (b->tacctype && b->tacctype != ACC_histolink) {
		/* another accelerator already exists; leave it */
		BAT *v = VIEWhead(BATmirror(b));
		*h = BATorder(BATkunique(v));
		BBPreclaim(v);
		return *h?GDK_SUCCEED:GDK_FAIL;
	} 
	if (b->tacctype == ACC_histolink) {
		histo = check_hl(&b->taccelerator,b);
	}
	if (histo == 0) {
		if (b->tacctype) {
			ACCdestroy(b->tacctype, BATmirror(b), &b->taccelerator);
		}
		histo = build_hl(&b->taccelerator, 0, BATmirror(b));
		if (histo == 0) return GDK_FAIL;
	} else {
		histo = ((histolink*) b->taccelerator.base)->histogram;
	}
	*h = VIEWhead(BBP_cache(histo));
	BBPunfix(histo);

        return GDK_SUCCEED;
}

@+ Core Grouping Algorithms
@T
We use hash-grouping all the way. This implementation employs
a simple sequential scan through the operands, adding group
values to a hash-table. This hash-table gives access to the group
identifiers, which are always OIDs.

This strategy is also followed on binary groupings; here 
we construct a special integer consisting of the XORed hashnumber 
of both columns. In such a way, we can build a hash table on
map\_entries (instead of simple atomic values -- the unary case).

In the unary group case, we optimized processing on 1-byte
and 2-byte values by using direct mapping in an array instead of 
hashing.
@c
#define HASH_chr(p) ((hash_t) (*(unsigned char*) (p)))
#define HASH_sht(p) ((hash_t) (*(unsigned short*) (p)))
#define HASH_int(p) ((hash_t) *(unsigned int*) (p))
#define HASH_lng(p) ((hash_t)(((unsigned int*)(p))[0]^((unsigned int*)(p))[1]))
#define HASH_any(p) ((*hashfcn)(p))

#define match_sync(b,p,r) r += yy
#define match_hash(b,p,r) BUNfndOID(r,b,p); if (r == NULL) continue;

#define declare_atom int any = b->ttype; hash_t (*hashfcn)(ptr) = BATatoms[any].atomHash;
#define declare_simple /* any and hash would otherwise give unused variable warning */

#define htype_sync(b) BAThdense(b)?TYPE_void:TYPE_oid
#define htype_hash(b) TYPE_oid

#define ttype_simple(b,t) t 
#define ttype_atom(b,t) b->ttype 

#define STANDARD_MASK ((hash_t) 1023)

/* 
   Note:
     following macros take advantage of clustered property;
     if b is clustered, then we can stop early traversing collision lists.

     BTW, simply stopping possibly breaks chain construction, so the resulting
     map is not directly reuseable as a hash table; the current Monet cannot
     however handle multiple accellerators, so this ain't a real problem for now :)
 */
						    
#define declare_unclustered /* avoid warning */
#define declare_clustered   int samecluster = TRUE;

#define chain_unclustered   for (zz = hash[c]; zz > 0; zz = e->link)
#define chain_clustered     for (zz = hash[c]; (zz > 0) && (samecluster); zz = e->link)

#define tst_grp_unclustered(eq,p,t)    (eq(p, tcur, t))
#define tst_grp_clustered(eq,p,t)      (samecluster = eq(p, tcur, t))

#define tst_derive_unclustered(eq,p,t) (e->hcur == hcur && eq(p, tcur, t))
#define tst_derive_clustered(eq,p,t)   ((samecluster = e->hcur == hcur) && eq(p, tcur, t))

/* NOTE: the first two fields of idxentry_t and mapentry_t MUST be the
   same */
typedef struct {
	oid hcur;		/* old group id */
	hash_t link;		/* hash link */
} idxentry_t;

typedef struct {
	oid hcur;		/* old group id */
	hash_t link;		/* hash link */
	oid gid;		/* new group id */
	int cnt;		/* histogram count */		
} mapentry_t;

typedef struct {
	BAT *map;		/* [mapentry,value] elements */
	hash_t *hash, mask;	/* hash buckets and mask */
	Heap hp;		/* storage for hash buckets */
} map_T;

@:map_init_def(STANDARD,STANDARD_MASK,4096)@
@:map_init_def(CUSTOM,custom_MASK,custom_rng)@

@= map_init_def
#define map_init_@1(map,hash,mask,entry,mapsize)			\
	if (m) {							\
		map = m->map; hash = m->hash; mask = m->mask;		\
    	} else {							\
		map = BATnew(TYPE_mapentry, tailtype(b,TRUE), @3);	\
		hash = (hash_t*) alloca((int)(sizeof(hash_t)*((mask=@2)+1))); \
		for(xx=0; xx<=@2; xx++) {				\
			hash[xx] = 0;					\
		}							\
    	}								\
	entry.cnt = 1;							\
	mapsize = BUNindex(map, BUNlast(map));
@c
void map_free(map_T m) {
	BBPreclaim(m.map);
	HEAPfree(&m.hp);
}

#ifndef offsetof
#define offsetof(type, member)	((size_t) &((type *) 0)->member)
#endif

BAT *map2histo(BAT *map) {
	if (map == NULL || map->htype != TYPE_mapentry || VIEWparent(map) ||
	    map->batSharecnt > 1 || BATgetaccess(map) != BAT_WRITE) 
	{
		if (map) BBPreclaim(map);
		return NULL;
	}
        /* trickily transform a bat[mapentry,any] into bat[oid,int] */
	map->htype = BATmirror(map)->ttype = TYPE_oid;
	strcpy(map->hatom, "oid");
	map->ttype = BATmirror(map)->htype = TYPE_int;
	strcpy(map->tatom, "int");
	if (map->tvarsized && map->theap.base) {
		HEAPfree(&map->theap);
	}
	BATmirror(map)->hvarsized = map->tvarsized = 0;
	/* do these in the right order: map->dims.headloc is used, so
	 * change it at end
	 */
	BATmirror(map)->dims.headloc = 
	    map->dims.tailloc = map->dims.headloc + (int) offsetof(mapentry_t,cnt);
	BATmirror(map)->dims.tailloc = 
	    map->dims.headloc = map->dims.headloc + (int) offsetof(mapentry_t,gid);
	return map;
}
 
@T
The group macro is split along three dimensions:
\begin{description}
\item [type:] Type specific implementation for selecting the right
hash function and data size etc.;
\item [clustered:] The \{clustered and unclustered\} select the
appropriate algorithm, i.e., with or without taking advantage of 
an order of values in the parent groups;
\item [physical properties:] Values \{standard and custom\}, 
chosing between a fixed predefined and a custom hashmask. Custom
allows the user to determine the size of the hashmask (and indirectly 
the estimated size of the result). The hashmask is $2^n - 1$ where $n$ 
is given by the user, or 1023 otherwise, and the derived result
size is $4 \cdot 2^n$.
\end{description}

Further research should point out whether fitting a simple statistical 
model (possibly a simple mixture model) can help choose these parameters 
automatically; the current idea is that the user (which could be a
domain-specific extension of the higher-level language) knows the 
properties of the data, especially for IR in which the standard grouping
settings differ significantly from the original datamining application.
@c
#define group_params_STANDARD /* fixed */
#define group_params_CUSTOM   hash_t custom_MASK, int custom_rng,

@= group
BAT *CTgroup_@1_@4_@5( group_params_@5 BAT *b, BAT *bn, map_T *m ){
	oid *dst = (oid*) BUNfirst(bn);
	hash_t xx, *hash, mask;
	size_t zz, mapsize;
	mapentry_t entry, *e;
	BUN p, q, r;
	BAT *map = NULL;
	declare_@3
 
	map_init_@5(map,hash,mask,entry,mapsize); 
	if (map == NULL) return NULL;

	/* core hash grouping algorithm */
	BATloopFast(b, p, q, xx) {
		declare_@4
		ptr tcur = BUN@2(b,p);

		/* hash-lookup of 'tcur' in map */
    		hash_t c = HASH_@1(tcur);
		c = mix_int(c) & mask;
		chain_@4 {
			r = BUNptr(map,zz);
			e = (mapentry_t*) BUNhloc(map,r);
			if (tst_grp_@4(@3_EQ, BUN@2(map,r), @1)) {
				if (m == NULL) e->cnt++; 
				goto found;
			} 
		} 

		/* not found-> insert new element in map (and hash) */ 
		if (m) {
		  	zz = mapsize;
		} else {
			entry.gid = *(oid*) BUNhead(b,p);
		}
		entry.link = hash[c];
		hash[c] = mapsize++;
		bunfastins(map, &entry, tcur);
		e = &entry;

found:		/* ultra-fast 'insert' of [oid,gid] into ct */
		if (bn->htype) *dst++ = *(oid*) BUNhead(b,p);
		*dst++ = m?zz:e->gid;
	}
	bn->batBuns->free = ((BUN) dst) - bn->batBuns->base; 
	bn->tsorted = 0;
	ALIGNsetH(bn,b);
	return m?NULL:map2histo(map);
bunins_failed:
	BBPreclaim(bn);
	return NULL;
} 
@c
int tailtype(BAT *b, int str_trick) {
	int tpe = ATOMstorage(b->ttype); /* standard type remappings */

	/* more daring remappings possible under simple equality */
	if (tpe == TYPE_flt) {
		return TYPE_int; 
	} else if (tpe == TYPE_dbl) {
		return TYPE_lng; 
	} else if (tpe == TYPE_str && str_trick && GDK_ELIMDOUBLES((&b->theap))) {
		return TYPE_var; /* string offsets are identifying integers */
	}
	return tpe;
}

/* Generate both 'normal' CTgroup and clustered CTgroups */
@= wrappedgroupinner
@:group(chr,tloc,simple,@1,@2)@
@:group(sht,tloc,simple,@1,@2)@
@:group(int,tloc,simple,@1,@2)@
@:group(lng,tloc,simple,@1,@2)@
@:group(any,tail,atom,@1,@2)@

/* Generate both 'normal' CTgroup and parameterized CTgroups */
@= wrappedgroupouter
@:wrappedgroupinner(unclustered,@1)@
@:wrappedgroupinner(clustered,@1)@

@c
@:wrappedgroupouter(STANDARD)@
@:wrappedgroupouter(CUSTOM)@

@= returnvalue
        @1 =
@c
#define declare_mask_STANDARD	/* fixed */
#define declare_mask_CUSTOM 	hash_t mask = (1 << *N) - 1;

int
CTgroup(
   BAT**        retval, /* put pointer to BAT[oid,oid] record here. */
   BAT*         b       /* pointer to BAT[oid,oid] record.*/
)
{
  @:CTgroupbody(STANDARD)@
}

int
CTgroup_custom(
   BAT**        retval, 	/* put pointer to BAT[oid,oid] record here. */
   BAT*         b,      	/* pointer to BAT[oid,oid] record.*/
   int*	        N,		/* number of bits for hashmask */
   int*	        rng		/* expected number of entries in map */
)
{
  @:CTgroupbody(CUSTOM)@
}

@= CTgroupbody
	BAT *histo = NULL, *bn = BATnew(b->htype, TYPE_oid, BATcount(b));
        declare_mask_@1 
	if (bn == NULL) {
		return GDK_FAIL;
	}
	/* Poor man's clustered test: sorted & !keyed => clustered  */
	if ( (b->tsorted) && !(b->tkey) ) {
	  @:choosegroup@1(tailtype(b,TRUE),bn,NULL,clustered,histo)@
	} else {
	  @:choosegroup@1(tailtype(b,TRUE),bn,NULL,unclustered,histo)@
	}
	if (histo == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	bn->tsorted = 0;
	ALIGNsetH(bn, b);
	CTbuild_hl(&bn->taccelerator, &histo->batCacheid, BATmirror(bn));
	if (bn->taccelerator.base == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
	*retval = bn;
	return GDK_SUCCEED;

@= choosegroupSTANDARD
	/* Choose appropriate @4 CTgroup implementation */
	switch(@1) {
	case TYPE_chr: @?@5:returnvalue(@5)@ CTgroup_chr_@4_STANDARD(b,@2,@3); break;
	case TYPE_sht: @?@5:returnvalue(@5)@ CTgroup_sht_@4_STANDARD(b,@2,@3); break;
	case TYPE_int: @?@5:returnvalue(@5)@ CTgroup_int_@4_STANDARD(b,@2,@3); break;
	case TYPE_lng: @?@5:returnvalue(@5)@ CTgroup_lng_@4_STANDARD(b,@2,@3); break;
	default:       @?@5:returnvalue(@5)@ CTgroup_any_@4_STANDARD(b,@2,@3);
	}

@= choosegroupCUSTOM
	/* Choose appropriate @4 CTgroup implementation */
	switch(@1) {
	case TYPE_chr: 
	  @?@5:returnvalue(@5)@ CTgroup_chr_@4_CUSTOM(mask,*rng,b,@2,@3); 
	  break;
	case TYPE_sht: 
	  @?@5:returnvalue(@5)@ CTgroup_sht_@4_CUSTOM(mask,*rng,b,@2,@3);
	  break;
	case TYPE_int: 
	  @?@5:returnvalue(@5)@ CTgroup_int_@4_CUSTOM(mask,*rng,b,@2,@3); 
	  break;
	case TYPE_lng: 
	  @?@5:returnvalue(@5)@ CTgroup_lng_@4_CUSTOM(mask,*rng,b,@2,@3); 
	  break;
	default:       
	  @?@5:returnvalue(@5)@ CTgroup_any_@4_CUSTOM(mask,*rng,b,@2,@3);
	}

@= derive
BAT* CTderive_@1_@2_@5(BAT* ct, BAT *b, BAT *bn, map_T *m) { 
	oid *dst = (oid*) BUNfirst(bn);
	size_t yy = BUNsize(ct), zz, mapsize;
	hash_t xx, *hash, mask;
	BUN p, q, r, cp = BUNfirst(ct) - yy;
	mapentry_t entry, *e;
	BAT *map;
	declare_@4
   
	map_init_STANDARD(map,hash,mask,entry,mapsize); 
	if (map == NULL) return NULL;

	/* core hash grouping algorithm */
	BATloopFast(b, p, q, xx) {
		ptr tcur = BUN@3(b,p);
		hash_t c;
		oid hcur;
		declare_@5

		/* find corresponding value in 'ct' */
		match_@1(ct, BUNhead(b,p), cp); 
		hcur = *(oid*) BUNtloc(ct,cp);

		/* hash-lookup of [hcur,tcur] in map */
		c = (((hash_t) hcur) ^ HASH_@2(tcur));
		c = mix_int(c) & mask;
		chain_@5 {
			r = BUNptr(map,zz);
			e = (mapentry_t*) BUNhloc(map,r);
			if (tst_derive_@5(@4_EQ, BUN@3(map,r), @2)) {
				if (m == NULL) e->cnt++; 
				goto found;
			} 
		} 
		/* not found-> insert new element in map (and hash) */ 
		if (m) {
			zz = mapsize;
		} else {
			entry.gid = *(oid*) BUNhead(b,p);
		}
		entry.hcur = hcur;
		entry.link =  hash[c];
		hash[c] = mapsize++;
		bunfastins(map, &entry, tcur);
		e = &entry;

found:		/* ultra-fast 'insert' of [oid,gid] into ct */
		if (bn->htype) *dst++ = *(oid*) BUNhead(b,p);
		*dst++ = m?zz:e->gid;
	}
	bn->batBuns->free = ((BUN) dst) - bn->batBuns->base; 
	return m?NULL:map2histo(map);
bunins_failed:
	BBPreclaim(bn);
	return NULL;
}

@c

/* Generate both 'normal' CTderive and clustered CTderive */
@= wrappedderive
@:derive(sync,chr,tloc,simple,@1)@
@:derive(sync,sht,tloc,simple,@1)@
@:derive(sync,int,tloc,simple,@1)@
@:derive(sync,lng,tloc,simple,@1)@
@:derive(sync,any,tail,atom,@1)@
@:derive(hash,chr,tloc,simple,@1)@
@:derive(hash,sht,tloc,simple,@1)@
@:derive(hash,int,tloc,simple,@1)@
@:derive(hash,lng,tloc,simple,@1)@
@:derive(hash,any,tail,atom,@1)@
@c
@:wrappedderive(unclustered)@
@:wrappedderive(clustered)@

@= choosederive
	/* Choose appropriate (@1 && @2) CTderive implementation */
	switch(tt) {
	case TYPE_chr: histo = CTderive_@1_chr_@2(ct,b,bn,m); break;
	case TYPE_sht: histo = CTderive_@1_sht_@2(ct,b,bn,m); break;
	case TYPE_int: histo = CTderive_@1_int_@2(ct,b,bn,m); break;
	case TYPE_lng: histo = CTderive_@1_lng_@2(ct,b,bn,m); break;
	default:       histo = CTderive_@1_any_@2(ct,b,bn,m);
	}
@c
int derive(BAT** retval, BAT* ct, BAT* b, int tt, map_T *m) {
	BAT *histo = NULL, *bn = *retval;
	int synced = ALIGNsynced(ct,b);

	/* preprocess/create the result bat 'bn' */
	if (bn) {
		ACCremoveall(bn);
	} else {
		int ht = (synced&&BAThdense(b))?TYPE_void:TYPE_oid;
		bn = BATnew(ht, TYPE_oid,BATcount(b));
		if (bn == NULL) {
			return GDK_FAIL;
		}
	}

	/* CTderive with correct lookup method (hash,synced) and type */
	if (synced) {
	   if ((ct->tsorted) && !(ct->tkey)) {
		@:choosederive(sync,clustered)@
	   } else {
		@:choosederive(sync,unclustered)@
           }
	} else {
	   if ((ct->tsorted) && !(ct->tkey)) {
		@:choosederive(hash,clustered)@
	   } else {
		@:choosederive(hash,unclustered)@
           }
	}
	if (histo == NULL) {
		BBPunfix(bn->batCacheid);
	}

	/* postprocess the result bat 'bn' */
	bn->tsorted = 0;
	if (BATcount(bn) == BATcount(b)) {
		ALIGNsetH(bn, b);
	} else {
		bn->hsorted = BAThordered(b);
		if (b->hkey) BATkey(bn, TRUE);
	}
	if (histo) {
		CTbuild_hl(&bn->taccelerator, &histo->batCacheid, BATmirror(bn));
		if (bn->taccelerator.base == NULL) {
			BBPreclaim(bn);
			return GDK_FAIL;
		}
	}
	*retval = bn;
	return GDK_SUCCEED;
}

int regroup(BAT** retval, BAT* ct, BAT* b, int tt, map_T *m) {
	BBPfix(ct->batCacheid);
	*retval = BATsetaccess(ct, BAT_WRITE);
	(*retval)->batDirty = 1;
        (*retval)->batDirtybuns = 1;
        (*retval)->batDirtydesc = 1;
	return derive(retval, ct, b, tt, m);
}

int CTregroup(BAT** retval, BAT* ct, BAT* b) {	
	return regroup(retval, ct, b, tailtype(b, TRUE), NULL);
}

int CTderive(BAT** retval, BAT* ct, BAT* b) {	
	BAT *bn = NULL;
	int ret;
	if (tailtype(ct, TRUE) != TYPE_int) {
		if (!CTgroup(&bn, ct)) return GDK_FAIL;
		ct = bn;
	} 
        *retval = NULL;
	ret = derive(retval, ct, b, tailtype(b, TRUE), NULL);
	if (bn) BBPreclaim(bn);
	return ret;
}

@-
The routine CThistosum takes an grouping and a histogram and produces
a new histogram by summing the old values within the same group.
@c
int
CThistosum(
   BAT**        retval, /* put pointer to BAT[oid,int] record here. */
   BAT*         b,   /* pointer to BAT[oid,oid] record.*/
   BAT*         c    /* pointer to BAT[oid,int] record.*/
)
{	
        BAT *res = BATnew(TYPE_oid,TYPE_int, BATcount(b));
        BUN p,q, qb;
        int xx,i,*z;
        oid ot,oh;
	(void) c;
	if (res == NULL) {
		return GDK_FAIL;
	}
	BATloopFast(b,p,q,xx) {
		oh = *(oid *) BUNhloc(b,p);
		i  = *(int *) BUNtloc(b,p);

		BUNfndOID(qb, b, &oh);
		if( qb == NULL ) {
			GDKerror("CThistosum: Matching count entry not found\n");
			continue;
		}
		ot = *(oid *) BUNtloc(b,qb);

		BUNfndOID(qb, res, &ot);
		if( qb == NULL) {
			BUNins(res, &ot, &i);
		} else {
			z = (int *) BUNtloc(res,qb);
			*z += i;
		}
	}
	res->hsorted = res->tsorted = 0;
	*retval = res;
	return GDK_SUCCEED;
}



int CTsubhisto(BAT **ret, BAT *sel, BAT *grp, BAT *dom) {
	bit *filter = (bit*) BUNtloc(sel,BUNfirst(sel));
        size_t size=BATcount(dom);
	int xx, zz;
	hash_t yy, mask, *hash = NULL;
        BUN r, p, q;
	BAT *bn = BATnew(TYPE_idxentry, TYPE_int, size);
	if (bn == NULL) return GDK_FAIL;

	/* we know the domain; go for perfect hashing */
	for(mask=1; mask<size; mask<<=1);
	if (mask < 256) mask = 256;
	hash = (hash_t*) GDKmalloc(sizeof(hash_t)*mask);
	if (hash == NULL) {
		BBPreclaim(bn);
		return GDK_FAIL;
	}
        for(yy = 0; yy < mask; yy++) {
        	hash[yy] = 0;
	} mask--;

	/* insert all values in the hash table, and in bn with count zero */
	r = BUNfirst(bn);
	yy = 1;
        BATloopFast(dom, p, q, xx) { 
		oid v = *(oid*) BUNhloc(dom,p);
                hash_t c = v & mask; 
		((idxentry_t *) BUNhloc(bn, r))->hcur = v;
		((idxentry_t *) BUNhloc(bn, r))->link = hash[c];
		* (int *) BUNtloc(bn, r) = 0;
		r = BUNnext(bn, r);
                hash[c] = yy; 
		yy++;
	}
	bn->batBuns->free = (char *) r - (char *) Bunbase(bn);
	bn->tsorted = 0;
	bn->htype = BATmirror(bn)->ttype = TYPE_oid;
	strcpy(bn->hatom, "oid");
	/* assert(offsetof(idxentry_t,hcur) == 0); 
	ALIGNsetH(bn, dom); */

        /* add the counts for this selection using the hash table */
	zz = BUNsize(sel);
        BATloopFast(grp, p, q, xx) { 
	    if (*filter == TRUE) {
		oid v = *(oid*) BUNtloc(grp,p);
                hash_t c = v & mask; 
                for(yy = hash[c]; yy > 0; yy = ((idxentry_t *) BUNhloc(bn, r))->link) {
			r = BUNptr(bn, yy);
			if (((idxentry_t *) BUNhloc(bn, r))->hcur == v) {
				* (int *) BUNtloc(bn, r) += 1;
				break;
                        }
                }
  	    }
	    filter += zz;
     
	}
	GDKfree(hash);
	*ret = bn;
        return GDK_SUCCEED;
}

@+ Support for Order-by
@c
#define DEFAULT_SIZE 10000

struct refine {
	var_t off;
	oid o;
};

static INLINE
oid* sort_flush(struct refine *buf, size_t size, int tpe, BUN base, oid* dst, oid *idp, int reverse) {
	int (*cmp)(ptr,ptr) = BATatoms[tpe].atomCmp;
	struct refine *end = buf + size; 
	oid id = *idp + 1;
	ptr cur, val;

	/* StM: we don't need to sort voids, do we??? */
	if (tpe != TYPE_void) {
		/* qsort works fine for small amount of tuples; with few duplicates */
		if (reverse) {
			GDKqsort_rev(buf, base, size, (int)sizeof(struct refine), tpe, offsetof(struct refine, off));
		} else {
			GDKqsort(buf, base, size, (int)sizeof(struct refine), tpe, offsetof(struct refine, off));
		}
	}

	cur = base + buf->off;
	while(buf < end) {
		val = base + buf->off;
		if ((*cmp)(cur, val)) { cur=val; id++;}
		*dst++ = buf->o;
		*dst++ = id;
		buf++;
	}
	*idp = id;
	return dst;
}

static
int refine(BAT **res, BAT *b, BAT *a, int rv) {
    BAT *bn = NULL;
    if (b->tkey) { /* if key, no further refinements can take place */
	bn = BATmark(b, 0);
    } else {
	int (*cmp)(ptr,ptr) = BATatoms[b->ttype].atomCmp;
	BUN p, q, r, last = BUNfirst(b), base = a->theap.base?NULL:a->batBuns->base;
	struct refine *buf, *cur, *end;
	int xx;
	size_t size = DEFAULT_SIZE;
	oid *dst, o, id = 0;

	/* create tmp BAT that holds one cluster; estimate required size using sampling */
	if (BATcount(b) > DEFAULT_SIZE) { 
		BAT *histo = NULL, *sample = BATsample(b, DEFAULT_SIZE);
		if (sample) {
			histo = BAThistogram(sample);
			if (histo) {
				BATmax(histo,&xx);
				if (xx > 1) size = MAX(size, (size_t) (xx*(((float) BATcount(b))/DEFAULT_SIZE)));
				BBPreclaim(histo);
			}
			BBPreclaim(sample);
		}
		if (histo == NULL) return GDK_FAIL;
	} 
	/* create a temporary BAT of the estimated size holding pointers to the a tail atoms */
	buf = cur = (struct refine*) GDKmalloc(size*sizeof(struct refine));
	end = buf + size;
	if (buf == NULL) return GDK_FAIL;

        /* create result BAT */
	bn = BATnew(TYPE_oid, TYPE_oid, BATcount(b));
	if (bn == NULL) {
		GDKfree(buf);
		return GDK_FAIL;
	}
	bn->hsorted = bn->tsorted = FALSE;
	dst = (oid*) BUNfirst(bn);

  	/* merge-scan tail of b, finding chunks with equal values; then sort each chunk on a */
	BATloopFast(b, p, q, xx) {
		if ((*cmp)(BUNtail(b,last), BUNtail(b,p))) {
			dst = sort_flush(buf, (size_t) (cur-buf), a->ttype, base?base:a->theap.base, dst, &id, rv);
		    	last = p; cur = buf;
		}
		o = *(oid*) BUNhead(b,p);
		BUNfndOID(r, a, &o);
		if (r == NULL) {
			*dst++ = o;
			*dst++ = id;
			continue;
		}
		if (cur >= end) {
			size_t off = (size_t) (cur - buf);
			buf = (struct refine *) GDKrealloc(buf, (size*=2)*sizeof(struct refine));
			end = buf + size;
			cur = buf + off;
		}
		r += a->tloc;
		cur->off = base ? (var_t) (r - base) : *(var_t*) r;
		cur->o = o;
		cur++;
	} 
	dst = sort_flush(buf, (size_t) (cur-buf), a->ttype, base?base:a->theap.base, dst, &id, rv);
	GDKfree(buf);
	bn->batBuns->free = ((BUN) dst) - bn->batBuns->base;
	bn->tsorted = 1;
    } 
    *res = bn;
    return GDK_SUCCEED;
}

int CTrefine(BAT **res, BAT *b, BAT *a) {
	return refine(res, b, a, FALSE);
}

int CTrefine_rev(BAT **res, BAT *b, BAT *a) {
	return refine(res, b, a, TRUE);
}
@}
@}
